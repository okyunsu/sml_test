import sys 
import torch 
import platform 
import psutil 
print("="*60) 
print("RTX 2080 newstun-service ì¢…í•© í™˜ê²½ í…ŒìŠ¤íŠ¸") 
print("="*60) 
print() 
print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´:") 
print(f"   OS: {platform.system()} {platform.release()}") 
print(f"   CPU: {platform.processor()}") 
print(f"   RAM: {psutil.virtual_memory().total / 1024**3:.1f}GB") 
print(f"   Python: {sys.version.split()[0]}") 
print() 
print("ğŸ Python íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸:") 
try: 
    import torch 
    print(f"   âœ“ PyTorch: {torch.__version__}") 
except ImportError: 
    print("   âœ— PyTorch ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ") 
try: 
    import transformers 
    print(f"   âœ“ Transformers: {transformers.__version__}") 
except ImportError: 
    print("   âœ— Transformers ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ") 
try: 
    import fastapi 
    print(f"   âœ“ FastAPI: {fastapi.__version__}") 
except ImportError: 
    print("   âœ— FastAPI ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ") 
try: 
    import datasets 
    print(f"   âœ“ Datasets: {datasets.__version__}") 
except ImportError: 
    print("   âœ— Datasets ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ") 
try: 
    import accelerate 
    print(f"   âœ“ Accelerate: {accelerate.__version__}") 
except ImportError: 
    print("   âœ— Accelerate ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ") 
print() 
print("ğŸ® GPU/CUDA í™˜ê²½:") 
if torch.cuda.is_available(): 
    print(f"   âœ“ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.version.cuda}") 
    device_count = torch.cuda.device_count() 
    print(f"   âœ“ GPU ê°œìˆ˜: {device_count}") 
    for i in range(device_count): 
        gpu_name = torch.cuda.get_device_name(i) 
        gpu_props = torch.cuda.get_device_properties(i) 
        total_memory = gpu_props.total_memory / 1024**3 
        print(f"   GPU {i}: {gpu_name}") 
        print(f"          ë©”ëª¨ë¦¬: {total_memory:.1f}GB") 
        if "RTX 2080" in gpu_name: 
            print("          ğŸš€ RTX 2080 ìµœì í™” ê°€ëŠ¥!") 
        torch.cuda.empty_cache() 
        test_tensor = torch.randn(100, 100).cuda(i) 
        allocated = torch.cuda.memory_allocated(i) / 1024**2 
        print(f"          í…ŒìŠ¤íŠ¸ í• ë‹¹: {allocated:.1f}MB") 
        del test_tensor 
        torch.cuda.empty_cache() 
else: 
    print("   âš  CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰") 
print() 
print("ğŸ¤– í•œêµ­ì–´ ëª¨ë¸ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸:") 
recommended_models = [ 
    "klue/roberta-base", 
    "beomi/KcBERT-Base", 
    "klue/bert-base" 
] 
try: 
    from transformers import AutoTokenizer, AutoModel 
    for model_name in recommended_models: 
        try: 
            tokenizer = AutoTokenizer.from_pretrained(model_name) 
            print(f"   âœ“ {model_name} í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ") 
        except Exception as e: 
            print(f"   âš  {model_name} í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {str(e)[:50]}...") 
except ImportError: 
    print("   âœ— Transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë¶ˆê°€") 
print() 
print("ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸:") 
import os 
required_dirs = ['data', 'data/training', 'models', 'output', 'logs', 'app/sample_datasets'] 
for dir_path in required_dirs: 
    if os.path.exists(dir_path): 
        print(f"   âœ“ {dir_path}") 
    else: 
        print(f"   âœ— {dir_path} (ëˆ„ë½)") 
print() 
