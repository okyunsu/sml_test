from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import logging
import re
from collections import defaultdict
import math

from ..model.materiality_dto import MaterialityTopic, MaterialityAssessment
from .materiality_mapping_service import MaterialityMappingService

logger = logging.getLogger(__name__)

class NewsAnalysisEngine:
    """ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„ ì—”ì§„
    
    sasb-serviceì—ì„œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬:
    - ì¤‘ëŒ€ì„± í‰ê°€ í† í”½ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    - sentiment ë¶„ì„ ê²°ê³¼ í™œìš©
    - í‚¤ì›Œë“œ ë§¤ì¹­ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
    - ë‰´ìŠ¤ ë¹ˆë„ ë° ì¤‘ìš”ë„ ë¶„ì„
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.mapping_service = MaterialityMappingService()
        
        # ğŸ¯ ì¤‘ëŒ€ì„± í† í”½ë³„ í‚¤ì›Œë“œ ì‚¬ì „ (ê´‘ë²”ìœ„í•œ ë§¤í•‘)
        self.topic_keyword_dict = {
            "ê¸°í›„ë³€í™” ëŒ€ì‘": [
                # ğŸŒ± ê¸°ë³¸ í™˜ê²½/ê¸°í›„ í‚¤ì›Œë“œ (ë‰´ìŠ¤ì—ì„œ ìì£¼ ì‚¬ìš©)
                "ê¸°í›„ë³€í™”", "ê¸°í›„ìœ„ê¸°", "ì§€êµ¬ì˜¨ë‚œí™”", "ì˜¨ì‹¤ê°€ìŠ¤", "íƒ„ì†Œë°°ì¶œ", "íƒ„ì†Œì¤‘ë¦½", 
                "ë„·ì œë¡œ", "íƒ„ì†Œê°ì¶•", "íƒ„ì†Œì €ê°", "ì¹œí™˜ê²½", "ê·¸ë¦°", "ì²­ì •", "ì—ì½”",
                "ì§€ì†ê°€ëŠ¥", "ESG", "í™˜ê²½", "í™˜ê²½ë³´í˜¸", "í™˜ê²½ê·œì œ", "í™˜ê²½ì •ì±…",
                
                # âš¡ ì—ë„ˆì§€ ì „í™˜ (ì¼ë°˜ì  í‘œí˜„)
                "ì—ë„ˆì§€ì „í™˜", "ì¬ìƒì—ë„ˆì§€", "ì‹ ì¬ìƒì—ë„ˆì§€", "ì‹ ì—ë„ˆì§€", "ì²­ì •ì—ë„ˆì§€",
                "RE100", "ì—ë„ˆì§€ì •ì±…", "ì—ë„ˆì§€ë¯¹ìŠ¤", "ì „ì›ë¯¹ìŠ¤", "ì—ë„ˆì§€íš¨ìœ¨",
                
                # ğŸ”‹ êµ¬ì²´ì  ê¸°ìˆ  (ê°„ë‹¨í•œ ìš©ì–´)
                "ì—°ë£Œì „ì§€", "ìˆ˜ì†Œ", "ìˆ˜ì†Œì—°ë£Œì „ì§€", "ìˆ˜ì†Œë°œì „", "ìˆ˜ì†Œê²½ì œ", "ê·¸ë¦°ìˆ˜ì†Œ",
                "íƒœì–‘ê´‘", "í’ë ¥", "ESS", "ë°°í„°ë¦¬", "ì—ë„ˆì§€ì €ì¥", "ì „ê¸°ì°¨", "EV",
                "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ë§ˆì´í¬ë¡œê·¸ë¦¬ë“œ", "ì „ë ¥ë³€í™˜", "ì¸ë²„í„°",
                
                # ğŸ“‹ ì •ì±…/ì œë„ (ê°„ë‹¨í•œ í‘œí˜„)
                "íƒ„ì†Œì„¸", "ë°°ì¶œê¶Œ", "íƒ„ì†Œêµ­ê²½ì„¸", "K-ETS", "íŒŒë¦¬í˜‘ì •", "TCFD",
                "ê·¸ë¦°ë‰´ë”œ", "í•œêµ­íŒë‰´ë”œ", "íƒ„ì†Œê°ì¶•ëª©í‘œ", "2050íƒ„ì†Œì¤‘ë¦½"
            ],
            
            "ìˆœí™˜ê²½ì œ": [
                # â™»ï¸ ê¸°ë³¸ ìˆœí™˜ê²½ì œ í‚¤ì›Œë“œ
                "ìˆœí™˜ê²½ì œ", "ìì›ìˆœí™˜", "ì¬í™œìš©", "ì¬ì‚¬ìš©", "ì¬ì œì¡°", "ì—…ì‚¬ì´í´ë§",
                "íê¸°ë¬¼", "íê¸°ë¬¼ê°ì¶•", "íê¸°ë¬¼ì²˜ë¦¬", "íê¸°ë¬¼ê´€ë¦¬", "ì œë¡œì›¨ì´ìŠ¤íŠ¸",
                "ìì›íš¨ìœ¨", "ìì›ì ˆì•½", "ìì›ê´€ë¦¬", "ì›ë£ŒíšŒìˆ˜", "ì†Œì¬íšŒìˆ˜",
                
                # ğŸ”„ êµ¬ì²´ì  ì¬í™œìš© ë¶„ì•¼
                "íë°°í„°ë¦¬", "ë°°í„°ë¦¬ì¬í™œìš©", "ííŒ¨ë„", "íƒœì–‘ê´‘íŒ¨ë„ì¬í™œìš©", "íí”Œë¼ìŠ¤í‹±",
                "ê¸ˆì†ì¬í™œìš©", "í¬í† ë¥˜", "í•µì‹¬ê´‘ë¬¼", "í•µì‹¬ì†Œì¬", "ì›ë£Œí™•ë³´",
                
                # ğŸ­ ì‚°ì—… ì—°ê³„
                "ìƒì‚°ê³µì •", "ì œì¡°ê³µì •", "ê³µì •ê°œì„ ", "íš¨ìœ¨ì„±", "ìƒì‚°ì„±",
                "ì›ê°€ì ˆê°", "ë¹„ìš©ì ˆê°", "ì›ë£Œë¹„", "ì†Œì¬ë¹„"
            ],
            
            "ì œí’ˆ í™˜ê²½ì˜í–¥ ì €ê°": [
                # ğŸŒ í™˜ê²½ì˜í–¥ ê¸°ë³¸ í‚¤ì›Œë“œ
                "í™˜ê²½ì˜í–¥", "í™˜ê²½ì¹œí™”", "ì¹œí™˜ê²½ì œí’ˆ", "ê·¸ë¦°ì œí’ˆ", "ì—ì½”ì œí’ˆ",
                "í™˜ê²½ì„±ëŠ¥", "í™˜ê²½íš¨ê³¼", "í™˜ê²½ê°œì„ ", "ì˜¤ì—¼ì €ê°", "ë°°ì¶œì €ê°",
                "ëŒ€ê¸°ì˜¤ì—¼", "ìˆ˜ì§ˆì˜¤ì—¼", "í† ì–‘ì˜¤ì—¼", "ì†ŒìŒ", "ì§„ë™",
                
                # âš¡ ì œí’ˆ íš¨ìœ¨ì„± (ì¼ë°˜ì  í‘œí˜„)
                "íš¨ìœ¨ì„±", "ê³ íš¨ìœ¨", "ì—ë„ˆì§€íš¨ìœ¨", "ì „ë ¥íš¨ìœ¨", "ë°œì „íš¨ìœ¨",
                "ì„±ëŠ¥", "ì„±ëŠ¥ê°œì„ ", "ì„±ëŠ¥í–¥ìƒ", "í’ˆì§ˆ", "í’ˆì§ˆê°œì„ ",
                "ê¸°ìˆ ê°œë°œ", "R&D", "ì—°êµ¬ê°œë°œ", "í˜ì‹ ", "ê¸°ìˆ í˜ì‹ ",
                
                # ğŸ”§ êµ¬ì²´ì  ê¸°ìˆ  (ê°„ë‹¨í•œ ìš©ì–´)
                "ì—°ë£Œì „ì§€", "ìˆ˜ì†Œê¸°ìˆ ", "ë°œì „ê¸°ìˆ ", "ì „ë ¥ê¸°ìˆ ", "ë³€ì••ê¸°",
                "ì¸ë²„í„°", "ì „ë ¥ë³€í™˜", "ë°°í„°ë¦¬ê¸°ìˆ ", "ì €ì¥ê¸°ìˆ ",
                "ìŠ¤ë§ˆíŠ¸ê¸°ìˆ ", "ë””ì§€í„¸ê¸°ìˆ ", "ìë™í™”", "IoT", "AI",
                
                # ğŸ“Š ì„±ê³¼/í‰ê°€
                "LCA", "ìƒì• ì£¼ê¸°", "íƒ„ì†Œë°œìêµ­", "í™˜ê²½ë¼ë²¨", "ì¸ì¦",
                "í™˜ê²½ì¸ì¦", "í’ˆì§ˆì¸ì¦", "ì„±ëŠ¥ì¸ì¦", "íš¨ìœ¨ë“±ê¸‰"
            ],
            
            "ì‚¬ì—…ì¥ ì•ˆì „ë³´ê±´": [
                # ğŸš¨ ê¸°ë³¸ ì•ˆì „ í‚¤ì›Œë“œ (ë‰´ìŠ¤ì—ì„œ ìì£¼ ì‚¬ìš©)
                "ì•ˆì „", "ì•ˆì „ì‚¬ê³ ", "ì‚°ì—…ì•ˆì „", "ì‘ì—…ì•ˆì „", "ì‚¬ê³ ", "ì¬í•´",
                "ì‚°ì—…ì¬í•´", "ì¤‘ëŒ€ì¬í•´", "ì•ˆì „ê´€ë¦¬", "ì•ˆì „ë³´ê±´", "ìœ„í—˜",
                "ìœ„í—˜ìš”ì†Œ", "ì•ˆì „ì ê²€", "ì•ˆì „êµìœ¡", "ì•ˆì „í›ˆë ¨",
                
                # âš ï¸ êµ¬ì²´ì  ì‚¬ê³  ìœ í˜•
                "í™”ì¬", "í­ë°œ", "ê°ì „", "ì¶”ë½", "ë‚™í•˜", "í˜‘ì°©", "ë¼ì„",
                "ê°€ìŠ¤ëˆ„ì¶œ", "í™”í•™ë¬¼ì§ˆ", "ìœ ë…ê°€ìŠ¤", "ì§ˆì‹", "í™”ìƒ",
                
                # ğŸ­ ì‚°ì—…ë³„ ì•ˆì „ (ë°œì „/ì „ê¸° ë¶„ì•¼)
                "ë°œì „ì†Œì•ˆì „", "ì „ê¸°ì•ˆì „", "ê³ ì••ì „ê¸°", "ì •ì „", "ì „ë ¥ì‚¬ê³ ",
                "ì„¤ë¹„ì•ˆì „", "ì¥ë¹„ì•ˆì „", "ë³´ì¼ëŸ¬", "í„°ë¹ˆ", "ë³€ì••ê¸°ì‚¬ê³ ",
                
                # ğŸ“‹ ì•ˆì „ ì œë„/ê´€ë¦¬
                "ì•ˆì „ìˆ˜ì¹™", "ì•ˆì „ê·œì •", "ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•", "KOSHA", "ì•ˆì „ë³´ê±´ê³µë‹¨",
                "ì•ˆì „ê´€ë¦¬ì", "ë³´ê±´ê´€ë¦¬ì", "ìœ„í—˜ì„±í‰ê°€", "ì•ˆì „ì§„ë‹¨",
                "ì˜ˆë°©", "ì˜ˆë°©ì¡°ì¹˜", "ì•ˆì „ëŒ€ì±…", "ê°œì„ ì¡°ì¹˜"
            ],
            
            "ë°˜ë¶€íŒ¨ ìœ¤ë¦¬ê²½ì˜ ê°•í™”": [
                # ğŸ›ï¸ ê¸°ë³¸ ìœ¤ë¦¬/ê±°ë²„ë„ŒìŠ¤ í‚¤ì›Œë“œ
                "ìœ¤ë¦¬", "ìœ¤ë¦¬ê²½ì˜", "íˆ¬ëª…", "íˆ¬ëª…ì„±", "íˆ¬ëª…ê²½ì˜", "ê±°ë²„ë„ŒìŠ¤",
                "ê¸°ì—…ì§€ë°°êµ¬ì¡°", "ì»´í”Œë¼ì´ì–¸ìŠ¤", "ì¤€ë²•", "ë²•ê·œì¤€ìˆ˜",
                "ë‚´ë¶€í†µì œ", "ë¦¬ìŠ¤í¬ê´€ë¦¬", "ìœ„í—˜ê´€ë¦¬",
                
                # ğŸš« ë°˜ë¶€íŒ¨ ê´€ë ¨
                "ë¶€íŒ¨", "ë°˜ë¶€íŒ¨", "ë¶€íŒ¨ë°©ì§€", "ë¹„ë¦¬", "ë¹„ìœ„", "íš¡ë ¹",
                "ë°°ì„", "ë‡Œë¬¼", "í›„ë°©ê³µê¸‰", "íŠ¹í˜œ", "ì´í•´ì¶©ëŒ",
                "ê³µì •ê±°ë˜", "ë‹´í•©", "ì¹´ë¥´í…”", "ë…ê³¼ì ",
                
                # ğŸ” ì¡°ì‚¬/ê°ì‚¬
                "ê°ì‚¬", "ë‚´ë¶€ê°ì‚¬", "ì™¸ë¶€ê°ì‚¬", "ê²€ì°°ìˆ˜ì‚¬", "ê²€ì°°ì¡°ì‚¬",
                "êµ­ì •ê°ì‚¬", "êµ­ê°", "ì¡°ì‚¬", "ìˆ˜ì‚¬", "ê¸°ì†Œ", "ì²˜ë²Œ",
                "ì œì¬", "ê³¼ì§•ê¸ˆ", "ë²Œê¸ˆ", "ì˜ì—…ì •ì§€",
                
                # ğŸ“‹ ê´€ë ¨ ì œë„
                "ì´ì‚¬íšŒ", "ë…ë¦½ì´ì‚¬", "ê°ì‚¬ìœ„ì›íšŒ", "ë‚´ë¶€ì‹ ê³ ", "ì‹ ê³ ì œë„",
                "ìœ¤ë¦¬ê°•ë ¹", "í–‰ë™ê°•ë ¹", "ì¤€ë²•í†µì œ", "ì¤€ë²•ê°ì‹œ"
            ],
            
            "ê³ ìš© ë° ë…¸ì‚¬ê´€ê³„": [
                # ğŸ‘¥ ê¸°ë³¸ ê³ ìš© í‚¤ì›Œë“œ
                "ê³ ìš©", "ì±„ìš©", "ì¸ë ¥", "ì¸ì¬", "ì§ì›", "ê·¼ë¡œì", "ì„ì§ì›",
                "ì¼ìë¦¬", "ì·¨ì—…", "êµ¬ì¸", "êµ¬ì§", "ì²­ë…„ê³ ìš©", "ì‹ ì…ì‚¬ì›",
                "ê²½ë ¥ì§", "ì¸ë ¥ì¶©ì›", "ì¸ë ¥í™•ëŒ€", "ì¸ë ¥ê°ì¶•",
                
                # ğŸ¤ ë…¸ì‚¬ê´€ê³„
                "ë…¸ì‚¬", "ë…¸ì‚¬ê´€ê³„", "ë…¸ë™ì¡°í•©", "ë…¸ì¡°", "íŒŒì—…", "íƒœì—…",
                "ë‹¨ì²´êµì„­", "ì„ê¸ˆêµì„­", "í˜‘ìƒ", "í•©ì˜", "ê°ˆë“±", "ëŒ€ë¦½",
                "ë…¸ì‚¬í˜‘ë ¥", "ë…¸ì‚¬í™”í•©", "ìƒìƒ", "ìƒìƒí˜‘ë ¥",
                
                # ğŸ’° ì„ê¸ˆ/ë³µë¦¬í›„ìƒ
                "ì„ê¸ˆ", "ê¸‰ì—¬", "ì—°ë´‰", "ë³´ë„ˆìŠ¤", "ì„±ê³¼ê¸‰", "ì¸ì„¼í‹°ë¸Œ",
                "ë³µë¦¬í›„ìƒ", "ë³µì§€", "ìˆ˜ë‹¹", "íœ´ê°€", "ìœ¡ì•„íœ´ì§",
                "ì›Œë¼ë°¸", "ì¼ìƒí™œê· í˜•", "ê·¼ë¬´í™˜ê²½", "ê·¼ë¬´ì¡°ê±´",
                
                # ğŸ“š êµìœ¡/ê°œë°œ
                "êµìœ¡", "í›ˆë ¨", "êµìœ¡í›ˆë ¨", "ì—­ëŸ‰ê°œë°œ", "ì¸ì¬ê°œë°œ",
                "ìŠ¤í‚¬ì—…", "ë¦¬ìŠ¤í‚¬ë§", "ì—…ìŠ¤í‚¬ë§", "ì „ë¬¸êµìœ¡",
                "ìŠ¹ì§„", "ì¸ì‚¬", "ì¸ì‚¬ì œë„", "í‰ê°€", "ì„±ê³¼í‰ê°€"
            ],
            
            "ì§€ì†ê°€ëŠ¥í•œ ê³µê¸‰ë§ ê´€ë¦¬": [
                # ğŸ”— ê¸°ë³¸ ê³µê¸‰ë§ í‚¤ì›Œë“œ
                "ê³µê¸‰ë§", "í˜‘ë ¥ì‚¬", "í˜‘ë ¥ì—…ì²´", "íŒŒíŠ¸ë„ˆ", "íŒŒíŠ¸ë„ˆì‚¬", "ë²¤ë”",
                "ì¡°ë‹¬", "êµ¬ë§¤", "ë‚©í’ˆ", "ê³µê¸‰", "ë°œì£¼", "ìˆ˜ì£¼", "ê³„ì•½",
                "ìƒìƒ", "ìƒìƒí˜‘ë ¥", "ë™ë°˜ì„±ì¥", "win-win", "íŒŒíŠ¸ë„ˆì‹­",
                
                # ğŸŒ ê¸€ë¡œë²Œ ê³µê¸‰ë§
                "ê¸€ë¡œë²Œê³µê¸‰ë§", "í•´ì™¸ê³µê¸‰ë§", "êµ­ì œì¡°ë‹¬", "ìˆ˜ì¶œì…", "ë¬´ì—­",
                "ê³µê¸‰ë§ìœ„ê¸°", "ê³µê¸‰ë§ë¦¬ìŠ¤í¬", "ê³µê¸‰ì°¨ì§ˆ", "ê³µê¸‰ë¶€ì¡±",
                "ì›ìì¬", "ë¶€í’ˆ", "ì†Œì¬", "í•µì‹¬ì†Œì¬", "ë°˜ë„ì²´", "ë°°í„°ë¦¬",
                
                # ğŸ“Š ê³µê¸‰ë§ ê´€ë¦¬
                "ê³µê¸‰ë§ê´€ë¦¬", "SCM", "í’ˆì§ˆê´€ë¦¬", "í’ˆì§ˆë³´ì¦", "QA", "QC",
                "ë‚©ê¸°", "ë‚©ê¸°ê´€ë¦¬", "ì¬ê³ ", "ì¬ê³ ê´€ë¦¬", "ë¬¼ë¥˜", "ìœ í†µ",
                "ë””ì§€í„¸í™”", "ë””ì§€í„¸ì „í™˜", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬", "ìë™í™”",
                
                # ğŸŒ± ì§€ì†ê°€ëŠ¥ ê³µê¸‰ë§
                "ESGê³µê¸‰ë§", "ì§€ì†ê°€ëŠ¥ì¡°ë‹¬", "ì¹œí™˜ê²½ì¡°ë‹¬", "ê·¸ë¦°ê³µê¸‰ë§",
                "ì¸ê¶Œ", "ë…¸ë™ì¸ê¶Œ", "ì•„ë™ë…¸ë™", "ê°•ì œë…¸ë™", "ê³µì •ë¬´ì—­",
                "íˆ¬ëª…ì„±", "ì¶”ì ê°€ëŠ¥ì„±", "ê³µê¸‰ë§ì‹¤ì‚¬", "due diligence"
            ],
            
            "ì—ë„ˆì§€ íš¨ìœ¨ í–¥ìƒ": [
                # âš¡ ê¸°ë³¸ ì—ë„ˆì§€ íš¨ìœ¨ í‚¤ì›Œë“œ
                "ì—ë„ˆì§€íš¨ìœ¨", "íš¨ìœ¨", "ê³ íš¨ìœ¨", "ì ˆì•½", "ì ˆì „", "ì—ë„ˆì§€ì ˆì•½",
                "íš¨ìœ¨ì„±", "íš¨ìœ¨ê°œì„ ", "íš¨ìœ¨í–¥ìƒ", "ì„±ëŠ¥", "ì„±ëŠ¥ê°œì„ ",
                "ìµœì í™”", "ìš´ì˜ìµœì í™”", "ì—ë„ˆì§€ê´€ë¦¬", "ì „ë ¥ê´€ë¦¬",
                
                # ğŸ­ ì‚°ì—… ì—ë„ˆì§€ íš¨ìœ¨
                "ê³µì¥íš¨ìœ¨", "ì„¤ë¹„íš¨ìœ¨", "ìƒì‚°íš¨ìœ¨", "ë°œì „íš¨ìœ¨", "ì „ë ¥íš¨ìœ¨",
                "ë³€í™˜íš¨ìœ¨", "ì „ë ¥ë³€í™˜", "ì†ì‹¤", "ì „ë ¥ì†ì‹¤", "ì†ì‹¤ê°ì†Œ",
                "ì—´íš¨ìœ¨", "ëƒ‰ê°íš¨ìœ¨", "ì‹œìŠ¤í…œíš¨ìœ¨", "í†µí•©íš¨ìœ¨",
                
                # ğŸ”§ ê¸°ìˆ ì  ê°œì„ 
                "ê¸°ìˆ ê°œë°œ", "ê°œì„ ", "ê°œëŸ‰", "ì—…ê·¸ë ˆì´ë“œ", "modernization",
                "í˜ì‹ ", "ê¸°ìˆ í˜ì‹ ", "ê³µì •ê°œì„ ", "ì‹œìŠ¤í…œê°œì„ ", "ì„¤ë¹„ê°œì„ ",
                "ìŠ¤ë§ˆíŠ¸í™”", "ë””ì§€í„¸í™”", "ìë™í™”", "ì§€ëŠ¥í™”", "AIë„ì…",
                
                # ğŸ“Š ì„±ê³¼/ì¸¡ì •
                "ì—ë„ˆì§€ì„±ê³¼", "íš¨ìœ¨ë“±ê¸‰", "íš¨ìœ¨ì¸ì¦", "ì—ë„ˆì§€ë¼ë²¨",
                "ì ˆê°", "ì ˆê°íš¨ê³¼", "ë¹„ìš©ì ˆê°", "ì›ê°€ì ˆê°", "ìš´ì˜ë¹„ì ˆê°",
                "ROI", "íˆ¬ìíšŒìˆ˜", "ê²½ì œì„±", "ìˆ˜ìµì„±"
            ],
            
            "ì§€ì—­ì‚¬íšŒê³µí—Œ": [
                # ğŸ˜ï¸ ê¸°ë³¸ ì§€ì—­ì‚¬íšŒ í‚¤ì›Œë“œ
                "ì§€ì—­ì‚¬íšŒ", "ì§€ì—­", "ì§€ì—­ê²½ì œ", "ì§€ì—­ë°œì „", "ì§€ì—­ìƒìƒ",
                "ì‚¬íšŒê³µí—Œ", "CSR", "ê³µí—Œ", "ê¸°ì—¬", "ì°¸ì—¬", "í˜‘ë ¥",
                "ì§€ì—­í˜‘ë ¥", "ìƒìƒí˜‘ë ¥", "ë™ë°˜ì„±ì¥", "ìƒìƒë°œì „",
                
                # ğŸ’ êµ¬ì²´ì  ê³µí—Œ í™œë™
                "ê¸°ë¶€", "í›„ì›", "ì¥í•™ê¸ˆ", "ë´‰ì‚¬", "ìì›ë´‰ì‚¬", "ë‚˜ëˆ”",
                "ì‚¬íšŒë´‰ì‚¬", "ì§€ì—­ë´‰ì‚¬", "í™˜ê²½ë´‰ì‚¬", "ë³µì§€", "ì‚¬íšŒë³µì§€",
                "ë¬¸í™”", "ë¬¸í™”ì§€ì›", "ìŠ¤í¬ì¸ ", "ì²´ìœ¡", "êµìœ¡", "êµìœ¡ì§€ì›",
                
                # ğŸ‘¥ ì§€ì—­ ì´í•´ê´€ê³„ì
                "ì£¼ë¯¼", "ì§€ì—­ì£¼ë¯¼", "ì‹œë¯¼", "ì§€ì—­ì‹œë¯¼", "ìì¹˜ë‹¨ì²´",
                "ì§€ë°©ì •ë¶€", "ì§€ìì²´", "ì‹œì²­", "êµ°ì²­", "êµ¬ì²­", "ë™ì‚¬ë¬´ì†Œ",
                "ì§€ì—­ê¸°ê´€", "ì§€ì—­ë‹¨ì²´", "ì‹œë¯¼ë‹¨ì²´", "NGO", "NPO",
                
                # ğŸ’¼ ì§€ì—­ ê²½ì œ/ê³ ìš©
                "ì¼ìë¦¬", "ì§€ì—­ì¼ìë¦¬", "ê³ ìš©ì°½ì¶œ", "ì·¨ì—…", "ì°½ì—…ì§€ì›",
                "ì¤‘ì†Œê¸°ì—…", "ì§€ì—­ê¸°ì—…", "í˜‘ë ¥ì—…ì²´", "ì§€ì—­ìƒê¶Œ", "ìƒê¶Œí™œì„±í™”",
                "íˆ¬ì", "ì§€ì—­íˆ¬ì", "ì‹œì„¤íˆ¬ì", "ì¸í”„ë¼", "ì§€ì—­ì¸í”„ë¼"
            ],
            
            "ìš©ìˆ˜ê´€ë¦¬": [
                # ğŸ’§ ê¸°ë³¸ ìš©ìˆ˜/ë¬¼ ê´€ë¦¬ í‚¤ì›Œë“œ
                "ìš©ìˆ˜", "ë¬¼", "ìˆ˜ìì›", "ê¸‰ìˆ˜", "ê¸‰ìˆ˜ì‹œì„¤", "ìƒìˆ˜ë„",
                "ê³µì—…ìš©ìˆ˜", "ëƒ‰ê°ìˆ˜", "ë³´ì¼ëŸ¬ìš©ìˆ˜", "ìš©ìˆ˜ê³µê¸‰", "ë¬¼ê³µê¸‰",
                "ìˆ˜ì§ˆ", "ìˆ˜ì§ˆê´€ë¦¬", "ìˆ˜ì§ˆê°œì„ ", "ìˆ˜ì§ˆì˜¤ì—¼", "ì˜¤ì—¼ë°©ì§€",
                
                # ğŸ­ ì‚°ì—… ìš©ìˆ˜ (ë°œì „ì†Œ íŠ¹í™”)
                "ëƒ‰ê°ìˆ˜", "ìˆœí™˜ëƒ‰ê°ìˆ˜", "ëƒ‰ê°íƒ‘", "ë³µìˆ˜ê¸°", "ì·¨ìˆ˜", "ë°©ë¥˜",
                "ì˜¨ë°°ìˆ˜", "ì—´ì˜¤ì—¼", "ìˆ˜ì˜¨", "ìˆ˜ì˜¨ìƒìŠ¹", "ìƒíƒœê³„ì˜í–¥",
                "ì–´ë¥˜", "í•´ì–‘ìƒë¬¼", "í•´ì–‘ìƒíƒœê³„", "ë‹´ìˆ˜ìƒíƒœê³„",
                
                # ğŸ”„ ìš©ìˆ˜ ì²˜ë¦¬/ì¬í™œìš©
                "íìˆ˜", "íìˆ˜ì²˜ë¦¬", "í•˜ìˆ˜ì²˜ë¦¬", "ì •ìˆ˜", "ì •ìˆ˜ì²˜ë¦¬",
                "ì¬í™œìš©", "ì¬ì´ìš©", "ë¬¼ì¬í™œìš©", "ì¤‘ìˆ˜ë„", "ë¹—ë¬¼í™œìš©",
                "ì ˆì•½", "ë¬¼ì ˆì•½", "ì ˆìˆ˜", "ìš©ìˆ˜ì ˆì•½", "íš¨ìœ¨ì ì‚¬ìš©",
                
                # ğŸ“‹ ê´€ë ¨ ê·œì œ/ê¸°ì¤€
                "í™˜ê²½ê¸°ì¤€", "ë°°ì¶œê¸°ì¤€", "ìˆ˜ì§ˆê¸°ì¤€", "í—ˆê°€", "ì·¨ìˆ˜í—ˆê°€",
                "ë°©ë¥˜í—ˆê°€", "í™˜ê²½ì˜í–¥í‰ê°€", "ëª¨ë‹ˆí„°ë§", "ìˆ˜ì§ˆê²€ì‚¬",
                "ê·œì œ", "í™˜ê²½ê·œì œ", "ë¬¼í™˜ê²½ë³´ì „ë²•", "ìˆ˜ì§ˆë³´ì „"
            ]
        }
        
        # ğŸ¯ íšŒì‚¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ (ê°„ë‹¨í•˜ê³  ê´‘ë²”ìœ„í•˜ê²Œ)
        self.company_keywords = {
            "ë‘ì‚°í“¨ì–¼ì…€": [
                # ğŸ¢ ê¸°ì—… ì‹ë³„
                "ë‘ì‚°", "ë‘ì‚°í“¨ì–¼ì…€", "doosan", "fuel cell", "DFCL",
                
                # âš¡ í•µì‹¬ ì‚¬ì—… (ê°„ë‹¨í•œ ìš©ì–´)
                "ì—°ë£Œì „ì§€", "ìˆ˜ì†Œ", "ìˆ˜ì†Œì—°ë£Œì „ì§€", "ìˆ˜ì†Œë°œì „", "ìˆ˜ì†Œê²½ì œ",
                "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ìˆ˜ì†Œì‚¬ì—…", "ìˆ˜ì†Œê¸°ìˆ ", "ì—°ë£Œì „ì§€ì‚¬ì—…",
                "ë°œì „", "ë°œì „ì‚¬ì—…", "ì „ë ¥", "ì „ë ¥ìƒì‚°", "ì—ë„ˆì§€", "ì²­ì •ì—ë„ˆì§€",
                
                # ğŸ”‹ ì œí’ˆ/ê¸°ìˆ  (ì¼ë°˜ì  í‘œí˜„)
                "ì—°ë£Œì „ì§€ì‹œìŠ¤í…œ", "ë°œì „ì‹œìŠ¤í…œ", "ì—ë„ˆì§€ì‹œìŠ¤í…œ", "ì „ë ¥ì‹œìŠ¤í…œ",
                "ê°€ì •ìš©", "ê±´ë¬¼ìš©", "ë°œì „ìš©", "ëŒ€ìš©ëŸ‰", "ë¶„ì‚°ì „ì›", "ë§ˆì´í¬ë¡œê·¸ë¦¬ë“œ",
                "íš¨ìœ¨", "ê³ íš¨ìœ¨", "ì„±ëŠ¥", "ë‚´êµ¬ì„±", "ìˆ˜ëª…", "ì•ˆì •ì„±",
                
                # ğŸŒ± ê´€ë ¨ ë¶„ì•¼
                "ì¹œí™˜ê²½", "ì²­ì •", "ë¬´ê³µí•´", "ì œë¡œë°°ì¶œ", "íƒ„ì†Œì¤‘ë¦½", "ESG",
                "ì‹ ì¬ìƒì—ë„ˆì§€", "ì¬ìƒì—ë„ˆì§€", "ì—ë„ˆì§€ì „í™˜", "ê·¸ë¦°ë‰´ë”œ"
            ],
            
            "LS ELECTRIC": [
                # ğŸ¢ ê¸°ì—… ì‹ë³„
                "LS", "LSì¼ë ‰íŠ¸ë¦­", "LS ELECTRIC", "ì—˜ì—ìŠ¤", "ì—˜ì—ìŠ¤ì¼ë ‰íŠ¸ë¦­",
                
                # âš¡ í•µì‹¬ ì‚¬ì—… (ê°„ë‹¨í•œ ìš©ì–´)
                "ì „ë ¥", "ì „ë ¥ê¸°ê¸°", "ì „ë ¥ì„¤ë¹„", "ì „ê¸°", "ì „ê¸°ì„¤ë¹„",
                "ë³€ì••ê¸°", "ì°¨ë‹¨ê¸°", "ê°œíê¸°", "ë°°ì „", "ì†¡ì „", "ìˆ˜ë°°ì „",
                "ì „ë ¥ë³€í™˜", "ì¸ë²„í„°", "ì»¨ë²„í„°", "ESS", "ì—ë„ˆì§€ì €ì¥",
                
                # ğŸ”§ ì œí’ˆ/ê¸°ìˆ  (ì¼ë°˜ì  í‘œí˜„)
                "ì „ë ¥ì‹œìŠ¤í…œ", "ë°°ì „ì‹œìŠ¤í…œ", "ì œì–´ì‹œìŠ¤í…œ", "ìë™í™”ì‹œìŠ¤í…œ",
                "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ê·¸ë¦¬ë“œ", "ì „ë ¥ë§", "ì „ë ¥ê³„í†µ", "ê³„í†µì—°ê³„",
                "ë°˜ë„ì²´", "íŒŒì›Œë°˜ë„ì²´", "ì „ë ¥ìš©ë°˜ë„ì²´", "ëª¨ë“ˆ", "ì „ìë¶€í’ˆ",
                
                # ğŸ­ ì ìš© ë¶„ì•¼
                "ì‚°ì—…ìë™í™”", "ê³µì¥ìë™í™”", "ë¹Œë”©ìë™í™”", "ì² ë„", "ì „ì² ",
                "ì‹ ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥", "ë°°í„°ë¦¬", "ì „ê¸°ì°¨", "EV",
                "ë°ì´í„°ì„¼í„°", "í†µì‹ ", "IT", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬"
            ],
            
            "í•œêµ­ì¤‘ë¶€ë°œì „": [
                # ğŸ¢ ê¸°ì—… ì‹ë³„  
                "ì¤‘ë¶€ë°œì „", "í•œêµ­ì¤‘ë¶€ë°œì „", "KOMIPO", "ì¤‘ë°œ", "ì¤‘ë¶€ë°œì „ì†Œ",
                
                # âš¡ í•µì‹¬ ì‚¬ì—… (ê°„ë‹¨í•œ ìš©ì–´)
                "ë°œì „", "ë°œì „ì†Œ", "ì „ë ¥", "ì „ë ¥ìƒì‚°", "ì „ë ¥ê³µê¸‰", "ì „ê¸°",
                "í™”ë ¥ë°œì „", "ì„íƒ„í™”ë ¥", "LNGë°œì „", "ê°€ìŠ¤ë°œì „", "ë³µí•©í™”ë ¥",
                "ì—´ë³‘í•©", "ì§‘ë‹¨ì—ë„ˆì§€", "ë°œì „ì‚¬ì—…", "ì „ë ¥ì‚¬ì—…",
                
                # ğŸŒ± ì¹œí™˜ê²½ ì „í™˜
                "ì‹ ì¬ìƒì—ë„ˆì§€", "ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥", "ì—°ë£Œì „ì§€",
                "ESS", "ì—ë„ˆì§€ì €ì¥", "ê·¸ë¦°ìˆ˜ì†Œ", "ìˆ˜ì†Œë°œì „", "ì•”ëª¨ë‹ˆì•„ë°œì „",
                "íƒ„ì†Œì¤‘ë¦½", "ì¹œí™˜ê²½", "ì²­ì •ë°œì „", "ì—ë„ˆì§€ì „í™˜", "íƒˆì„íƒ„",
                
                # ğŸ­ ì‹œì„¤/ì„¤ë¹„ (ì¼ë°˜ì  í‘œí˜„)
                "ë°œì „ì„¤ë¹„", "ë°œì „ê¸°", "í„°ë¹ˆ", "ë³´ì¼ëŸ¬", "ë³€ì••ê¸°", "ì†¡ì „",
                "ë°œì „ë‹¨ì§€", "ë°œì „ì†Œë¶€ì§€", "ì‹œì„¤", "ì„¤ë¹„", "ì¸í”„ë¼",
                "ì •ë¹„", "ë³´ìˆ˜", "ì ê²€", "ê°œì„ ", "ì—…ê·¸ë ˆì´ë“œ", "í˜„ëŒ€í™”",
                
                # ğŸŒ í™˜ê²½/ì•ˆì „
                "í™˜ê²½", "í™˜ê²½ì„¤ë¹„", "ëŒ€ê¸°ì˜¤ì—¼", "ë¯¸ì„¸ë¨¼ì§€", "ë°°ì¶œ", "ë°°ì¶œì €ê°",
                "í™˜ê²½ê°œì„ ", "í™˜ê²½íˆ¬ì", "ì•ˆì „", "ì•ˆì „ê´€ë¦¬", "ì‚¬ê³ ", "ì˜ˆë°©",
                "ì˜¨ì‹¤ê°€ìŠ¤", "ì´ì‚°í™”íƒ„ì†Œ", "ì§ˆì†Œì‚°í™”ë¬¼", "í™©ì‚°í™”ë¬¼",
                
                # ğŸ‘¥ ì§€ì—­/ì‚¬íšŒ
                "ì§€ì—­", "ì§€ì—­ì‚¬íšŒ", "ìƒìƒ", "ì§€ì—­ìƒìƒ", "ì£¼ë¯¼", "ì§€ì—­ì£¼ë¯¼",
                "ì§€ì—­ê²½ì œ", "ì¼ìë¦¬", "ê³ ìš©", "íˆ¬ì", "í˜‘ë ¥", "ìƒìƒí˜‘ë ¥"
            ]
        }
        
        # ğŸ¯ ì¼ë°˜ì ì¸ ESG/ë¹„ì¦ˆë‹ˆìŠ¤ í‚¤ì›Œë“œ ì¶”ê°€ (ê´‘ë²”ìœ„í•œ ë§¤ì¹­ìš©)
        self.general_esg_keywords = [
            # í™˜ê²½ (E)
            "í™˜ê²½", "ì¹œí™˜ê²½", "ê·¸ë¦°", "ì²­ì •", "ì—ì½”", "ì§€ì†ê°€ëŠ¥", "ESG",
            "ê¸°í›„", "íƒ„ì†Œ", "ì˜¨ì‹¤ê°€ìŠ¤", "ë°°ì¶œ", "ì˜¤ì—¼", "ì ˆì•½", "íš¨ìœ¨",
            "ì¬ìƒì—ë„ˆì§€", "ì‹ ì¬ìƒ", "ì—ë„ˆì§€", "ì „ë ¥", "ë°œì „", "ìˆ˜ì†Œ", "ë°°í„°ë¦¬",
            
            # ì‚¬íšŒ (S)  
            "ì•ˆì „", "ì‚¬ê³ ", "ì¬í•´", "ê³ ìš©", "ì¼ìë¦¬", "êµìœ¡", "í›ˆë ¨",
            "ì§€ì—­ì‚¬íšŒ", "ìƒìƒ", "í˜‘ë ¥", "ê¸°ë¶€", "ë´‰ì‚¬", "ê³µí—Œ", "ë³µì§€",
            "ì¸ê¶Œ", "ë‹¤ì–‘ì„±", "í¬ìš©", "ìƒìƒ", "ë™ë°˜ì„±ì¥",
            
            # ê±°ë²„ë„ŒìŠ¤ (G)
            "ê±°ë²„ë„ŒìŠ¤", "ìœ¤ë¦¬", "íˆ¬ëª…", "ì»´í”Œë¼ì´ì–¸ìŠ¤", "ì¤€ë²•", "ë°˜ë¶€íŒ¨",
            "ì´ì‚¬íšŒ", "ê°ì‚¬", "ë‚´ë¶€í†µì œ", "ë¦¬ìŠ¤í¬", "ìœ„í—˜ê´€ë¦¬"
        ]
        
        # ğŸ”§ ê°€ì¤‘ì¹˜ ì„¤ì • (ë§¤ì¹­ë¥  í–¥ìƒì„ ìœ„í•´ ì¡°ì •)
        self.weights = {
            'title_match': 3.0,          # ì œëª© ë§¤ì¹­ (ì ë‹¹íˆ ë†’ê²Œ)
            'content_match': 1.5,        # ë³¸ë¬¸ ë§¤ì¹­ (ê¸°ë³¸ê°’)
            'company_exact_match': 5.0,  # íšŒì‚¬ëª… ì •í™• ë§¤ì¹­ (ë§¤ìš° ë†’ê²Œ)
            'company_partial_match': 2.0, # íšŒì‚¬ëª… ë¶€ë¶„ ë§¤ì¹­
            'keyword_exact_match': 2.0,  # í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­
            'keyword_partial_match': 1.0, # í‚¤ì›Œë“œ ë¶€ë¶„ ë§¤ì¹­ (ì¶”ê°€)
            'general_esg_match': 0.8,    # ì¼ë°˜ ESG í‚¤ì›Œë“œ ë§¤ì¹­ (ë‚®ê²Œ)
            'sentiment_positive': 1.2,   # ê¸ì • sentiment ê°€ì¤‘ì¹˜
            'sentiment_negative': 0.9,   # ë¶€ì • sentiment ê°€ì¤‘ì¹˜
            'recent_news': 1.3,          # ìµœê·¼ ë‰´ìŠ¤ ê°€ì¤‘ì¹˜
            'keyword_density': 1.8,      # í‚¤ì›Œë“œ ë°€ë„ ê°€ì¤‘ì¹˜
            'multiple_keyword_bonus': 1.5 # ë³µìˆ˜ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ì¶”ê°€)
        }
        
        # ğŸ¯ ê´€ë ¨ì„± ì„ê³„ê°’ ëŒ€í­ í•˜í–¥ ì¡°ì • (ë§¤ì¹­ë¥  í–¥ìƒ)
        self.relevance_threshold = 0.1  # 0.3 â†’ 0.1ë¡œ ëŒ€í­ í•˜í–¥
    
    def analyze_news_for_materiality(
        self,
        news_articles: List[Dict[str, Any]],
        materiality_topics: List[MaterialityTopic],
        company_name: str
    ) -> Dict[str, Dict[str, Any]]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¤‘ëŒ€ì„± í‰ê°€ í† í”½ë³„ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            news_articles: sasb-serviceì—ì„œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            materiality_topics: ì¤‘ëŒ€ì„± í‰ê°€ í† í”½ ë¦¬ìŠ¤íŠ¸
            company_name: ê¸°ì—…ëª…
            
        Returns:
            Dict[str, Dict[str, Any]]: í† í”½ë³„ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info(f"ğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {len(news_articles)}ê°œ ê¸°ì‚¬, {len(materiality_topics)}ê°œ í† í”½")
        
        analysis_results = {}
        
        for topic in materiality_topics:
            topic_name = topic.topic_name
            
            # 1. ğŸ¯ ê°•í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ (í† í”½ + íšŒì‚¬ íŠ¹í™”)
            related_keywords = self.extract_enhanced_keywords(topic, company_name)
            
            # 2. ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ ë° ì ìˆ˜ ê³„ì‚°
            topic_news_analysis = self._analyze_topic_news(
                news_articles, topic_name, related_keywords, company_name
            )
            
            # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            comprehensive_score = self._calculate_comprehensive_score(
                topic_news_analysis['articles'], related_keywords
            )
            
            # 4. íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis = self._analyze_news_trend(topic_news_analysis['articles'])
            
            analysis_results[topic_name] = {
                'topic_name': topic_name,
                'related_keywords': related_keywords,
                'total_news_count': len(topic_news_analysis['articles']),
                'relevant_news_count': topic_news_analysis['relevant_count'],
                'comprehensive_score': comprehensive_score,
                'trend_analysis': trend_analysis,
                'top_articles': topic_news_analysis['top_articles'][:5],  # ìƒìœ„ 5ê°œ ê¸°ì‚¬
                'sentiment_distribution': topic_news_analysis['sentiment_distribution'],
                # ğŸ¯ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                'debug_info': {
                    'keyword_count': len(related_keywords),
                    'sample_keywords': related_keywords[:10],  # ìƒ˜í”Œ í‚¤ì›Œë“œ
                    'relevance_threshold': self.relevance_threshold,
                    'top_matched_articles': [
                        {
                            'title': art['article'].get('title', '')[:100],
                            'relevance_score': art['relevance_score'],
                            'matched_keywords': art['matched_keywords'][:5]
                        }
                        for art in topic_news_analysis['top_articles'][:3]
                    ]
                }
            }
            
            self.logger.info(f"âœ… {topic_name} ë¶„ì„ ì™„ë£Œ: {topic_news_analysis['relevant_count']}/{len(news_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ (ì ìˆ˜: {comprehensive_score:.3f})")
        
        self.logger.info(f"ğŸ“ˆ ì „ì²´ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ í† í”½")
        return analysis_results
    
    def extract_enhanced_keywords(
        self, 
        topic: MaterialityTopic, 
        company_name: Optional[str] = None
    ) -> List[str]:
        """ğŸ¯ ê°•í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ - í† í”½ + íšŒì‚¬ íŠ¹í™” í‚¤ì›Œë“œ ì¡°í•©"""
        keywords = []
        topic_name = topic.topic_name
        
        # 1. ğŸ¯ í† í”½ë³„ í‚¤ì›Œë“œ ì‚¬ì „ì—ì„œ ë§¤ì¹­
        if topic_name in self.topic_keyword_dict:
            topic_keywords = self.topic_keyword_dict[topic_name]
            keywords.extend(topic_keywords)
            self.logger.info(f"ğŸ“‹ {topic_name}: {len(topic_keywords)}ê°œ í† í”½ í‚¤ì›Œë“œ ì¶”ê°€")
        
        # 2. ğŸ¯ íšŒì‚¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ê°€ (MVP ëŒ€ìƒ ê¸°ì—…)
        if company_name and company_name in self.company_keywords:
            company_keywords = self.company_keywords[company_name]
            keywords.extend(company_keywords)
            self.logger.info(f"ğŸ¢ {company_name}: {len(company_keywords)}ê°œ íšŒì‚¬ í‚¤ì›Œë“œ ì¶”ê°€")
        
        # 3. í† í”½ëª… ê¸°ë°˜ ìœ ì‚¬ì„± ë§¤ì¹­
        for dict_topic, dict_keywords in self.topic_keyword_dict.items():
            if topic_name != dict_topic:
                similarity = self._calculate_topic_similarity(topic_name, dict_topic)
                if similarity > 0.5:
                    keywords.extend(dict_keywords[:8])  # ìƒìœ„ 8ê°œë§Œ
                    self.logger.info(f"ğŸ”— ìœ ì‚¬ í† í”½ {dict_topic}: {similarity:.2f} ìœ ì‚¬ì„±, {len(dict_keywords[:8])}ê°œ í‚¤ì›Œë“œ ì¶”ê°€")
        
        # 4. ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë³´ì™„)
        basic_keywords = self._extract_keywords_from_text(topic_name)
        keywords.extend(basic_keywords)
        
        # 5. SASB ë§¤í•‘ í‚¤ì›Œë“œ (ë³´ì™„)
        try:
            sasb_keywords = self.mapping_service.find_related_keywords(topic_name)
            keywords.extend(sasb_keywords.get('industry_keywords', [])[:5])
            keywords.extend(sasb_keywords.get('sasb_keywords', [])[:5])
        except Exception as e:
            self.logger.warning(f"SASB í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 6. ì¤‘ë³µ ì œê±° ë° ì •ì œ
        unique_keywords = list(set(keywords))
        cleaned_keywords = [kw.strip() for kw in unique_keywords if len(kw.strip()) > 1]
        
        self.logger.info(f"âœ… {topic_name} ({company_name or 'N/A'}): ì´ {len(cleaned_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
        return cleaned_keywords
    
    def _extract_topic_keywords(self, topic: MaterialityTopic) -> List[str]:
        """í† í”½ í‚¤ì›Œë“œ ì¶”ì¶œ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
        return self.extract_enhanced_keywords(topic, None)
    
    def _calculate_topic_similarity(self, topic1: str, topic2: str) -> float:
        """í† í”½ëª… ê°„ ìœ ì‚¬ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²¹ì¹¨ ê¸°ë°˜ ìœ ì‚¬ì„± ê³„ì‚°
        words1 = set(topic1.split())
        words2 = set(topic2.split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
        keywords = []
        
        # 1. ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        words = text.split()
        keywords.extend([word.strip() for word in words if len(word.strip()) > 1])
        
        # 2. íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ì •ì œ
        cleaned_keywords = []
        for keyword in keywords:
            # íŠ¹ìˆ˜ ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)
            cleaned = re.sub(r'[^\w\sê°€-í£]', '', keyword)
            if len(cleaned) > 1:
                cleaned_keywords.append(cleaned)
        
        return cleaned_keywords
    
    def _analyze_topic_news(
        self,
        news_articles: List[Dict[str, Any]],
        topic_name: str,
        related_keywords: List[str],
        company_name: str
    ) -> Dict[str, Any]:
        """í† í”½ë³„ ë‰´ìŠ¤ ë¶„ì„"""
        relevant_articles = []
        sentiment_distribution = {'positive': 0, 'negative': 0, 'neutral': 0}
        
        for article in news_articles:
            # 1. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            relevance_score = self._calculate_article_relevance(
                article, related_keywords, company_name
            )
            
            # 2. ê´€ë ¨ì„± ì„ê³„ê°’ ì ìš©
            if relevance_score > self.relevance_threshold:  # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ê°’
                article_analysis = {
                    'article': article,
                    'relevance_score': relevance_score,
                    'matched_keywords': self._find_matched_keywords(article, related_keywords)
                }
                relevant_articles.append(article_analysis)
                
                # 3. sentiment ë¶„í¬ ì—…ë°ì´íŠ¸
                sentiment = article.get('sentiment', 'neutral')
                if sentiment in sentiment_distribution:
                    sentiment_distribution[sentiment] += 1
        
        # 4. ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        relevant_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        return {
            'articles': relevant_articles,
            'relevant_count': len(relevant_articles),
            'top_articles': relevant_articles[:10],  # ìƒìœ„ 10ê°œ
            'sentiment_distribution': sentiment_distribution
        }
    
    def _calculate_article_relevance(
        self,
        article: Dict[str, Any],
        keywords: List[str],
        company_name: str
    ) -> float:
        """ğŸ¯ ëŒ€í­ ê°œì„ ëœ ê¸°ì‚¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê´‘ë²”ìœ„í•œ ë§¤ì¹­)"""
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '') or article.get('description', '')
        sentiment = article.get('sentiment', 'neutral')
        published_at = article.get('published_at', '')
        
        total_score = 0.0
        
        # 1. ğŸ¢ íšŒì‚¬ëª… ë§¤ì¹­ (ëŒ€í­ ê°•í™”)
        company_score = self._calculate_company_relevance(title, content, company_name)
        total_score += company_score
        
        # 2. ğŸ¯ í† í”½ í‚¤ì›Œë“œ ë§¤ì¹­
        keyword_score = self._calculate_keyword_relevance(title, content, keywords)
        total_score += keyword_score
        
        # 3. ğŸŒ ì¼ë°˜ ESG í‚¤ì›Œë“œ ë§¤ì¹­ (ê¸°ë³¸ì ì¸ ESG ê´€ë ¨ì„±)
        esg_score = self._calculate_esg_relevance(title, content)
        total_score += esg_score
        
        # 4. ğŸ’¯ ë³µìˆ˜ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        multiple_keyword_bonus = self._calculate_multiple_keyword_bonus(title, content, keywords)
        total_score += multiple_keyword_bonus
        
        # 5. ğŸ˜Š sentiment ê°€ì¤‘ì¹˜ ì ìš©
        if sentiment == 'positive':
            total_score *= self.weights['sentiment_positive']
        elif sentiment == 'negative':
            total_score *= self.weights['sentiment_negative']
        
        # 6. ğŸ“… ìµœê·¼ì„± ê°€ì¤‘ì¹˜ ì ìš©
        if self._is_recent_news(published_at):
            total_score *= self.weights['recent_news']
        
        # 7. ğŸ“Š í‚¤ì›Œë“œ ë°€ë„ ê°€ì¤‘ì¹˜
        keyword_density = self._calculate_keyword_density(title + ' ' + content, keywords)
        total_score += keyword_density * self.weights['keyword_density']
        
        return max(0.0, total_score)  # ìŒìˆ˜ ë°©ì§€
    
    def _calculate_company_relevance(self, title: str, content: str, company_name: str) -> float:
        """ğŸ¢ íšŒì‚¬ëª… ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê°•í™”ëœ ë²„ì „)"""
        if not company_name:
            return 0.0
        
        score = 0.0
        full_text = (title + ' ' + content).lower()
        
        # íšŒì‚¬ë³„ ë‹¤ì–‘í•œ í‘œí˜„ ì •ì˜
        company_variations = {
            "ë‘ì‚°í“¨ì–¼ì…€": ["ë‘ì‚°", "ë‘ì‚°í“¨ì–¼ì…€", "doosan", "fuel cell", "dfcl", "í“¨ì–¼ì…€", "ì—°ë£Œì „ì§€"],
            "LS ELECTRIC": ["ls", "lsì¼ë ‰íŠ¸ë¦­", "ls electric", "ì—˜ì—ìŠ¤", "ì—˜ì—ìŠ¤ì¼ë ‰íŠ¸ë¦­", "lsì „ê¸°"],
            "í•œêµ­ì¤‘ë¶€ë°œì „": ["ì¤‘ë¶€ë°œì „", "í•œêµ­ì¤‘ë¶€ë°œì „", "komipo", "ì¤‘ë°œ", "ì¤‘ë¶€ë°œì „ì†Œ", "í•œì „ì¤‘ë¶€ë°œì „"]
        }
        
        # 1. ì •í™•í•œ íšŒì‚¬ëª… ë§¤ì¹­
        if company_name.lower() in full_text:
            score += self.weights['company_exact_match']
        
        # 2. íšŒì‚¬ ë³€í˜• í‘œí˜„ ë§¤ì¹­
        if company_name in company_variations:
            for variation in company_variations[company_name]:
                if variation.lower() in full_text:
                    score += self.weights['company_partial_match']
                    break  # ì¤‘ë³µ ì ìˆ˜ ë°©ì§€
        
        # 3. ì œëª©ì—ì„œ íšŒì‚¬ëª… ë§¤ì¹­ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
        title_lower = title.lower()
        if company_name.lower() in title_lower:
            score += self.weights['company_exact_match'] * 0.5  # ì¶”ê°€ ë³´ë„ˆìŠ¤
        
        return score
    
    def _calculate_keyword_relevance(self, title: str, content: str, keywords: List[str]) -> float:
        """ğŸ¯ í‚¤ì›Œë“œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not keywords:
            return 0.0
        
        score = 0.0
        
        # 1. ì œëª©ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
        title_exact = self._count_exact_keyword_matches(title, keywords)
        title_partial = self._count_partial_keyword_matches(title, keywords)
        score += title_exact * self.weights['title_match'] * self.weights['keyword_exact_match']
        score += title_partial * self.weights['title_match'] * self.weights['keyword_partial_match']
        
        # 2. ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
        content_exact = self._count_exact_keyword_matches(content, keywords)
        content_partial = self._count_partial_keyword_matches(content, keywords)
        score += content_exact * self.weights['content_match'] * self.weights['keyword_exact_match']
        score += content_partial * self.weights['content_match'] * self.weights['keyword_partial_match']
        
        return score
    
    def _calculate_esg_relevance(self, title: str, content: str) -> float:
        """ğŸŒ ì¼ë°˜ ESG í‚¤ì›Œë“œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not hasattr(self, 'general_esg_keywords') or not self.general_esg_keywords:
            return 0.0
        
        full_text = (title + ' ' + content).lower()
        esg_matches = 0
        
        for esg_keyword in self.general_esg_keywords:
            if esg_keyword.lower() in full_text:
                esg_matches += 1
        
        return esg_matches * self.weights['general_esg_match']
    
    def _calculate_multiple_keyword_bonus(self, title: str, content: str, keywords: List[str]) -> float:
        """ğŸ’¯ ë³µìˆ˜ í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚°"""
        if not keywords or len(keywords) < 2:
            return 0.0
        
        full_text = (title + ' ' + content).lower()
        matched_keywords = []
        
        for keyword in keywords:
            if keyword.lower() in full_text:
                matched_keywords.append(keyword)
        
        # ë³µìˆ˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œ ë³´ë„ˆìŠ¤ ì ìˆ˜
        if len(matched_keywords) >= 2:
            bonus_multiplier = min(len(matched_keywords) / len(keywords), 0.5)  # ìµœëŒ€ 50% ë³´ë„ˆìŠ¤
            return bonus_multiplier * self.weights['multiple_keyword_bonus']
        
        return 0.0
    
    def _count_keyword_matches(self, text: str, keywords: List[str]) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚°"""
        if not text:
            return 0
        
        text_lower = text.lower()
        matches = 0
        
        for keyword in keywords:
            if keyword.lower() in text_lower:
                matches += 1
        
        return matches
    
    def _count_exact_keyword_matches(self, text: str, keywords: List[str]) -> int:
        """ğŸ¯ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚° (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤)"""
        if not text:
            return 0
        
        text_lower = text.lower()
        matches = 0
        
        import re
        for keyword in keywords:
            # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ë§¤ì¹­
            pattern = r'\b' + re.escape(keyword.lower()) + r'\b'
            if re.search(pattern, text_lower):
                matches += 1
        
        return matches
    
    def _count_partial_keyword_matches(self, text: str, keywords: List[str]) -> int:
        """ğŸ¯ ë¶€ë¶„ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚° (í¬í•¨ ê´€ê³„)"""
        if not text:
            return 0
        
        text_lower = text.lower()
        exact_matches = self._count_exact_keyword_matches(text, keywords)
        total_matches = self._count_keyword_matches(text, keywords)
        
        # ë¶€ë¶„ ë§¤ì¹­ = ì „ì²´ ë§¤ì¹­ - ì •í™•í•œ ë§¤ì¹­
        return max(0, total_matches - exact_matches)
    
    def _find_matched_keywords(self, article: Dict[str, Any], keywords: List[str]) -> List[str]:
        """ê¸°ì‚¬ì—ì„œ ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì°¾ê¸°"""
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '')
        full_text = (title + ' ' + content).lower()
        
        matched = []
        for keyword in keywords:
            if keyword.lower() in full_text:
                matched.append(keyword)
        
        return matched
    
    def _is_recent_news(self, published_at: str) -> bool:
        """ìµœê·¼ ë‰´ìŠ¤ì¸ì§€ í™•ì¸ (30ì¼ ì´ë‚´)"""
        try:
            if not published_at:
                return False
            
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            import dateutil.parser
            pub_date = dateutil.parser.parse(published_at)
            now = datetime.now()
            days_diff = (now - pub_date).days
            
            return days_diff <= 30
        except:
            return False
    
    def _calculate_keyword_density(self, text: str, keywords: List[str]) -> float:
        """í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°"""
        if not text or not keywords:
            return 0.0
        
        text_lower = text.lower()
        total_words = len(text_lower.split())
        
        if total_words == 0:
            return 0.0
        
        keyword_count = 0
        for keyword in keywords:
            keyword_count += text_lower.count(keyword.lower())
        
        return keyword_count / total_words
    
    def _calculate_comprehensive_score(
        self,
        articles: List[Dict[str, Any]],
        keywords: List[str]
    ) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        if not articles:
            return 0.0
        
        # 1. í‰ê·  ê´€ë ¨ì„± ì ìˆ˜
        avg_relevance = sum(article['relevance_score'] for article in articles) / len(articles)
        
        # 2. ë‰´ìŠ¤ ê°œìˆ˜ ê°€ì¤‘ì¹˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        news_count_weight = math.log(len(articles) + 1)
        
        # 3. í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¹„ìœ¨)
        if keywords:
            all_matched_keywords = set()
            for article in articles:
                all_matched_keywords.update(article.get('matched_keywords', []))
            keyword_coverage = len(all_matched_keywords) / len(keywords)
        else:
            keyword_coverage = 0.0
        
        # 4. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        comprehensive_score = (
            avg_relevance * 0.4 +
            news_count_weight * 0.3 +
            keyword_coverage * 0.3
        )
        
        return round(comprehensive_score, 3)
    
    def _analyze_news_trend(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„"""
        if not articles:
            return {
                'trend_direction': 'stable',
                'recent_increase': False,
                'peak_period': None,
                'avg_sentiment': 'neutral'
            }
        
        # 1. ë‚ ì§œë³„ ë‰´ìŠ¤ ë¶„í¬
        date_distribution = defaultdict(int)
        sentiment_scores = []
        
        for article in articles:
            article_data = article.get('article', {})
            published_at = article_data.get('published_at', '')
            sentiment = article_data.get('sentiment', 'neutral')
            
            try:
                import dateutil.parser
                pub_date = dateutil.parser.parse(published_at)
                date_key = pub_date.strftime('%Y-%m')
                date_distribution[date_key] += 1
                
                # sentiment ì ìˆ˜í™”
                if sentiment == 'positive':
                    sentiment_scores.append(1)
                elif sentiment == 'negative':
                    sentiment_scores.append(-1)
                else:
                    sentiment_scores.append(0)
            except:
                continue
        
        # 2. íŠ¸ë Œë“œ ë°©í–¥ ë¶„ì„
        if len(date_distribution) >= 2:
            dates = sorted(date_distribution.keys())
            recent_count = date_distribution[dates[-1]]
            prev_count = date_distribution[dates[-2]] if len(dates) >= 2 else 0
            
            if recent_count > prev_count * 1.5:
                trend_direction = 'increasing'
                recent_increase = True
            elif recent_count < prev_count * 0.5:
                trend_direction = 'decreasing'
                recent_increase = False
            else:
                trend_direction = 'stable'
                recent_increase = False
        else:
            trend_direction = 'stable'
            recent_increase = False
        
        # 3. í‰ê·  sentiment
        if sentiment_scores:
            avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)
            if avg_sentiment_score > 0.2:
                avg_sentiment = 'positive'
            elif avg_sentiment_score < -0.2:
                avg_sentiment = 'negative'
            else:
                avg_sentiment = 'neutral'
        else:
            avg_sentiment = 'neutral'
        
        # 4. í”¼í¬ ê¸°ê°„ ì°¾ê¸°
        peak_period = max(date_distribution.keys(), key=lambda x: date_distribution[x]) if date_distribution else None
        
        return {
            'trend_direction': trend_direction,
            'recent_increase': recent_increase,
            'peak_period': peak_period,
            'avg_sentiment': avg_sentiment,
            'monthly_distribution': dict(date_distribution)
        } 