from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import logging
import re
from collections import defaultdict
import math

from ..model.materiality_dto import MaterialityTopic, MaterialityAssessment
from .materiality_mapping_service import MaterialityMappingService

logger = logging.getLogger(__name__)

class NewsAnalysisEngine:
    """ë‰´ìŠ¤ ë°ì´í„° ë¶„ì„ ì—”ì§„
    
    sasb-serviceì—ì„œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬:
    - ì¤‘ëŒ€ì„± í‰ê°€ í† í”½ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    - sentiment ë¶„ì„ ê²°ê³¼ í™œìš©
    - í‚¤ì›Œë“œ ë§¤ì¹­ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
    - ë‰´ìŠ¤ ë¹ˆë„ ë° ì¤‘ìš”ë„ ë¶„ì„
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.mapping_service = MaterialityMappingService()
        
        # ğŸ¯ ì¤‘ëŒ€ì„± í† í”½ë³„ í‚¤ì›Œë“œ ì‚¬ì „ (í™•ì¥ëœ ë§¤í•‘)
        self.topic_keyword_dict = {
            "ê¸°í›„ë³€í™” ëŒ€ì‘": [
                # í•µì‹¬ í‚¤ì›Œë“œ
                "ê¸°í›„ë³€í™”", "íƒ„ì†Œì¤‘ë¦½", "ë„·ì œë¡œ", "net zero", "íƒ„ì†Œë°°ì¶œ", "ì˜¨ì‹¤ê°€ìŠ¤",
                "ê¸°í›„ìœ„ê¸°", "íƒ„ì†Œê°ì¶•", "ì—ë„ˆì§€ì „í™˜", "RE100", "K-RE100",
                
                # ë‘ì‚°í“¨ì–¼ì…€ íŠ¹í™” ê¸°ìˆ 
                "ì—°ë£Œì „ì§€", "ìˆ˜ì†Œì—°ë£Œì „ì§€", "SOFC", "ê³ ì²´ì‚°í™”ë¬¼ì—°ë£Œì „ì§€", "ìˆ˜ì†Œë°œì „",
                "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ìˆ˜ì†Œê²½ì œ", "ìˆ˜ì†Œì¶©ì „ì†Œ", "ìˆ˜ì†Œëª¨ë¹Œë¦¬í‹°",
                
                # LS ELECTRIC íŠ¹í™” ê¸°ìˆ 
                "ESS", "ì—ë„ˆì§€ì €ì¥ì¥ì¹˜", "ë°°í„°ë¦¬ì €ì¥ì‹œìŠ¤í…œ", "ê·¸ë¦¬ë“œì—°ê³„",
                "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ë§ˆì´í¬ë¡œê·¸ë¦¬ë“œ", "ì „ë ¥ë³€í™˜", "ì¸ë²„í„°", "PCS",
                
                # ê³µí†µ ì¬ìƒì—ë„ˆì§€ ê¸°ìˆ 
                "ì¬ìƒì—ë„ˆì§€", "ì‹ ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥", "ì „ê¸°ì°¨", "EV", "ë°°í„°ë¦¬",
                
                # ì •ì±…/ì œë„
                "íƒ„ì†Œì„¸", "ë°°ì¶œê¶Œ", "K-ETS", "ETS", "íƒ„ì†Œêµ­ê²½ì„¸", "CBAM",
                "ESG", "ì§€ì†ê°€ëŠ¥", "íŒŒë¦¬í˜‘ì •", "TCFD", "SBTi"
            ],
            
            "ê³µê¸‰ë§ ê´€ë¦¬ ë° ìƒìƒê²½ì˜": [
                # ê³µê¸‰ë§ ê´€ë¦¬
                "ê³µê¸‰ë§", "í˜‘ë ¥ì‚¬", "íŒŒíŠ¸ë„ˆì‚¬", "ë²¤ë”", "ì¡°ë‹¬", "êµ¬ë§¤",
                "ìƒìƒê²½ì˜", "ë™ë°˜ì„±ì¥", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬", "ë””ì§€í„¸ì „í™˜",
                
                # ìœ„í—˜ ê´€ë¦¬
                "ê³µê¸‰ë§ë¦¬ìŠ¤í¬", "supply chain", "ESGê²½ì˜", "ì§€ì†ê°€ëŠ¥ê²½ì˜",
                "íˆ¬ëª…ì„±", "ì¶”ì ê°€ëŠ¥ì„±", "ì¸ê¶Œê²½ì˜", "ì•„ë™ë…¸ë™", "ê°•ì œë…¸ë™",
                
                # ê´€ë ¨ í‘œì¤€
                "ISO", "OECD", "UN Global Compact", "SA8000"
            ],
            
            "ì œí’ˆ ì—ë„ˆì§€ íš¨ìœ¨ í–¥ìƒ ë° í™˜ê²½ì˜í–¥ ê°ì†Œ": [
                # ë‘ì‚°í“¨ì–¼ì…€ ì œí’ˆ íš¨ìœ¨ì„±
                "ì—°ë£Œì „ì§€íš¨ìœ¨", "ìˆ˜ì†Œì´ìš©íš¨ìœ¨", "ë°œì „íš¨ìœ¨", "ì—´íš¨ìœ¨", "ì „ë ¥ë³€í™˜íš¨ìœ¨",
                "SOFCíš¨ìœ¨", "ì—°ë£Œì „ì§€ìˆ˜ëª…", "ë‚´êµ¬ì„±", "ì„±ëŠ¥ê°œì„ ", "ì¶œë ¥ë°€ë„",
                
                # LS ELECTRIC ì œí’ˆ íš¨ìœ¨ì„±
                "ì „ë ¥íš¨ìœ¨", "ë³€ì••ê¸°íš¨ìœ¨", "ì¸ë²„í„°íš¨ìœ¨", "ëª¨í„°íš¨ìœ¨", "ì „ë ¥ì†ì‹¤",
                "ê³ íš¨ìœ¨ë³€ì••ê¸°", "IGBT", "SiC", "íŒŒì›Œë°˜ë„ì²´", "ì „ë ¥ë³€í™˜",
                "ë¬´íš¨ì „ë ¥", "ì—­ë¥ ê°œì„ ", "ê³ ì¡°íŒŒ", "ì „ë ¥í’ˆì§ˆ",
                
                # ê³µí†µ ì—ë„ˆì§€ íš¨ìœ¨
                "ì—ë„ˆì§€íš¨ìœ¨", "ì—ë„ˆì§€ì ˆì•½", "ê³ íš¨ìœ¨", "íš¨ìœ¨ì„±", "ì ˆì „",
                "ìŠ¤ë§ˆíŠ¸ê¸°ê¸°", "IoT", "AI", "ë¹…ë°ì´í„°", "ë””ì§€í„¸íŠ¸ìœˆ",
                
                # í™˜ê²½ ì˜í–¥
                "í™˜ê²½ì˜í–¥", "í™˜ê²½ì¹œí™”", "ì¹œí™˜ê²½", "ì—ì½”", "ê·¸ë¦°",
                "ìƒì• ì£¼ê¸°", "LCA", "Life Cycle Assessment", "íƒ„ì†Œë°œìêµ­",
                "ìì›ìˆœí™˜", "íê¸°ë¬¼ê°ì¶•", "ì¬í™œìš©", "ì—…ì‚¬ì´í´ë§",
                
                # ì œí’ˆ í˜ì‹ 
                "ì œí’ˆí˜ì‹ ", "R&D", "ì—°êµ¬ê°œë°œ", "ê¸°ìˆ ê°œë°œ", "íŠ¹í—ˆ",
                "ì°¨ì„¸ëŒ€", "ìŠ¤ë§ˆíŠ¸", "ë””ì§€í„¸í™”", "ìë™í™”"
            ],
            
            "ë””ì§€í„¸ ì „í™˜ ë° ì‚¬ì´ë²„ë³´ì•ˆ": [
                # ë””ì§€í„¸ ì „í™˜
                "ë””ì§€í„¸ì „í™˜", "DX", "ë””ì§€í„¸í™”", "ìë™í™”", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬",
                "IoT", "AI", "ì¸ê³µì§€ëŠ¥", "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ",
                "ë¡œë´‡", "RPA", "ë¸”ë¡ì²´ì¸", "AR", "VR",
                
                # ì‚¬ì´ë²„ë³´ì•ˆ
                "ì‚¬ì´ë²„ë³´ì•ˆ", "ì •ë³´ë³´ì•ˆ", "ë°ì´í„°ë³´í˜¸", "ê°œì¸ì •ë³´", "í•´í‚¹",
                "ëœì„¬ì›¨ì–´", "ë³´ì•ˆì†”ë£¨ì…˜", "ì•”í˜¸í™”", "ì¸ì¦", "ë°©í™”ë²½"
            ],
            
            "ì¸ì¬ ìœ¡ì„± ë° ë…¸ë™ ì•ˆì „": [
                # ì¸ì¬ ìœ¡ì„±
                "ì¸ì¬ìœ¡ì„±", "êµìœ¡í›ˆë ¨", "ì—­ëŸ‰ê°œë°œ", "ìŠ¤í‚¬ì—…", "ë¦¬ìŠ¤í‚¬ë§",
                "ì—…ìŠ¤í‚¬ë§", "ì¸ì¬ê°œë°œ", "ì „ë¬¸ì¸ë ¥", "ê¸°ìˆ ì¸ë ¥", "ì²­ë…„ì±„ìš©",
                
                # ë…¸ë™ ì•ˆì „
                "ì•ˆì „", "ì‚°ì—…ì•ˆì „", "ì‘ì—…ì•ˆì „", "ì•ˆì „ì‚¬ê³ ", "ì¬í•´",
                "ì•ˆì „ë³´ê±´", "KOSHA", "ISO45001", "ì•ˆì „ê´€ë¦¬",
                "ë³´í˜¸êµ¬", "ì•ˆì „êµìœ¡", "ìœ„í—˜ì„±í‰ê°€"
            ],
            
            "ê±°ë²„ë„ŒìŠ¤ ë° ìœ¤ë¦¬ê²½ì˜": [
                # ê±°ë²„ë„ŒìŠ¤
                "ê±°ë²„ë„ŒìŠ¤", "ì´ì‚¬íšŒ", "ë…ë¦½ì´ì‚¬", "ê°ì‚¬ìœ„ì›íšŒ",
                "ë‚´ë¶€í†µì œ", "ì»´í”Œë¼ì´ì–¸ìŠ¤", "ì¤€ë²•", "ë¦¬ìŠ¤í¬ê´€ë¦¬",
                
                # ìœ¤ë¦¬ê²½ì˜
                "ìœ¤ë¦¬ê²½ì˜", "íˆ¬ëª…ê²½ì˜", "ë°˜ë¶€íŒ¨", "ë¶€íŒ¨ë°©ì§€",
                "ê³µì •ê±°ë˜", "ì´í•´ì¶©ëŒ", "ë‚´ë¶€ì‹ ê³ ", "ìœ¤ë¦¬ê°•ë ¹"
            ]
        }
        
        # ğŸ¯ íšŒì‚¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ (MVP ëŒ€ìƒ ê¸°ì—…)
        self.company_keywords = {
            "ë‘ì‚°í“¨ì–¼ì…€": [
                # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤
                "ì—°ë£Œì „ì§€", "SOFC", "ê³ ì²´ì‚°í™”ë¬¼ì—°ë£Œì „ì§€", "ìˆ˜ì†Œ", "ìˆ˜ì†Œê²½ì œ",
                "ê·¸ë¦°ìˆ˜ì†Œ", "ì²­ì •ìˆ˜ì†Œ", "ìˆ˜ì†Œë°œì „", "ìˆ˜ì†Œì¶©ì „ì†Œ", "ìˆ˜ì†Œëª¨ë¹Œë¦¬í‹°",
                "ë°œì „ìš©ì—°ë£Œì „ì§€", "ê°€ì •ìš©ì—°ë£Œì „ì§€", "ê±´ë¬¼ìš©ì—°ë£Œì „ì§€",
                
                # ê¸°ìˆ /ì œí’ˆ
                "ìŠ¤íƒ", "ì…€", "ì „í•´ì§ˆ", "ì „ê·¹", "ê°œì§ˆê¸°", "BOP",
                "ë°œì „íš¨ìœ¨", "ë‚´êµ¬ì„±", "ìˆ˜ëª…", "ì¶œë ¥ë°€ë„", "ì‹œìŠ¤í…œí†µí•©",
                
                # ì‚°ì—… ë¶„ì•¼
                "ì—ë„ˆì§€", "ë°œì „", "ì „ë ¥", "ì—´ë³‘í•©", "ë¶„ì‚°ì „ì›", "ë§ˆì´í¬ë¡œê·¸ë¦¬ë“œ"
            ],
            "LS ELECTRIC": [
                # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ 
                "ì „ë ¥ê¸°ê¸°", "ë³€ì••ê¸°", "ì°¨ë‹¨ê¸°", "ê°œíê¸°", "ë°°ì „ë°˜", "ìˆ˜ë°°ì „",
                "ì „ë ¥ë³€í™˜", "ì¸ë²„í„°", "ì»¨ë²„í„°", "PCS", "ESS", "ì—ë„ˆì§€ì €ì¥",
                "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ê·¸ë¦¬ë“œì—°ê³„", "ì „ë ¥ê³„í†µ", "ì†¡ë°°ì „",
                
                # ê¸°ìˆ /ì œí’ˆ
                "IGBT", "SiC", "íŒŒì›Œë°˜ë„ì²´", "ì „ë ¥ìš©ë°˜ë„ì²´", "ëª¨ë“ˆ",
                "ë³€ì••ê¸°íš¨ìœ¨", "ì†ì‹¤", "ì ˆì—°", "ëƒ‰ê°", "ë³´í˜¸ê³„ì „",
                "SCADA", "EMS", "ìë™í™”", "ì œì–´ì‹œìŠ¤í…œ",
                
                # ì‚°ì—… ë¶„ì•¼
                "ì „ë ¥", "ì² ë„", "ì‚°ì—…ìë™í™”", "ë¹Œë”©ìë™í™”", "ê³µì¥ìë™í™”",
                "ì‹ ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥", "ë°°í„°ë¦¬", "ì „ê¸°ì°¨"
            ],
            "í•œêµ­ì¤‘ë¶€ë°œì „": [
                # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤
                "í™”ë ¥ë°œì „", "ë°œì „ì†Œ", "ì „ë ¥ìƒì‚°", "ì „ë ¥ê³µê¸‰", "ë°œì „ì„¤ë¹„",
                "ì„íƒ„í™”ë ¥", "LNGë°œì „", "ë³µí•©í™”ë ¥", "ì§‘ë‹¨ì—ë„ˆì§€", "ì—´ë³‘í•©",
                "ì‹ ì¬ìƒì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥", "ì—°ë£Œì „ì§€", "ì—ë„ˆì§€ì €ì¥",
                
                # í™˜ê²½/ê¸°í›„ë³€í™”
                "ì˜¨ì‹¤ê°€ìŠ¤", "íƒ„ì†Œì¤‘ë¦½", "íƒ„ì†Œë°°ì¶œ", "í™˜ê²½ì„¤ë¹„", "íƒˆì„íƒ„",
                "ì—ë„ˆì§€ì „í™˜", "ê·¸ë¦°ë‰´ë”œ", "ì¹œí™˜ê²½ë°œì „", "ë°°ì¶œì €ê°", "ëŒ€ê¸°ì˜¤ì—¼",
                "ë¯¸ì„¸ë¨¼ì§€", "ì§ˆì†Œì‚°í™”ë¬¼", "í™©ì‚°í™”ë¬¼", "íìˆ˜ì²˜ë¦¬", "íê¸°ë¬¼",
                
                # ê¸°ìˆ /ì‹œì„¤
                "ë°œì „íš¨ìœ¨", "ì—´íš¨ìœ¨", "ì„¤ë¹„ê°œì„ ", "ì •ë¹„", "ë³´ìˆ˜", "ì ê²€",
                "í„°ë¹ˆ", "ë³´ì¼ëŸ¬", "ë°œì „ê¸°", "ë³€ì••ê¸°", "ì†¡ì „", "ë°°ì „",
                "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ESS", "ì—ë„ˆì§€ì €ì¥ì¥ì¹˜", "ê³„í†µì—°ê³„",
                
                # ì•ˆì „/ìš´ì˜
                "ì•ˆì „ê´€ë¦¬", "ì‘ì—…ì•ˆì „", "ì„¤ë¹„ì•ˆì „", "ì˜ˆë°©ì •ë¹„", "ì •ì „",
                "ì „ë ¥ìˆ˜ê¸‰", "ì „ë ¥ê³„í†µ", "ì „ë ¥ê±°ë˜", "SMP", "ìš©ëŸ‰ìš”ê¸ˆ",
                
                # ì‚¬íšŒ/ì§€ì—­
                "ì§€ì—­ìƒìƒ", "ì£¼ë¯¼ìˆ˜ìš©ì„±", "í™˜ê²½ì˜í–¥í‰ê°€", "ë°œì „ë‹¨ì§€",
                "ì¼ìë¦¬ì°½ì¶œ", "ì§€ì—­ê²½ì œ", "ì‚¬íšŒê³µí—Œ", "ìƒìƒí˜‘ë ¥"
            ]
        }
        
        # ê°€ì¤‘ì¹˜ ì„¤ì • (ì¡°ì •ë¨)
        self.weights = {
            'title_match': 5.0,      # ì œëª© ë§¤ì¹­ ê°€ì¤‘ì¹˜ ì¦ê°€
            'content_match': 2.0,    # ë³¸ë¬¸ ë§¤ì¹­ ê°€ì¤‘ì¹˜ ì¦ê°€
            'exact_match': 3.0,      # ì •í™•í•œ ë§¤ì¹­ ì¶”ê°€
            'partial_match': 1.0,    # ë¶€ë¶„ ë§¤ì¹­
            'sentiment_positive': 1.3,  # ê¸ì • sentiment ê°€ì¤‘ì¹˜
            'sentiment_negative': 0.9,  # ë¶€ì • sentiment ê°€ì¤‘ì¹˜
            'recent_news': 1.5,      # ìµœê·¼ ë‰´ìŠ¤ ê°€ì¤‘ì¹˜
            'keyword_density': 2.5,  # í‚¤ì›Œë“œ ë°€ë„ ê°€ì¤‘ì¹˜ ì¦ê°€
            'company_mention': 2.0   # íšŒì‚¬ëª… ì–¸ê¸‰ ê°€ì¤‘ì¹˜ ì¶”ê°€
        }
        
        # ê´€ë ¨ì„± ì„ê³„ê°’ ìƒí–¥ ì¡°ì •
        self.relevance_threshold = 0.3  # 0.1 â†’ 0.3ìœ¼ë¡œ ìƒí–¥
    
    def analyze_news_for_materiality(
        self,
        news_articles: List[Dict[str, Any]],
        materiality_topics: List[MaterialityTopic],
        company_name: str
    ) -> Dict[str, Dict[str, Any]]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¤‘ëŒ€ì„± í‰ê°€ í† í”½ë³„ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            news_articles: sasb-serviceì—ì„œ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            materiality_topics: ì¤‘ëŒ€ì„± í‰ê°€ í† í”½ ë¦¬ìŠ¤íŠ¸
            company_name: ê¸°ì—…ëª…
            
        Returns:
            Dict[str, Dict[str, Any]]: í† í”½ë³„ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info(f"ğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘: {len(news_articles)}ê°œ ê¸°ì‚¬, {len(materiality_topics)}ê°œ í† í”½")
        
        analysis_results = {}
        
        for topic in materiality_topics:
            topic_name = topic.topic_name
            
            # 1. ğŸ¯ ê°•í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ (í† í”½ + íšŒì‚¬ íŠ¹í™”)
            related_keywords = self.extract_enhanced_keywords(topic, company_name)
            
            # 2. ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ ë° ì ìˆ˜ ê³„ì‚°
            topic_news_analysis = self._analyze_topic_news(
                news_articles, topic_name, related_keywords, company_name
            )
            
            # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            comprehensive_score = self._calculate_comprehensive_score(
                topic_news_analysis['articles'], related_keywords
            )
            
            # 4. íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis = self._analyze_news_trend(topic_news_analysis['articles'])
            
            analysis_results[topic_name] = {
                'topic_name': topic_name,
                'related_keywords': related_keywords,
                'total_news_count': len(topic_news_analysis['articles']),
                'relevant_news_count': topic_news_analysis['relevant_count'],
                'comprehensive_score': comprehensive_score,
                'trend_analysis': trend_analysis,
                'top_articles': topic_news_analysis['top_articles'][:5],  # ìƒìœ„ 5ê°œ ê¸°ì‚¬
                'sentiment_distribution': topic_news_analysis['sentiment_distribution'],
                # ğŸ¯ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                'debug_info': {
                    'keyword_count': len(related_keywords),
                    'sample_keywords': related_keywords[:10],  # ìƒ˜í”Œ í‚¤ì›Œë“œ
                    'relevance_threshold': self.relevance_threshold,
                    'top_matched_articles': [
                        {
                            'title': art['article'].get('title', '')[:100],
                            'relevance_score': art['relevance_score'],
                            'matched_keywords': art['matched_keywords'][:5]
                        }
                        for art in topic_news_analysis['top_articles'][:3]
                    ]
                }
            }
            
            self.logger.info(f"âœ… {topic_name} ë¶„ì„ ì™„ë£Œ: {topic_news_analysis['relevant_count']}/{len(news_articles)}ê°œ ê´€ë ¨ ê¸°ì‚¬ (ì ìˆ˜: {comprehensive_score:.3f})")
        
        self.logger.info(f"ğŸ“ˆ ì „ì²´ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ í† í”½")
        return analysis_results
    
    def extract_enhanced_keywords(
        self, 
        topic: MaterialityTopic, 
        company_name: Optional[str] = None
    ) -> List[str]:
        """ğŸ¯ ê°•í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ - í† í”½ + íšŒì‚¬ íŠ¹í™” í‚¤ì›Œë“œ ì¡°í•©"""
        keywords = []
        topic_name = topic.topic_name
        
        # 1. ğŸ¯ í† í”½ë³„ í‚¤ì›Œë“œ ì‚¬ì „ì—ì„œ ë§¤ì¹­
        if topic_name in self.topic_keyword_dict:
            topic_keywords = self.topic_keyword_dict[topic_name]
            keywords.extend(topic_keywords)
            self.logger.info(f"ğŸ“‹ {topic_name}: {len(topic_keywords)}ê°œ í† í”½ í‚¤ì›Œë“œ ì¶”ê°€")
        
        # 2. ğŸ¯ íšŒì‚¬ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ê°€ (MVP ëŒ€ìƒ ê¸°ì—…)
        if company_name and company_name in self.company_keywords:
            company_keywords = self.company_keywords[company_name]
            keywords.extend(company_keywords)
            self.logger.info(f"ğŸ¢ {company_name}: {len(company_keywords)}ê°œ íšŒì‚¬ í‚¤ì›Œë“œ ì¶”ê°€")
        
        # 3. í† í”½ëª… ê¸°ë°˜ ìœ ì‚¬ì„± ë§¤ì¹­
        for dict_topic, dict_keywords in self.topic_keyword_dict.items():
            if topic_name != dict_topic:
                similarity = self._calculate_topic_similarity(topic_name, dict_topic)
                if similarity > 0.5:
                    keywords.extend(dict_keywords[:8])  # ìƒìœ„ 8ê°œë§Œ
                    self.logger.info(f"ğŸ”— ìœ ì‚¬ í† í”½ {dict_topic}: {similarity:.2f} ìœ ì‚¬ì„±, {len(dict_keywords[:8])}ê°œ í‚¤ì›Œë“œ ì¶”ê°€")
        
        # 4. ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë³´ì™„)
        basic_keywords = self._extract_keywords_from_text(topic_name)
        keywords.extend(basic_keywords)
        
        # 5. SASB ë§¤í•‘ í‚¤ì›Œë“œ (ë³´ì™„)
        try:
            sasb_keywords = self.mapping_service.find_related_keywords(topic_name)
            keywords.extend(sasb_keywords.get('industry_keywords', [])[:5])
            keywords.extend(sasb_keywords.get('sasb_keywords', [])[:5])
        except Exception as e:
            self.logger.warning(f"SASB í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 6. ì¤‘ë³µ ì œê±° ë° ì •ì œ
        unique_keywords = list(set(keywords))
        cleaned_keywords = [kw.strip() for kw in unique_keywords if len(kw.strip()) > 1]
        
        self.logger.info(f"âœ… {topic_name} ({company_name or 'N/A'}): ì´ {len(cleaned_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
        return cleaned_keywords
    
    def _extract_topic_keywords(self, topic: MaterialityTopic) -> List[str]:
        """í† í”½ í‚¤ì›Œë“œ ì¶”ì¶œ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
        return self.extract_enhanced_keywords(topic, None)
    
    def _calculate_topic_similarity(self, topic1: str, topic2: str) -> float:
        """í† í”½ëª… ê°„ ìœ ì‚¬ì„± ê³„ì‚°"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²¹ì¹¨ ê¸°ë°˜ ìœ ì‚¬ì„± ê³„ì‚°
        words1 = set(topic1.split())
        words2 = set(topic2.split())
        
        if not words1 or not words2:
            return 0.0
            
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
        keywords = []
        
        # 1. ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        words = text.split()
        keywords.extend([word.strip() for word in words if len(word.strip()) > 1])
        
        # 2. íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ì •ì œ
        cleaned_keywords = []
        for keyword in keywords:
            # íŠ¹ìˆ˜ ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)
            cleaned = re.sub(r'[^\w\sê°€-í£]', '', keyword)
            if len(cleaned) > 1:
                cleaned_keywords.append(cleaned)
        
        return cleaned_keywords
    
    def _analyze_topic_news(
        self,
        news_articles: List[Dict[str, Any]],
        topic_name: str,
        related_keywords: List[str],
        company_name: str
    ) -> Dict[str, Any]:
        """í† í”½ë³„ ë‰´ìŠ¤ ë¶„ì„"""
        relevant_articles = []
        sentiment_distribution = {'positive': 0, 'negative': 0, 'neutral': 0}
        
        for article in news_articles:
            # 1. ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            relevance_score = self._calculate_article_relevance(
                article, related_keywords, company_name
            )
            
            # 2. ê´€ë ¨ì„± ì„ê³„ê°’ ì ìš©
            if relevance_score > self.relevance_threshold:  # ìµœì†Œ ê´€ë ¨ì„± ì„ê³„ê°’
                article_analysis = {
                    'article': article,
                    'relevance_score': relevance_score,
                    'matched_keywords': self._find_matched_keywords(article, related_keywords)
                }
                relevant_articles.append(article_analysis)
                
                # 3. sentiment ë¶„í¬ ì—…ë°ì´íŠ¸
                sentiment = article.get('sentiment', 'neutral')
                if sentiment in sentiment_distribution:
                    sentiment_distribution[sentiment] += 1
        
        # 4. ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        relevant_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        return {
            'articles': relevant_articles,
            'relevant_count': len(relevant_articles),
            'top_articles': relevant_articles[:10],  # ìƒìœ„ 10ê°œ
            'sentiment_distribution': sentiment_distribution
        }
    
    def _calculate_article_relevance(
        self,
        article: Dict[str, Any],
        keywords: List[str],
        company_name: str
    ) -> float:
        """ğŸ¯ ê°œì„ ëœ ê¸°ì‚¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '') or article.get('description', '')
        sentiment = article.get('sentiment', 'neutral')
        published_at = article.get('published_at', '')
        
        total_score = 0.0
        
        # 1. ğŸ¯ ì œëª©ì—ì„œ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
        title_exact_matches = self._count_exact_keyword_matches(title, keywords)
        title_partial_matches = self._count_partial_keyword_matches(title, keywords)
        total_score += title_exact_matches * self.weights['title_match'] * self.weights['exact_match']
        total_score += title_partial_matches * self.weights['title_match'] * self.weights['partial_match']
        
        # 2. ğŸ¯ ë³¸ë¬¸ì—ì„œ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
        content_exact_matches = self._count_exact_keyword_matches(content, keywords)
        content_partial_matches = self._count_partial_keyword_matches(content, keywords)
        total_score += content_exact_matches * self.weights['content_match'] * self.weights['exact_match']
        total_score += content_partial_matches * self.weights['content_match'] * self.weights['partial_match']
        
        # 3. ê¸°ì—…ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if company_name.lower() in title.lower() or company_name.lower() in content.lower():
            total_score += self.weights['company_mention']
        
        # 4. sentiment ê°€ì¤‘ì¹˜ ì ìš©
        if sentiment == 'positive':
            total_score *= self.weights['sentiment_positive']
        elif sentiment == 'negative':
            total_score *= self.weights['sentiment_negative']
        
        # 5. ìµœê·¼ì„± ê°€ì¤‘ì¹˜ ì ìš©
        if self._is_recent_news(published_at):
            total_score *= self.weights['recent_news']
        
        # 6. í‚¤ì›Œë“œ ë°€ë„ ê°€ì¤‘ì¹˜
        keyword_density = self._calculate_keyword_density(title + ' ' + content, keywords)
        total_score += keyword_density * self.weights['keyword_density']
        
        return total_score
    
    def _count_keyword_matches(self, text: str, keywords: List[str]) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚°"""
        if not text:
            return 0
        
        text_lower = text.lower()
        matches = 0
        
        for keyword in keywords:
            if keyword.lower() in text_lower:
                matches += 1
        
        return matches
    
    def _count_exact_keyword_matches(self, text: str, keywords: List[str]) -> int:
        """ğŸ¯ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚° (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤)"""
        if not text:
            return 0
        
        text_lower = text.lower()
        matches = 0
        
        import re
        for keyword in keywords:
            # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ì •í™•í•œ ë§¤ì¹­
            pattern = r'\b' + re.escape(keyword.lower()) + r'\b'
            if re.search(pattern, text_lower):
                matches += 1
        
        return matches
    
    def _count_partial_keyword_matches(self, text: str, keywords: List[str]) -> int:
        """ğŸ¯ ë¶€ë¶„ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚° (í¬í•¨ ê´€ê³„)"""
        if not text:
            return 0
        
        text_lower = text.lower()
        exact_matches = self._count_exact_keyword_matches(text, keywords)
        total_matches = self._count_keyword_matches(text, keywords)
        
        # ë¶€ë¶„ ë§¤ì¹­ = ì „ì²´ ë§¤ì¹­ - ì •í™•í•œ ë§¤ì¹­
        return max(0, total_matches - exact_matches)
    
    def _find_matched_keywords(self, article: Dict[str, Any], keywords: List[str]) -> List[str]:
        """ê¸°ì‚¬ì—ì„œ ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì°¾ê¸°"""
        title = article.get('title', '')
        content = article.get('content', '') or article.get('summary', '')
        full_text = (title + ' ' + content).lower()
        
        matched = []
        for keyword in keywords:
            if keyword.lower() in full_text:
                matched.append(keyword)
        
        return matched
    
    def _is_recent_news(self, published_at: str) -> bool:
        """ìµœê·¼ ë‰´ìŠ¤ì¸ì§€ í™•ì¸ (30ì¼ ì´ë‚´)"""
        try:
            if not published_at:
                return False
            
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            import dateutil.parser
            pub_date = dateutil.parser.parse(published_at)
            now = datetime.now()
            days_diff = (now - pub_date).days
            
            return days_diff <= 30
        except:
            return False
    
    def _calculate_keyword_density(self, text: str, keywords: List[str]) -> float:
        """í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°"""
        if not text or not keywords:
            return 0.0
        
        text_lower = text.lower()
        total_words = len(text_lower.split())
        
        if total_words == 0:
            return 0.0
        
        keyword_count = 0
        for keyword in keywords:
            keyword_count += text_lower.count(keyword.lower())
        
        return keyword_count / total_words
    
    def _calculate_comprehensive_score(
        self,
        articles: List[Dict[str, Any]],
        keywords: List[str]
    ) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        if not articles:
            return 0.0
        
        # 1. í‰ê·  ê´€ë ¨ì„± ì ìˆ˜
        avg_relevance = sum(article['relevance_score'] for article in articles) / len(articles)
        
        # 2. ë‰´ìŠ¤ ê°œìˆ˜ ê°€ì¤‘ì¹˜ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        news_count_weight = math.log(len(articles) + 1)
        
        # 3. í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ë¹„ìœ¨)
        if keywords:
            all_matched_keywords = set()
            for article in articles:
                all_matched_keywords.update(article.get('matched_keywords', []))
            keyword_coverage = len(all_matched_keywords) / len(keywords)
        else:
            keyword_coverage = 0.0
        
        # 4. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        comprehensive_score = (
            avg_relevance * 0.4 +
            news_count_weight * 0.3 +
            keyword_coverage * 0.3
        )
        
        return round(comprehensive_score, 3)
    
    def _analyze_news_trend(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„"""
        if not articles:
            return {
                'trend_direction': 'stable',
                'recent_increase': False,
                'peak_period': None,
                'avg_sentiment': 'neutral'
            }
        
        # 1. ë‚ ì§œë³„ ë‰´ìŠ¤ ë¶„í¬
        date_distribution = defaultdict(int)
        sentiment_scores = []
        
        for article in articles:
            article_data = article.get('article', {})
            published_at = article_data.get('published_at', '')
            sentiment = article_data.get('sentiment', 'neutral')
            
            try:
                import dateutil.parser
                pub_date = dateutil.parser.parse(published_at)
                date_key = pub_date.strftime('%Y-%m')
                date_distribution[date_key] += 1
                
                # sentiment ì ìˆ˜í™”
                if sentiment == 'positive':
                    sentiment_scores.append(1)
                elif sentiment == 'negative':
                    sentiment_scores.append(-1)
                else:
                    sentiment_scores.append(0)
            except:
                continue
        
        # 2. íŠ¸ë Œë“œ ë°©í–¥ ë¶„ì„
        if len(date_distribution) >= 2:
            dates = sorted(date_distribution.keys())
            recent_count = date_distribution[dates[-1]]
            prev_count = date_distribution[dates[-2]] if len(dates) >= 2 else 0
            
            if recent_count > prev_count * 1.5:
                trend_direction = 'increasing'
                recent_increase = True
            elif recent_count < prev_count * 0.5:
                trend_direction = 'decreasing'
                recent_increase = False
            else:
                trend_direction = 'stable'
                recent_increase = False
        else:
            trend_direction = 'stable'
            recent_increase = False
        
        # 3. í‰ê·  sentiment
        if sentiment_scores:
            avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)
            if avg_sentiment_score > 0.2:
                avg_sentiment = 'positive'
            elif avg_sentiment_score < -0.2:
                avg_sentiment = 'negative'
            else:
                avg_sentiment = 'neutral'
        else:
            avg_sentiment = 'neutral'
        
        # 4. í”¼í¬ ê¸°ê°„ ì°¾ê¸°
        peak_period = max(date_distribution.keys(), key=lambda x: date_distribution[x]) if date_distribution else None
        
        return {
            'trend_direction': trend_direction,
            'recent_increase': recent_increase,
            'peak_period': peak_period,
            'avg_sentiment': avg_sentiment,
            'monthly_distribution': dict(date_distribution)
        } 