"""
ë‰´ìŠ¤ ê²€ìƒ‰ í—¬í¼ ëª¨ë“ˆ
SASB ì„œë¹„ìŠ¤ì™€ Material ì„œë¹„ìŠ¤ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë‰´ìŠ¤ ê²€ìƒ‰ ë¡œì§
"""
import random
import logging
from typing import List, Optional, Set, Dict, Any, Tuple

logger = logging.getLogger(__name__)

class NewsSearchHelper:
    """ë‰´ìŠ¤ ê²€ìƒ‰ ê´€ë ¨ ê³µí†µ í—¬í¼ í´ë˜ìŠ¤"""
    
    @staticmethod
    def sample_keywords(
        domain_keywords: List[str], 
        issue_keywords: List[str],
        max_domain: int = 3,
        max_issues: int = 5
    ) -> Tuple[List[str], List[str]]:
        """
        í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ëœë¤ ìƒ˜í”Œë§
        
        Args:
            domain_keywords: ì‚°ì—…/ë„ë©”ì¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            issue_keywords: ì´ìŠˆ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸  
            max_domain: ìµœëŒ€ ë„ë©”ì¸ í‚¤ì›Œë“œ ìˆ˜
            max_issues: ìµœëŒ€ ì´ìŠˆ í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            (ìƒ˜í”Œë§ëœ ë„ë©”ì¸ í‚¤ì›Œë“œ, ìƒ˜í”Œë§ëœ ì´ìŠˆ í‚¤ì›Œë“œ)
        """
        sampled_domain = random.sample(
            domain_keywords, 
            min(max_domain, len(domain_keywords))
        )
        sampled_issues = random.sample(
            issue_keywords, 
            min(max_issues, len(issue_keywords))
        )
        
        logger.debug(f"í‚¤ì›Œë“œ ìƒ˜í”Œë§ ì™„ë£Œ: ë„ë©”ì¸ {len(sampled_domain)}ê°œ, ì´ìŠˆ {len(sampled_issues)}ê°œ")
        return sampled_domain, sampled_issues
    
    @staticmethod
    def generate_search_queries(
        domain_keywords: List[str],
        issue_keywords: List[str],
        company_name: Optional[str] = None,
        max_combinations: int = 5
    ) -> List[str]:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ ì¡°í•© ìƒì„±
        
        Args:
            domain_keywords: ë„ë©”ì¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            issue_keywords: ì´ìŠˆ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            company_name: íšŒì‚¬ëª… (ì„ íƒì )
            max_combinations: ìµœëŒ€ ì¡°í•© ìˆ˜
            
        Returns:
            ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        queries = []
        combinations_created = 0
        
        for domain_keyword in domain_keywords:
            for issue_keyword in issue_keywords:
                if combinations_created >= max_combinations:
                    break
                    
                # ì¿¼ë¦¬ ì¡°í•© ìƒì„±
                if company_name:
                    query = f"{company_name} {domain_keyword} {issue_keyword}"
                else:
                    query = f"{domain_keyword} {issue_keyword}"
                
                queries.append(query)
                combinations_created += 1
            
            if combinations_created >= max_combinations:
                break
        
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ {len(queries)}ê°œ ìƒì„± ì™„ë£Œ")
        return queries
    
    @staticmethod
    def deduplicate_news_items(news_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
        
        Args:
            news_items: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¤‘ë³µ ì œê±°ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        seen_links: Set[str] = set()
        unique_items = []
        
        for item in news_items:
            link = item.get('link', '')
            if link and link not in seen_links:
                seen_links.add(link)
                unique_items.append(item)
        
        logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(news_items)}ê°œ â†’ {len(unique_items)}ê°œ")
        return unique_items
    
    @staticmethod
    def deduplicate_news_by_similarity(
        news_items: List[Dict[str, Any]], 
        similarity_threshold: float = 0.6
    ) -> List[Dict[str, Any]]:
        """
        ğŸ¯ ë‚´ìš© ìœ ì‚¬ë„ ê¸°ë°˜ ë‰´ìŠ¤ ê¸°ì‚¬ ì¤‘ë³µ ì œê±°
        
        Args:
            news_items: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0~1.0, ê¸°ë³¸ê°’: 0.6)
            
        Returns:
            ì¤‘ë³µ ì œê±°ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        if not news_items:
            return []
        
        unique_items = []
        
        for current_item in news_items:
            current_text = NewsSearchHelper._extract_article_text(current_item)
            is_duplicate = False
            
            # ê¸°ì¡´ ê³ ìœ  ê¸°ì‚¬ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
            for unique_item in unique_items:
                unique_text = NewsSearchHelper._extract_article_text(unique_item)
                similarity = NewsSearchHelper._calculate_text_similarity(current_text, unique_text)
                
                if similarity >= similarity_threshold:
                    is_duplicate = True
                    logger.debug(f"ì¤‘ë³µ ê¸°ì‚¬ ë°œê²¬: ìœ ì‚¬ë„ {similarity:.2f} >= {similarity_threshold}")
                    break
            
            if not is_duplicate:
                unique_items.append(current_item)
        
        logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(news_items)}ê°œ â†’ {len(unique_items)}ê°œ (ì„ê³„ê°’: {similarity_threshold})")
        return unique_items
    
    @staticmethod
    def _extract_article_text(article: Dict[str, Any]) -> str:
        """ê¸°ì‚¬ì—ì„œ ë¹„êµìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        title = article.get('title', '').strip()
        description = article.get('description', '').strip()
        content = article.get('content', '').strip()
        
        # ì œëª©ì€ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì—¬ì„œ 2ë²ˆ í¬í•¨
        text = f"{title} {title} {description} {content}"
        return NewsSearchHelper._clean_text(text)
    
    @staticmethod
    def _clean_text(text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ (HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
        import re
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip().lower()
    
    @staticmethod
    def _calculate_text_similarity(text1: str, text2: str) -> float:
        """
        ğŸ¯ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard + í† í° ë§¤ì¹­ ì¡°í•©)
        
        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0.0~1.0)
        """
        if not text1 or not text2:
            return 0.0
        
        if text1 == text2:
            return 1.0
        
        # í† í° ë¶„í• 
        tokens1 = set(text1.split())
        tokens2 = set(text2.split())
        
        if not tokens1 or not tokens2:
            return 0.0
        
        # 1. Jaccard ìœ ì‚¬ë„ (ì§‘í•© ê¸°ë°˜)
        intersection = len(tokens1.intersection(tokens2))
        union = len(tokens1.union(tokens2))
        jaccard_similarity = intersection / union if union > 0 else 0.0
        
        # 2. ê³µí†µ í† í° ë¹„ìœ¨ (ê¸¸ì´ ì°¨ì´ ê³ ë ¤)
        min_tokens = min(len(tokens1), len(tokens2))
        max_tokens = max(len(tokens1), len(tokens2))
        common_ratio = intersection / min_tokens if min_tokens > 0 else 0.0
        length_penalty = min_tokens / max_tokens if max_tokens > 0 else 0.0
        
        # 3. ì¡°í•© ì ìˆ˜ (Jaccard + ê³µí†µë¹„ìœ¨ + ê¸¸ì´íŒ¨ë„í‹°)
        final_similarity = (
            jaccard_similarity * 0.5 +           # Jaccard ê°€ì¤‘ì¹˜ 50%
            common_ratio * 0.4 +                 # ê³µí†µ í† í° ë¹„ìœ¨ 40%
            length_penalty * 0.1                 # ê¸¸ì´ ìœ ì‚¬ì„± 10%
        )
        
        return round(final_similarity, 3)
    
    @staticmethod
    def create_search_query_combinations(
        keywords_groups: List[List[str]],
        max_combinations: int = 10,
        separator: str = " "
    ) -> List[str]:
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œ ê·¸ë£¹ì˜ ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        
        Args:
            keywords_groups: í‚¤ì›Œë“œ ê·¸ë£¹ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [[íšŒì‚¬ëª…], [ë„ë©”ì¸í‚¤ì›Œë“œ], [ì´ìŠˆí‚¤ì›Œë“œ]])
            max_combinations: ìµœëŒ€ ì¡°í•© ìˆ˜
            separator: í‚¤ì›Œë“œ êµ¬ë¶„ì
            
        Returns:
            ì¡°í•©ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        import itertools
        
        # ëª¨ë“  ì¡°í•© ìƒì„±
        all_combinations = list(itertools.product(*keywords_groups))
        
        # ìµœëŒ€ ì¡°í•© ìˆ˜ ì œí•œ
        if len(all_combinations) > max_combinations:
            all_combinations = random.sample(all_combinations, max_combinations)
        
        # ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„±
        queries = [separator.join(combination) for combination in all_combinations]
        
        logger.info(f"ì¡°í•© ì¿¼ë¦¬ {len(queries)}ê°œ ìƒì„±")
        return queries
    
    @staticmethod
    def filter_news_by_relevance(
        news_items: List[Dict[str, Any]], 
        required_keywords: List[str],
        min_keyword_matches: int = 1
    ) -> List[Dict[str, Any]]:
        """
        ê´€ë ¨ì„± ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ í•„í„°ë§
        
        Args:
            news_items: ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            required_keywords: í•„ìˆ˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            min_keyword_matches: ìµœì†Œ í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜
            
        Returns:
            í•„í„°ë§ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        filtered_items = []
        
        for item in news_items:
            title = item.get('title', '').lower()
            description = item.get('description', '').lower()
            full_text = f"{title} {description}"
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì¹´ìš´íŠ¸
            matches = sum(1 for keyword in required_keywords 
                         if keyword.lower() in full_text)
            
            if matches >= min_keyword_matches:
                item['keyword_matches'] = matches
                filtered_items.append(item)
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
        filtered_items.sort(key=lambda x: x.get('keyword_matches', 0), reverse=True)
        
        logger.info(f"ê´€ë ¨ì„± í•„í„°ë§ ì™„ë£Œ: {len(news_items)}ê°œ â†’ {len(filtered_items)}ê°œ")
        return filtered_items

class NewsAnalysisHelper:
    """ë‰´ìŠ¤ ë¶„ì„ ê´€ë ¨ í—¬í¼ í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_news_stats(news_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ í†µê³„ ê³„ì‚°"""
        if not news_items:
            return {
                "total_count": 0,
                "sentiment_distribution": {},
                "average_length": 0
            }
        
        # ê°ì • ë¶„í¬ ê³„ì‚°
        sentiment_counts = {}
        total_length = 0
        
        for item in news_items:
            sentiment = item.get('sentiment', 'ì¤‘ë¦½')
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
            
            title_len = len(item.get('title', ''))
            desc_len = len(item.get('description', ''))
            total_length += title_len + desc_len
        
        return {
            "total_count": len(news_items),
            "sentiment_distribution": sentiment_counts,
            "average_length": total_length // len(news_items) if news_items else 0
        }
    
    @staticmethod
    def extract_keywords_from_news(
        news_items: List[Dict[str, Any]], 
        min_frequency: int = 2
    ) -> List[str]:
        """ë‰´ìŠ¤ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        from collections import Counter
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_text = []
        for item in news_items:
            title = item.get('title', '')
            description = item.get('description', '')
            all_text.append(f"{title} {description}")
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ì •ê·œì‹ ì‚¬ìš©)
        combined_text = ' '.join(all_text)
        keywords = re.findall(r'[ê°€-í£]{2,}', combined_text)
        
        # ë¹ˆë„ ê³„ì‚° ë° í•„í„°ë§
        keyword_counts = Counter(keywords)
        frequent_keywords = [
            keyword for keyword, count in keyword_counts.items() 
            if count >= min_frequency
        ]
        
        return frequent_keywords[:20]  # ìƒìœ„ 20ê°œë§Œ ë°˜í™˜ 