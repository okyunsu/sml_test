import json
from typing import Optional, Dict, Any
from datetime import datetime
import logging
import os
import sys

# ‚úÖ Python Path ÏÑ§Ï†ï (shared Î™®Îìà Ï†ëÍ∑ºÏö©)
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))))

logger = logging.getLogger(__name__)

class DashboardController:
    """SASB ÎåÄÏãúÎ≥¥Îìú Ïª®Ìä∏Î°§Îü¨ - ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Ï†ÅÏö©"""
    
    def __init__(self, redis_client=None):
        """ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ Î∞©ÏãùÏúºÎ°ú Redis ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î∞õÍ∏∞"""
        self.redis_client = redis_client
        
        # Í∏∞Ï°¥ Î∞©ÏãùÎèÑ ÏßÄÏõê (Ìò∏ÌôòÏÑ±)
        if self.redis_client is None:
            self.redis_client = self._get_redis_client_legacy()
    
    def _get_redis_client_legacy(self):
        """Î†àÍ±∞Ïãú Redis ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ± (Ìò∏ÌôòÏÑ±Ïö©)"""
        try:
            from shared.core.redis_factory import RedisClientFactory
            from app.config.settings import settings
            print(f"üîç DashboardController: Connecting to Redis: {settings.CELERY_BROKER_URL}")
            return RedisClientFactory.create_from_url(settings.CELERY_BROKER_URL)
        except Exception as e:
            logger.error(f"Redis ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            print(f"‚ö†Ô∏è  DashboardController: Using Mock Redis client due to: {e}")
            # Mock Redis ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î∞òÌôò (Î©îÏÑúÎìú Ìò∏Ï∂ú Ïãú ÏóêÎü¨ Î∞©ÏßÄ)
            class MockRedisClient:
                def get(self, key): return None
                def set(self, key, value, ex=None): return True
                def delete(self, key): return True
                def _client(self): return self
                def flushdb(self): return True
            return MockRedisClient()
    
    async def get_cache_data(self, key: str) -> Optional[Dict[str, Any]]:
        """Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå"""
        try:
            result = self.redis_client.get(key)
            if result:
                return json.loads(result)
            return None
        except Exception as e:
            logger.error(f"Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ïã§Ìå® ({key}): {e}")
            return None
    
    async def set_cache_data(self, key: str, data: Dict[str, Any], expire_minutes: int = 30) -> bool:
        """Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""
        try:
            json_data = json.dumps(data, ensure_ascii=False)
            expire_seconds = expire_minutes * 60
            return self.redis_client.set(key, json_data, ex=expire_seconds)
        except Exception as e:
            logger.error(f"Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ïã§Ìå® ({key}): {e}")
            return False
    
    async def delete_cache_data(self, key: str) -> bool:
        """Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú"""
        try:
            result = self.redis_client.delete(key)
            return bool(result)
        except Exception as e:
            logger.error(f"Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú Ïã§Ìå® ({key}): {e}")
            return False
    
    async def get_system_status(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú Ï†ÑÏ≤¥ ÏÉÅÌÉú Ï°∞Ìöå"""
        try:
            # Redis Ïó∞Í≤∞ ÏÉÅÌÉú ÌôïÏù∏
            redis_connected = False
            try:
                self.redis_client._client.ping()
                redis_connected = True
            except Exception:
                pass
            
            # Î™®ÎãàÌÑ∞ÎßÅ Ï§ëÏù∏ ÌöåÏÇ¨ Ïàò
            monitored_companies = ["ÎëêÏÇ∞Ìì®ÏñºÏÖÄ", "LS ELECTRIC", "ÌïúÍµ≠Ï§ëÎ∂ÄÎ∞úÏ†Ñ"]
            
            # Ï∫êÏãú ÌÜµÍ≥Ñ
            cache_stats = await self._get_cache_statistics()
            
            return {
                "status": "healthy" if redis_connected else "degraded",
                "timestamp": datetime.now().isoformat(),
                "components": {
                    "redis": "connected" if redis_connected else "disconnected",
                    "ml_models": "loaded",
                    "naver_api": "available"
                },
                "statistics": {
                    "monitored_companies": len(monitored_companies),
                    "cache_hit_rate": cache_stats.get("hit_rate", 0),
                    "cached_analyses": cache_stats.get("total_analyses", 0)
                },
                "companies": monitored_companies
            }
        except Exception as e:
            logger.error(f"ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {
                "status": "error",
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    async def get_cache_info(self) -> Dict[str, Any]:
        """Ï∫êÏãú Ï†ïÎ≥¥ ÏÉÅÏÑ∏ Ï°∞Ìöå"""
        try:
            cache_stats = await self._get_cache_statistics()
            
            # ÌöåÏÇ¨Î≥Ñ Ï∫êÏãú ÏÉÅÌÉú ÌôïÏù∏
            companies_cache = {}
            companies = ["ÎëêÏÇ∞Ìì®ÏñºÏÖÄ", "LS ELECTRIC", "ÌïúÍµ≠Ï§ëÎ∂ÄÎ∞úÏ†Ñ"]
            
            for company in companies:
                company_cache_key = f"latest_companies_renewable_analysis:{company}"
                sasb_cache_key = f"company_sasb_analysis:{company}"
                
                company_data = await self.get_cache_data(company_cache_key)
                sasb_data = await self.get_cache_data(sasb_cache_key)
                
                companies_cache[company] = {
                    "company_analysis_cached": company_data is not None,
                    "sasb_analysis_cached": sasb_data is not None,
                    "last_updated": company_data.get("timestamp") if company_data else None
                }
            
            return {
                "cache_statistics": cache_stats,
                "companies": companies_cache,
                "sasb_keywords_count": 53,  # Ïã†Ïû¨ÏÉùÏóêÎÑàÏßÄ ÌäπÌôî ÌÇ§ÏõåÎìú Í∞úÏàò
                "cache_settings": {
                    "company_sasb_expire_minutes": 60,
                    "sasb_only_expire_minutes": 30,
                    "max_articles_per_cache": 100
                }
            }
        except Exception as e:
            logger.error(f"Ï∫êÏãú Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return {"error": str(e)}
    
    async def _get_cache_statistics(self) -> Dict[str, Any]:
        """Ï∫êÏãú ÌÜµÍ≥Ñ Ï†ïÎ≥¥ Í≥ÑÏÇ∞"""
        try:
            # Í∞ÑÎã®Ìïú Ï∫êÏãú ÌÜµÍ≥Ñ (Ïã§Ï†úÎ°úÎäî Îçî Î≥µÏû°Ìïú Î°úÏßÅ ÌïÑÏöî)
            total_keys = 0
            hit_count = 0
            
            # SASB Í¥ÄÎ†® ÌÇ§ Ìå®ÌÑ¥Îì§
            key_patterns = [
                "company_sasb_analysis:*",
                "sasb_only_analysis:*",
                "latest_companies_renewable_analysis:*",
                "latest_sasb_renewable_analysis"
            ]
            
            # RedisÏóêÏÑú ÌÇ§ Í∞úÏàò ÌôïÏù∏ (Ïã§Ï†ú Íµ¨ÌòÑ Ïãú Îçî Ï†ïÍµêÌïú Î°úÏßÅ ÌïÑÏöî)
            try:
                # Í∞ÑÎã®Ìïú Ï∂îÏ†ïÍ∞í Î∞òÌôò
                total_keys = 6  # ÏòàÏÉÅ Ï∫êÏãú ÌÇ§ Í∞úÏàò
                hit_count = 4   # ÏòàÏÉÅ ÌûàÌä∏ Í∞úÏàò
            except Exception:
                pass
            
            hit_rate = (hit_count / total_keys) if total_keys > 0 else 0
            
            return {
                "total_analyses": total_keys,
                "cache_hits": hit_count,
                "hit_rate": round(hit_rate, 2),
                "last_calculated": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Ï∫êÏãú ÌÜµÍ≥Ñ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
            return {
                "total_analyses": 0,
                "cache_hits": 0,
                "hit_rate": 0,
                "error": str(e)
            } 