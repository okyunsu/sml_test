import logging
import random
import os
import sys
from typing import List, Optional

# âœ… Python Path ì„¤ì • (shared ëª¨ë“ˆ ì ‘ê·¼ìš©)
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))))

# âœ… ê³µí†µ ë‰´ìŠ¤ ê²€ìƒ‰ í—¬í¼ ì‚¬ìš©
from shared.services.news_search_helper import NewsSearchHelper

from .naver_news_service import NaverNewsService
from .ml_inference_service import MLInferenceService
from ..model.sasb_dto import AnalyzedNewsArticle, NewsItem, SentimentResult

class AnalysisService:
    """
    Orchestrates the news analysis workflow:
    1. Fetches news from Naver based on keywords.
    2. Deduplicates articles.
    3. Analyzes sentiment for each unique article.
    4. Returns a list of analyzed articles.
    """
    def __init__(self):
        self.naver_news_service = NaverNewsService()
        self.ml_inference_service = MLInferenceService()

    async def analyze_and_cache_news(
        self, 
        keywords: List[str], 
        company_name: Optional[str] = None
    ) -> List[AnalyzedNewsArticle]:
        """
        Generic method to fetch, deduplicate, and analyze news for a list of keywords
        and an optional company name.
        """
        all_news_items: List[NewsItem] = []
        logging.info(f"ë¶„ì„ ì‹œìž‘. íšŒì‚¬: {company_name or 'N/A'}, í‚¤ì›Œë“œ: {keywords}")

        for keyword in keywords:
            query = f"{company_name} {keyword}" if company_name else keyword
            try:
                # Corrected method name from get_news to search_news
                news_items = await self.naver_news_service.search_news(query, display=100)
                all_news_items.extend(news_items)
            except Exception as e:
                logging.error(f"'{query}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        # ë§í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ì¤‘ë³µ ì œê±°
        seen_links = set()
        unique_news_items = []
        for item in all_news_items:
            if item.link not in seen_links:
                seen_links.add(item.link)
                unique_news_items.append(item)

        logging.info(f"ì´ {len(all_news_items)}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘, ì¤‘ë³µ ì œê±° í›„ {len(unique_news_items)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ë°œê²¬.")

        # ê°ì„± ë¶„ì„
        analyzed_articles = []
        for news_item in unique_news_items:
            try:
                sentiment_result = self.ml_inference_service.analyze_sentiment(news_item.title)
                analyzed_article = AnalyzedNewsArticle(
                    title=news_item.title,
                    link=news_item.link,
                    description=news_item.description,
                    sentiment=SentimentResult(**sentiment_result)
                )
                analyzed_articles.append(analyzed_article)
            except Exception as e:
                logging.error(f"ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        logging.info(f"{len(analyzed_articles)}ê°œì˜ ê¸°ì‚¬ì— ëŒ€í•œ ë¶„ì„ ì™„ë£Œ.")
        return analyzed_articles

    async def analyze_with_combined_keywords(
        self,
        domain_keywords: List[str],
        issue_keywords: List[str], 
        company_name: Optional[str] = None,
        max_combinations: int = 5
    ) -> List[AnalyzedNewsArticle]:
        """
        ðŸŽ¯ ê°œì„ ëœ ì¡°í•© ê²€ìƒ‰ ë©”ì„œë“œ (ë¦¬íŒ©í† ë§ë¨)
        (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 
        ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
        
        Args:
            domain_keywords: ì‹ ìž¬ìƒì—ë„ˆì§€ ì‚°ì—…/ë¶„ì•¼ í‚¤ì›Œë“œ 
            issue_keywords: SASB ì´ìŠˆ í‚¤ì›Œë“œ
            company_name: íšŒì‚¬ëª… (ì„ íƒì )
            max_combinations: ìµœëŒ€ ì¡°í•© ìˆ˜ (ë„ˆë¬´ ë§Žì€ API í˜¸ì¶œ ë°©ì§€)
        
        Returns:
            ë¶„ì„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        logging.info(f"ðŸŽ¯ ì¡°í•© ê²€ìƒ‰ ì‹œìž‘. íšŒì‚¬: {company_name or 'N/A'}")
        
        # 1. í‚¤ì›Œë“œ ìƒ˜í”Œë§ (ê³µí†µ í—¬í¼ ì‚¬ìš©)
        sampled_domain, sampled_issues = NewsSearchHelper.sample_keywords(
            domain_keywords, issue_keywords, max_domain=3, max_issues=5
        )
        
        logging.info(f"ì‚°ì—… í‚¤ì›Œë“œ ìƒ˜í”Œ: {sampled_domain}")
        logging.info(f"ì´ìŠˆ í‚¤ì›Œë“œ ìƒ˜í”Œ: {sampled_issues}")
        
        # 2. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê³µí†µ í—¬í¼ ì‚¬ìš©)
        search_queries = NewsSearchHelper.generate_search_queries(
            sampled_domain, sampled_issues, company_name, max_combinations
        )
        
        # 3. ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰ (í‚¤ì›Œë“œ ì •ë³´ í¬í•¨)
        all_news_items_with_keywords = await self._search_news_with_queries_tracked(search_queries)
        
        # 4. ì¤‘ë³µ ì œê±° (í‚¤ì›Œë“œ ì •ë³´ ìœ ì§€)
        unique_news_with_keywords = self._deduplicate_with_keyword_tracking(all_news_items_with_keywords)
        
        # 5. ê°ì • ë¶„ì„ (í‚¤ì›Œë“œ ì •ë³´ í¬í•¨)
        analyzed_articles = await self._analyze_sentiment_for_articles_with_keywords(unique_news_with_keywords)
        
        logging.info(f"ðŸŽ¯ ì¡°í•© ê²€ìƒ‰ ë¶„ì„ ì™„ë£Œ: {len(analyzed_articles)}ê°œ ê¸°ì‚¬ ë¶„ì„ë¨")
        return analyzed_articles
    
    async def _search_news_with_queries(self, queries: List[str]) -> List[NewsItem]:
        """ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰"""
        all_news_items: List[NewsItem] = []
        
        for i, query in enumerate(queries):
            try:
                logging.info(f"ê²€ìƒ‰ {i + 1}/{len(queries)}: '{query}'")
                news_items = await self.naver_news_service.search_news(query, display=100)
                all_news_items.extend(news_items)
            except Exception as e:
                logging.error(f"'{query}'ì— ëŒ€í•œ ì¡°í•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        
        logging.info(f"ì´ {len(all_news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_news_items
    
    async def _search_news_with_queries_tracked(self, queries: List[str]) -> List[dict]:
        """
        ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰ (í‚¤ì›Œë“œ ì •ë³´ ì¶”ì )
        
        Returns:
            List[dict]: [{"news_item": NewsItem, "search_query": str, "keywords": List[str]}]
        """
        all_news_items_with_keywords = []
        
        for i, query in enumerate(queries):
            try:
                logging.info(f"ðŸ” ê²€ìƒ‰ {i + 1}/{len(queries)}: '{query}'")
                news_items = await self.naver_news_service.search_news(query, display=100)
                
                # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬)
                keywords = query.split()
                
                # ê° ë‰´ìŠ¤ ì•„ì´í…œì— í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
                for news_item in news_items:
                    news_with_keywords = {
                        "news_item": news_item,
                        "search_query": query,
                        "keywords": keywords
                    }
                    all_news_items_with_keywords.append(news_with_keywords)
                    
            except Exception as e:
                logging.error(f"'{query}'ì— ëŒ€í•œ ì¡°í•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        
        logging.info(f"ðŸ” ì´ {len(all_news_items_with_keywords)}ê°œ ë‰´ìŠ¤ (í‚¤ì›Œë“œ ì •ë³´ í¬í•¨) ìˆ˜ì§‘ ì™„ë£Œ")
        return all_news_items_with_keywords
    
    def _deduplicate_with_keyword_tracking(self, news_with_keywords: List[dict]) -> List[dict]:
        """
        ðŸŽ¯ ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì •ë³´ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
        
        Args:
            news_with_keywords: [{"news_item": NewsItem, "search_query": str, "keywords": List[str]}]
        
        Returns:
            List[dict]: [{"news_item": NewsItem, "matched_keywords": List[str]}] (ì¤‘ë³µ ì œê±°ë¨)
        """
        unique_news_with_keywords = []
        similarity_threshold = 0.6
        
        for current_item_data in news_with_keywords:
            current_item = current_item_data["news_item"]
            current_keywords = current_item_data["keywords"]
            current_text = NewsSearchHelper._extract_article_text({
                'title': current_item.title,
                'description': current_item.description,
                'content': getattr(current_item, 'content', '')
            })
            
            # ê¸°ì¡´ ê³ ìœ  ê¸°ì‚¬ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
            found_similar = False
            for unique_item_data in unique_news_with_keywords:
                unique_item = unique_item_data["news_item"]
                unique_text = NewsSearchHelper._extract_article_text({
                    'title': unique_item.title,
                    'description': unique_item.description,
                    'content': getattr(unique_item, 'content', '')
                })
                
                similarity = NewsSearchHelper._calculate_text_similarity(current_text, unique_text)
                
                if similarity >= similarity_threshold:
                    # ìœ ì‚¬í•œ ê¸°ì‚¬ ë°œê²¬ -> í‚¤ì›Œë“œ ë³‘í•©
                    unique_item_data["matched_keywords"].extend(current_keywords)
                    unique_item_data["matched_keywords"] = list(set(unique_item_data["matched_keywords"]))  # ì¤‘ë³µ ì œê±°
                    found_similar = True
                    logging.debug(f"ðŸŽ¯ ìœ ì‚¬ ê¸°ì‚¬ ë³‘í•©: ìœ ì‚¬ë„ {similarity:.2f}")
                    break
            
            if not found_similar:
                # ìƒˆë¡œìš´ ê³ ìœ  ê¸°ì‚¬ ì¶”ê°€
                unique_news_with_keywords.append({
                    "news_item": current_item,
                    "matched_keywords": current_keywords.copy()
                })
        
        logging.info(f"ðŸŽ¯ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(news_with_keywords)}ê°œ â†’ {len(unique_news_with_keywords)}ê°œ ê³ ìœ  ê¸°ì‚¬ (í‚¤ì›Œë“œ ì •ë³´ ìœ ì§€)")
        return unique_news_with_keywords
    
    async def _analyze_sentiment_for_articles_with_keywords(self, news_with_keywords: List[dict]) -> List[AnalyzedNewsArticle]:
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì— ëŒ€í•œ ê°ì • ë¶„ì„ ìˆ˜í–‰ (í‚¤ì›Œë“œ ì •ë³´ í¬í•¨)
        
        Args:
            news_with_keywords: [{"news_item": NewsItem, "matched_keywords": List[str]}]
        
        Returns:
            List[AnalyzedNewsArticle]: matched_keywords í•„ë“œ í¬í•¨
        """
        analyzed_articles = []
        
        for item_data in news_with_keywords:
            news_item = item_data["news_item"]
            matched_keywords = item_data["matched_keywords"]
            
            try:
                sentiment_result = self.ml_inference_service.analyze_sentiment(news_item.title)
                analyzed_article = AnalyzedNewsArticle(
                    title=news_item.title,
                    link=news_item.link,
                    description=news_item.description,
                    sentiment=SentimentResult(**sentiment_result),
                    matched_keywords=matched_keywords  # ðŸŽ¯ í‚¤ì›Œë“œ ì •ë³´ í¬í•¨!
                )
                analyzed_articles.append(analyzed_article)
            except Exception as e:
                logging.error(f"ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í‚¤ì›Œë“œ: {matched_keywords}): {e}", exc_info=True)
        
        logging.info(f"ðŸŽ¯ {len(analyzed_articles)}ê°œ ê¸°ì‚¬ ê°ì • ë¶„ì„ ì™„ë£Œ (í‚¤ì›Œë“œ ì •ë³´ í¬í•¨)")
        return analyzed_articles
    
    async def _analyze_sentiment_for_articles(self, news_items: List[NewsItem]) -> List[AnalyzedNewsArticle]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì— ëŒ€í•œ ê°ì • ë¶„ì„ ìˆ˜í–‰"""
        analyzed_articles = []
        
        for news_item in news_items:
            try:
                sentiment_result = self.ml_inference_service.analyze_sentiment(news_item.title)
                analyzed_article = AnalyzedNewsArticle(
                    title=news_item.title,
                    link=news_item.link,
                    description=news_item.description,
                    sentiment=SentimentResult(**sentiment_result)
                )
                analyzed_articles.append(analyzed_article)
            except Exception as e:
                logging.error(f"ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        
        logging.info(f"{len(analyzed_articles)}ê°œ ê¸°ì‚¬ ê°ì • ë¶„ì„ ì™„ë£Œ")
        return analyzed_articles
    
    def _convert_news_item_to_dict(self, news_item: NewsItem) -> dict:
        """NewsItemì„ dictë¡œ ë³€í™˜ (ì¤‘ë³µ ì œê±°ìš©)"""
        return {
            'title': news_item.title,
            'link': news_item.link,
            'description': news_item.description
        }
    
    def _convert_dict_to_news_item(self, news_dict: dict) -> NewsItem:
        """dictë¥¼ NewsItemìœ¼ë¡œ ë³€í™˜"""
        return NewsItem(
            title=news_dict['title'],
            link=news_dict['link'],
            description=news_dict['description']
        ) 