import logging
import random
import os
import sys
from typing import List, Optional

# âœ… Python Path ì„¤ì • (shared ëª¨ë“ˆ ì ‘ê·¼ìš©)
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))))

# âœ… ê³µí†µ ë‰´ìŠ¤ ê²€ìƒ‰ í—¬í¼ ì‚¬ìš©
from shared.services.news_search_helper import NewsSearchHelper

from .naver_news_service import NaverNewsService
from .ml_inference_service import MLInferenceService
from ..model.sasb_dto import AnalyzedNewsArticle, NewsItem, SentimentResult

class AnalysisService:
    """
    Orchestrates the news analysis workflow:
    1. Fetches news from Naver based on keywords.
    2. Deduplicates articles.
    3. Analyzes sentiment for each unique article.
    4. Returns a list of analyzed articles.
    """
    def __init__(self):
        self.naver_news_service = NaverNewsService()
        self.ml_inference_service = MLInferenceService()

    async def analyze_and_cache_news(
        self, 
        keywords: List[str], 
        company_name: Optional[str] = None
    ) -> List[AnalyzedNewsArticle]:
        """
        Generic method to fetch, deduplicate, and analyze news for a list of keywords
        and an optional company name.
        """
        all_news_items: List[NewsItem] = []
        logging.info(f"ë¶„ì„ ì‹œìž‘. íšŒì‚¬: {company_name or 'N/A'}, í‚¤ì›Œë“œ: {keywords}")

        for keyword in keywords:
            query = f"{company_name} {keyword}" if company_name else keyword
            try:
                # Corrected method name from get_news to search_news
                news_items = await self.naver_news_service.search_news(query, display=100)
                all_news_items.extend(news_items)
            except Exception as e:
                logging.error(f"'{query}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        # ë§í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ì¤‘ë³µ ì œê±°
        seen_links = set()
        unique_news_items = []
        for item in all_news_items:
            if item.link not in seen_links:
                seen_links.add(item.link)
                unique_news_items.append(item)

        logging.info(f"ì´ {len(all_news_items)}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘, ì¤‘ë³µ ì œê±° í›„ {len(unique_news_items)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ë°œê²¬.")

        # ê°ì„± ë¶„ì„
        analyzed_articles = []
        for news_item in unique_news_items:
            try:
                sentiment_result = self.ml_inference_service.analyze_sentiment(news_item.title)
                analyzed_article = AnalyzedNewsArticle(
                    title=news_item.title,
                    link=news_item.link,
                    description=news_item.description,
                    sentiment=SentimentResult(**sentiment_result)
                )
                analyzed_articles.append(analyzed_article)
            except Exception as e:
                logging.error(f"ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        logging.info(f"{len(analyzed_articles)}ê°œì˜ ê¸°ì‚¬ì— ëŒ€í•œ ë¶„ì„ ì™„ë£Œ.")
        return analyzed_articles

    async def analyze_with_combined_keywords(
        self,
        domain_keywords: List[str],
        issue_keywords: List[str], 
        company_name: Optional[str] = None,
        max_combinations: int = 5
    ) -> List[AnalyzedNewsArticle]:
        """
        ðŸŽ¯ ê°œì„ ëœ ì¡°í•© ê²€ìƒ‰ ë©”ì„œë“œ (ë¦¬íŒ©í† ë§ë¨)
        (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 
        ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
        
        Args:
            domain_keywords: ì‹ ìž¬ìƒì—ë„ˆì§€ ì‚°ì—…/ë¶„ì•¼ í‚¤ì›Œë“œ 
            issue_keywords: SASB ì´ìŠˆ í‚¤ì›Œë“œ
            company_name: íšŒì‚¬ëª… (ì„ íƒì )
            max_combinations: ìµœëŒ€ ì¡°í•© ìˆ˜ (ë„ˆë¬´ ë§Žì€ API í˜¸ì¶œ ë°©ì§€)
        
        Returns:
            ë¶„ì„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        logging.info(f"ðŸŽ¯ ì¡°í•© ê²€ìƒ‰ ì‹œìž‘. íšŒì‚¬: {company_name or 'N/A'}")
        
        # 1. í‚¤ì›Œë“œ ìƒ˜í”Œë§ (ê³µí†µ í—¬í¼ ì‚¬ìš©)
        sampled_domain, sampled_issues = NewsSearchHelper.sample_keywords(
            domain_keywords, issue_keywords, max_domain=3, max_issues=5
        )
        
        logging.info(f"ì‚°ì—… í‚¤ì›Œë“œ ìƒ˜í”Œ: {sampled_domain}")
        logging.info(f"ì´ìŠˆ í‚¤ì›Œë“œ ìƒ˜í”Œ: {sampled_issues}")
        
        # 2. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê³µí†µ í—¬í¼ ì‚¬ìš©)
        search_queries = NewsSearchHelper.generate_search_queries(
            sampled_domain, sampled_issues, company_name, max_combinations
        )
        
        # 3. ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰
        all_news_items = await self._search_news_with_queries(search_queries)
        
        # 4. ì¤‘ë³µ ì œê±° (ê³µí†µ í—¬í¼ ì‚¬ìš©)
        news_items_dicts = [self._convert_news_item_to_dict(item) for item in all_news_items]
        unique_news_dicts = NewsSearchHelper.deduplicate_news_items(news_items_dicts)
        unique_news_items = [self._convert_dict_to_news_item(item) for item in unique_news_dicts]
        
        # 5. ê°ì • ë¶„ì„
        analyzed_articles = await self._analyze_sentiment_for_articles(unique_news_items)
        
        logging.info(f"ðŸŽ¯ ì¡°í•© ê²€ìƒ‰ ë¶„ì„ ì™„ë£Œ: {len(analyzed_articles)}ê°œ ê¸°ì‚¬ ë¶„ì„ë¨")
        return analyzed_articles
    
    async def _search_news_with_queries(self, queries: List[str]) -> List[NewsItem]:
        """ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰"""
        all_news_items: List[NewsItem] = []
        
        for i, query in enumerate(queries):
            try:
                logging.info(f"ê²€ìƒ‰ {i + 1}/{len(queries)}: '{query}'")
                news_items = await self.naver_news_service.search_news(query, display=100)
                all_news_items.extend(news_items)
            except Exception as e:
                logging.error(f"'{query}'ì— ëŒ€í•œ ì¡°í•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        
        logging.info(f"ì´ {len(all_news_items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_news_items
    
    async def _analyze_sentiment_for_articles(self, news_items: List[NewsItem]) -> List[AnalyzedNewsArticle]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì— ëŒ€í•œ ê°ì • ë¶„ì„ ìˆ˜í–‰"""
        analyzed_articles = []
        
        for news_item in news_items:
            try:
                sentiment_result = self.ml_inference_service.analyze_sentiment(news_item.title)
                analyzed_article = AnalyzedNewsArticle(
                    title=news_item.title,
                    link=news_item.link,
                    description=news_item.description,
                    sentiment=SentimentResult(**sentiment_result)
                )
                analyzed_articles.append(analyzed_article)
            except Exception as e:
                logging.error(f"ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        
        logging.info(f"{len(analyzed_articles)}ê°œ ê¸°ì‚¬ ê°ì • ë¶„ì„ ì™„ë£Œ")
        return analyzed_articles
    
    def _convert_news_item_to_dict(self, news_item: NewsItem) -> dict:
        """NewsItemì„ dictë¡œ ë³€í™˜ (ì¤‘ë³µ ì œê±°ìš©)"""
        return {
            'title': news_item.title,
            'link': news_item.link,
            'description': news_item.description
        }
    
    def _convert_dict_to_news_item(self, news_dict: dict) -> NewsItem:
        """dictë¥¼ NewsItemìœ¼ë¡œ ë³€í™˜"""
        return NewsItem(
            title=news_dict['title'],
            link=news_dict['link'],
            description=news_dict['description']
        ) 