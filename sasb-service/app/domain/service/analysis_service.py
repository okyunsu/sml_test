import logging
import random
from typing import List, Optional
from .naver_news_service import NaverNewsService
from .ml_inference_service import MLInferenceService
from ..model.sasb_dto import AnalyzedNewsArticle, NewsItem, SentimentResult

class AnalysisService:
    """
    Orchestrates the news analysis workflow:
    1. Fetches news from Naver based on keywords.
    2. Deduplicates articles.
    3. Analyzes sentiment for each unique article.
    4. Returns a list of analyzed articles.
    """
    def __init__(self):
        self.naver_news_service = NaverNewsService()
        self.ml_inference_service = MLInferenceService()

    async def analyze_and_cache_news(
        self, 
        keywords: List[str], 
        company_name: Optional[str] = None
    ) -> List[AnalyzedNewsArticle]:
        """
        Generic method to fetch, deduplicate, and analyze news for a list of keywords
        and an optional company name.
        """
        all_news_items: List[NewsItem] = []
        logging.info(f"ë¶„ì„ ì‹œì‘. íšŒì‚¬: {company_name or 'N/A'}, í‚¤ì›Œë“œ: {keywords}")

        for keyword in keywords:
            query = f"{company_name} {keyword}" if company_name else keyword
            try:
                # Corrected method name from get_news to search_news
                news_items = await self.naver_news_service.search_news(query, display=100)
                all_news_items.extend(news_items)
            except Exception as e:
                logging.error(f"'{query}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        # ë§í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ì¤‘ë³µ ì œê±°
        seen_links = set()
        unique_news_items = []
        for item in all_news_items:
            if item.link not in seen_links:
                seen_links.add(item.link)
                unique_news_items.append(item)

        logging.info(f"ì´ {len(all_news_items)}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘, ì¤‘ë³µ ì œê±° í›„ {len(unique_news_items)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ ë°œê²¬.")

        # ê°ì„± ë¶„ì„
        analyzed_articles = []
        for news_item in unique_news_items:
            try:
                sentiment_result = self.ml_inference_service.analyze_sentiment(news_item.title)
                analyzed_article = AnalyzedNewsArticle(
                    title=news_item.title,
                    link=news_item.link,
                    description=news_item.description,
                    sentiment=SentimentResult(**sentiment_result)
                )
                analyzed_articles.append(analyzed_article)
            except Exception as e:
                logging.error(f"ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        logging.info(f"{len(analyzed_articles)}ê°œì˜ ê¸°ì‚¬ì— ëŒ€í•œ ë¶„ì„ ì™„ë£Œ.")
        return analyzed_articles

    async def analyze_with_combined_keywords(
        self,
        domain_keywords: List[str],
        issue_keywords: List[str], 
        company_name: Optional[str] = None,
        max_combinations: int = 5
    ) -> List[AnalyzedNewsArticle]:
        """
        ğŸ¯ ê°œì„ ëœ ì¡°í•© ê²€ìƒ‰ ë©”ì„œë“œ
        (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 
        ê´€ë ¨ì„± ë†’ì€ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
        
        Args:
            domain_keywords: ì‹ ì¬ìƒì—ë„ˆì§€ ì‚°ì—…/ë¶„ì•¼ í‚¤ì›Œë“œ 
            issue_keywords: SASB ì´ìŠˆ í‚¤ì›Œë“œ
            company_name: íšŒì‚¬ëª… (ì„ íƒì )
            max_combinations: ìµœëŒ€ ì¡°í•© ìˆ˜ (ë„ˆë¬´ ë§ì€ API í˜¸ì¶œ ë°©ì§€)
        
        Returns:
            ë¶„ì„ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        all_news_items: List[NewsItem] = []
        
        # ì¡°í•© ìƒì„± ë° ì œí•œ
        combinations_tried = 0
        
        # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘í•œ ì¡°í•© ì‹œë„
        sampled_domain = random.sample(domain_keywords, min(3, len(domain_keywords)))
        sampled_issues = random.sample(issue_keywords, min(5, len(issue_keywords)))
        
        logging.info(f"ğŸ¯ ì¡°í•© ê²€ìƒ‰ ì‹œì‘. íšŒì‚¬: {company_name or 'N/A'}")
        logging.info(f"ì‚°ì—… í‚¤ì›Œë“œ ìƒ˜í”Œ: {sampled_domain}")
        logging.info(f"ì´ìŠˆ í‚¤ì›Œë“œ ìƒ˜í”Œ: {sampled_issues}")

        for domain_keyword in sampled_domain:
            for issue_keyword in sampled_issues:
                if combinations_tried >= max_combinations:
                    break
                    
                # ë„¤ì´ë²„ APIìš© AND ì¡°í•© ì¿¼ë¦¬ ìƒì„±
                if company_name:
                    # íšŒì‚¬ + ì‚°ì—… í‚¤ì›Œë“œ + ì´ìŠˆ í‚¤ì›Œë“œ
                    query = f"{company_name} {domain_keyword} {issue_keyword}"
                else:
                    # ì‚°ì—… í‚¤ì›Œë“œ + ì´ìŠˆ í‚¤ì›Œë“œ
                    query = f"{domain_keyword} {issue_keyword}"
                
                try:
                    logging.info(f"ê²€ìƒ‰ {combinations_tried + 1}/{max_combinations}: '{query}'")
                    news_items = await self.naver_news_service.search_news(query, display=100)
                    all_news_items.extend(news_items)
                    combinations_tried += 1
                    
                except Exception as e:
                    logging.error(f"'{query}'ì— ëŒ€í•œ ì¡°í•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            
            if combinations_tried >= max_combinations:
                break

        # ë§í¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ì¤‘ë³µ ì œê±°
        seen_links = set()
        unique_news_items = []
        for item in all_news_items:
            if item.link not in seen_links:
                seen_links.add(item.link)
                unique_news_items.append(item)

        logging.info(f"ğŸ¯ ì¡°í•© ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_news_items)}ê°œ ìˆ˜ì§‘, ì¤‘ë³µ ì œê±° í›„ {len(unique_news_items)}ê°œ ê³ ìœ  ê¸°ì‚¬")

        # ê°ì„± ë¶„ì„
        analyzed_articles = []
        for news_item in unique_news_items:
            try:
                sentiment_result = self.ml_inference_service.analyze_sentiment(news_item.title)
                analyzed_article = AnalyzedNewsArticle(
                    title=news_item.title,
                    link=news_item.link,
                    description=news_item.description,
                    sentiment=SentimentResult(**sentiment_result)
                )
                analyzed_articles.append(analyzed_article)
            except Exception as e:
                logging.error(f"ì¡°í•© ê²€ìƒ‰ ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

        logging.info(f"ğŸ¯ ì¡°í•© ê²€ìƒ‰ ë¶„ì„ ì™„ë£Œ: {len(analyzed_articles)}ê°œ ê¸°ì‚¬ ë¶„ì„ë¨")
        return analyzed_articles 