import asyncio
import json
from urllib.parse import urlparse
from typing import Optional, List, Dict, Any
from .celery_app import celery_app
from ..domain.service.analysis_service import AnalysisService
from ..domain.model.sasb_dto import NewsAnalysisRequest
from ..core.redis_client import RedisClient
from ..config.settings import settings
import logging

# =============================================================================
# ğŸ¯ ê°œì„ ëœ ì´ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
# ë¬¸ì œ: ê¸°ì¡´ ë‹¨ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì¸í•œ ë¹„ê´€ë ¨ ì‚°ì—… ë‰´ìŠ¤ ê³¼ë‹¤ ìˆ˜ì§‘ 
# í•´ê²°: (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•© ê²€ìƒ‰
# =============================================================================

# ê·¸ë£¹ 1: ì‹ ì¬ìƒì—ë„ˆì§€ ì‚°ì—…/ë¶„ì•¼ í‚¤ì›Œë“œ (Domain Keywords)
# ì´ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ì•¼ë§Œ ì‹ ì¬ìƒì—ë„ˆì§€ ì‚°ì—… ë‰´ìŠ¤ë¡œ ê°„ì£¼
RENEWABLE_DOMAIN_KEYWORDS = [
    # í•µì‹¬ ì—ë„ˆì§€ ë¶„ì•¼
    "ì‹ ì¬ìƒì—ë„ˆì§€", "ì¬ìƒì—ë„ˆì§€", "ì‹ ì—ë„ˆì§€", "ì²­ì •ì—ë„ˆì§€", "ì¹œí™˜ê²½ì—ë„ˆì§€",
    
    # ë°œì „ ê¸°ìˆ ë³„
    "íƒœì–‘ê´‘", "íƒœì–‘ì—´", "í’ë ¥", "ìˆ˜ë ¥", "ìˆ˜ë ¥ë°œì „", "ì¡°ë ¥", "ì§€ì—´", "ë°”ì´ì˜¤ì—ë„ˆì§€", 
    "ë°”ì´ì˜¤ë§¤ìŠ¤", "ë°”ì´ì˜¤ê°€ìŠ¤", "ì—°ë£Œì „ì§€",
    
    # ì—ë„ˆì§€ ì €ì¥ ë° ì¸í”„ë¼
    "ESS", "ì—ë„ˆì§€ì €ì¥ì¥ì¹˜", "ë°°í„°ë¦¬", "ìˆ˜ì†Œ", "ê·¸ë¦°ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ", "ì•”ëª¨ë‹ˆì•„",
    
    # ì „ë ¥ ì‚°ì—…
    "ë°œì „ì†Œ", "ë°œì „ì‚¬", "ë°œì „ê³µê¸°ì—…", "ì „ë ¥", "ì „ë ¥ê³µì‚¬", "í•œì „", "ì „ë ¥ê±°ë˜ì†Œ", 
    "ì†¡ì „", "ë°°ì „", "ì „ë ¥ë§", "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ë§ˆì´í¬ë¡œê·¸ë¦¬ë“œ",
    
    # ì—ë„ˆì§€ ì „í™˜
    "ì—ë„ˆì§€ì „í™˜", "ì „ì›ë¯¹ìŠ¤", "ì „ì›êµ¬ì„±", "ì—ë„ˆì§€ë¯¹ìŠ¤", "RE100", "K-RE100",
    
    # ê´€ë ¨ ê¸°ì—…/ê¸°ê´€
    "ì—ë„ˆì§€ê³µì‚¬", "ë°œì „íšŒì‚¬", "ì „ë ¥íšŒì‚¬", "ì—ë„ˆì§€ê¸°ì—…", "ì „ë ¥ì‚°ì—…"
]

# ê·¸ë£¹ 2: SASB ì´ìŠˆ í‚¤ì›Œë“œ (Issue Keywords) - ê¸°ì¡´ 53ê°œ í‚¤ì›Œë“œ ìœ ì§€
SASB_ISSUE_KEYWORDS = [
    # 1. Greenhouse Gas Emissions & Energy Resource Planning
    "íƒ„ì†Œì¤‘ë¦½", "íƒ„ì†Œë°°ì¶œ", "ì˜¨ì‹¤ê°€ìŠ¤", "RE100", "CF100", "ì—ë„ˆì§€ë¯¹ìŠ¤", "ì „ì›êµ¬ì„±",
    "íƒ„ì†Œêµ­ê²½ì„¸", "ìŠ¤ì½”í”„", "ê°ì¶•ëª©í‘œ", "NDC", "ìë°œì  íƒ„ì†Œì‹œì¥",
    
    # 2. Air Quality  
    "ë¯¸ì„¸ë¨¼ì§€", "ëŒ€ê¸°ì˜¤ì—¼", "í™©ì‚°í™”ë¬¼", "ì§ˆì†Œì‚°í™”ë¬¼", "ë°”ì´ì˜¤ë§¤ìŠ¤", "ë¹„ì‚°ë¨¼ì§€",
    
    # 3. Water Management
    "ìˆ˜ì²˜ë¦¬", "íìˆ˜", "ìˆ˜ì§ˆì˜¤ì—¼", "ëƒ‰ê°ìˆ˜", "ìˆ˜ë ¥ë°œì „", "ê·¸ë¦°ìˆ˜ì†Œ", "ìˆ˜ì „í•´", "í•´ì–‘ìƒíƒœê³„",
    
    # 4. Waste & Byproduct Management
    "íë°°í„°ë¦¬", "ííŒ¨ë„", "íë¸”ë ˆì´ë“œ", "ìì›ìˆœí™˜", "ì¬í™œìš©", "ì¬ì‚¬ìš©",
    "í•µì‹¬ê´‘ë¬¼", "í¬í† ë¥˜", "ìˆœí™˜ê²½ì œ",
    
    # 5. Energy Affordability
    "ì „ê¸°ìš”ê¸ˆ", "ì—ë„ˆì§€ë³µì§€", "SMP", "REC", "PPA", "ê·¸ë¦¬ë“œíŒ¨ë¦¬í‹°", "ì—ë„ˆì§€ë¹ˆê³¤ì¸µ",
    
    # 6. Workforce Health & Safety
    "ì¤‘ëŒ€ì¬í•´", "ì‚°ì—…ì¬í•´", "ê°ì „ì‚¬ê³ ", "ì¶”ë½ì‚¬ê³ ", "ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•", "ì•ˆì „ë³´ê±´",
    
    # 7. End-Use Efficiency & Demand
    "ì—ë„ˆì§€íš¨ìœ¨", "ìˆ˜ìš”ê´€ë¦¬", "DR", "ê°€ìƒë°œì „ì†Œ", "VPP", "ë¶„ì‚°ì—ë„ˆì§€", "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ",
    
    # 8. Critical Incident Management
    "ESSí™”ì¬", "í­ë°œ", "ëŒ€ê·œëª¨ì •ì „", "ë¸”ë™ì•„ì›ƒ", "ìì—°ì¬í•´", "ëŒë¶•ê´´", "ì•ˆì „ì§„ë‹¨",
    
    # 9. Grid Resiliency
    "ì „ë ¥ë§", "ê³„í†µì•ˆì •", "ì¶œë ¥ì œì–´", "ì¶œë ¥ì œí•œ", "ê°„í—ì„±", "ì£¼íŒŒìˆ˜", "ì†¡ë°°ì „ë§",
    
    # 10. Ecological Impacts & Community Relations
    "ì…ì§€ê°ˆë“±", "ì£¼ë¯¼ìˆ˜ìš©ì„±", "í™˜ê²½ì˜í–¥í‰ê°€", "ì‚°ë¦¼í›¼ì†", "ì´ê²©ê±°ë¦¬", "ì†ŒìŒ", "ë¹›ë°˜ì‚¬",
    "ì¡°ë¥˜ì¶©ëŒ", "í•´ì–‘ìƒíƒœê³„", "ê³µì²­íšŒ", "ì´ìµê³µìœ ì œ"
]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í‚¤ì›Œë“œ (deprecated, ìƒˆë¡œìš´ ë°©ì‹ ì‚¬ìš© ê¶Œì¥)
RENEWABLE_KEYWORDS = SASB_ISSUE_KEYWORDS

COMPANIES = ["ë‘ì‚°í“¨ì–¼ì…€", "LS ELECTRIC"]
MAX_ARTICLES_IN_CACHE = 100

# --- Helper Functions ---
def get_redis_client():
    """Helper function to create a Redis client from settings."""
    parsed_url = urlparse(settings.CELERY_BROKER_URL)
    return RedisClient(
        host=parsed_url.hostname or 'localhost',
        port=parsed_url.port or 6379,
        db=int(parsed_url.path[1:]) if parsed_url.path and parsed_url.path[1:] else 0
    )

async def async_analyze_and_cache_news(analysis_service: AnalysisService, keywords: List[str], company_name: Optional[str] = None):
    """ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ ë˜í¼ í•¨ìˆ˜ (ê¸°ì¡´ ë°©ì‹)"""
    return await analysis_service.analyze_and_cache_news(keywords=keywords, company_name=company_name)

async def async_analyze_with_combined_keywords(
    analysis_service: AnalysisService, 
    domain_keywords: List[str],
    issue_keywords: List[str],
    company_name: Optional[str] = None
):
    """ğŸ¯ ì¡°í•© ê²€ìƒ‰ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼ í•¨ìˆ˜ (ê°œì„ ëœ ë°©ì‹)"""
    return await analysis_service.analyze_with_combined_keywords(
        domain_keywords=domain_keywords,
        issue_keywords=issue_keywords, 
        company_name=company_name,
        max_combinations=5  # API í˜¸ì¶œ ì œí•œ
    )

def run_dual_search_analysis(
    redis_client: RedisClient,
    analysis_service: AnalysisService,
    keyword_list: List[str],
    index_redis_key: str,
    result_redis_key: str,
    status_redis_key: str,
    companies: Optional[List[str]] = None,
    search_type: str = "dual"  # "company_sasb", "sasb_only", "dual"
):
    """
    ìƒˆë¡œìš´ ì´ì¤‘ ê²€ìƒ‰ ë¡œì§: 
    1. íšŒì‚¬ + SASB í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰
    2. SASB í‚¤ì›Œë“œ ì „ìš© ê²€ìƒ‰
    ê²°ê³¼ë¥¼ Redisì— ìºì‹œí•©ë‹ˆë‹¤.
    """
    redis_client.set(status_redis_key, "IN_PROGRESS")
    try:
        # 1. Get the index of the keyword to use for this run
        current_index_str = redis_client.get(index_redis_key)
        current_index = int(current_index_str) if current_index_str else 0
        
        keyword_to_search = keyword_list[current_index]
        logging.info(f"ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì‹¤í–‰: '{keyword_to_search}' (íƒ€ì…: {search_type})")

        # 2. Run dual search analysis using proper async handling
        all_new_articles = []
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± (Celery ì›Œì»¤ì—ì„œ ì•ˆì „)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            if search_type in ["company_sasb", "dual"] and companies:
                # ë°©ì‹ 1: íšŒì‚¬ + SASB í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰
                for company in companies:
                    logging.info(f"íšŒì‚¬+SASB ê²€ìƒ‰: {company} + {keyword_to_search}")
                    company_sasb_articles = loop.run_until_complete(
                        async_analyze_and_cache_news(
                            analysis_service,
                            keywords=[keyword_to_search], 
                            company_name=company
                        )
                    )
                    # ê° ê¸°ì‚¬ì— ê²€ìƒ‰ íƒ€ì… ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    for article in company_sasb_articles:
                        article_dict: Dict[str, Any] = article.dict()
                        article_dict['search_type'] = 'company_sasb'
                        article_dict['company'] = company
                        all_new_articles.append(article_dict)
            
            if search_type in ["sasb_only", "dual"]:
                # ë°©ì‹ 2: SASB í‚¤ì›Œë“œ ì „ìš© ê²€ìƒ‰ (íšŒì‚¬ëª… ì—†ìŒ)
                logging.info(f"SASB ì „ìš© ê²€ìƒ‰: {keyword_to_search}")
                sasb_only_articles = loop.run_until_complete(
                    async_analyze_and_cache_news(
                        analysis_service,
                        keywords=[keyword_to_search], 
                        company_name=None
                    )
                )
                # ê° ê¸°ì‚¬ì— ê²€ìƒ‰ íƒ€ì… ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for article in sasb_only_articles:
                    article_dict: Dict[str, Any] = article.dict()
                    article_dict['search_type'] = 'sasb_only'
                    all_new_articles.append(article_dict)
        
        finally:
            loop.close()

        # 3. Cache management
        existing_data_str = redis_client.get(result_redis_key)
        existing_articles = json.loads(existing_data_str) if existing_data_str else []
        
        # 4. Merge and deduplicate
        combined_articles = existing_articles + all_new_articles
        seen_links = set()
        unique_articles = []
        for article in combined_articles:
            link = article.get('link', '')
            if link and link not in seen_links:
                seen_links.add(link)
                unique_articles.append(article)
        
        # 5. Limit the number of articles and save to Redis
        if len(unique_articles) > MAX_ARTICLES_IN_CACHE:
            unique_articles = unique_articles[:MAX_ARTICLES_IN_CACHE]
        
        redis_client.set(result_redis_key, json.dumps(unique_articles, ensure_ascii=False), ex=3600)
        
        # 6. Update the index for the next run
        next_index = (current_index + 1) % len(keyword_list)
        redis_client.set(index_redis_key, str(next_index))
        
        redis_client.set(status_redis_key, "COMPLETED")
        logging.info(f"ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì™„ë£Œ. ì´ {len(unique_articles)}ê°œ ê¸°ì‚¬ ìºì‹œë¨.")
        
        return len(unique_articles)
    
    except Exception as e:
        logging.error(f"ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        redis_client.set(status_redis_key, f"FAILED: {str(e)}")
        return 0


# --- Celery Tasks ---
@celery_app.task
def run_sasb_only_analysis():
    """Celery task for SASB-only renewable energy analysis (í‚¤ì›Œë“œë§Œ ì‚¬ìš©)."""
    logging.info("ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_sasb_only_analysis")
    try:
        redis_client = get_redis_client()
        analysis_service = AnalysisService()
        run_dual_search_analysis(
            redis_client=redis_client,
            analysis_service=analysis_service,
            keyword_list=RENEWABLE_KEYWORDS,
            index_redis_key="sasb_only_keyword_index",
            result_redis_key="latest_sasb_renewable_analysis",
            status_redis_key="status:sasb_renewable_analysis",
            companies=None,  # íšŒì‚¬ëª… ì—†ìŒ
            search_type="sasb_only"  # SASB í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        )
    except Exception as e:
        logging.error(f"run_sasb_only_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

@celery_app.task
def run_companies_dual_analysis():
    """
    Celery task that runs dual search analysis for each company:
    1. Company + SASB keyword combination
    2. SASB keyword only
    """
    logging.info("ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_companies_dual_analysis (ì´ì¤‘ ê²€ìƒ‰)")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()

    for company in COMPANIES:
        try:
            logging.info(f"'{company}'ì— ëŒ€í•œ ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì‹œì‘...")
            
            # Use company-specific Redis keys
            index_redis_key = f"company_dual_keyword_index:{company}"
            result_redis_key = f"latest_companies_renewable_analysis:{company}"
            status_redis_key = f"status:companies_renewable_analysis:{company}"

            run_dual_search_analysis(
                redis_client=redis_client,
                analysis_service=analysis_service,
                keyword_list=RENEWABLE_KEYWORDS,
                index_redis_key=index_redis_key,
                result_redis_key=result_redis_key,
                status_redis_key=status_redis_key,
                companies=[company],  # ë‹¨ì¼ íšŒì‚¬
                search_type="dual"  # ì´ì¤‘ ê²€ìƒ‰: íšŒì‚¬+SASB + SASBë§Œ
            )
            logging.info(f"'{company}'ì— ëŒ€í•œ ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì™„ë£Œ.")
        
        except Exception as e:
            logging.error(f"'{company}'ì— ëŒ€í•œ run_companies_dual_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

@celery_app.task
def run_company_sasb_only_analysis():
    """
    Celery task for company + SASB keyword combination analysis only.
    """
    logging.info("ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_company_sasb_only_analysis")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()

    for company in COMPANIES:
        try:
            logging.info(f"'{company}'ì— ëŒ€í•œ íšŒì‚¬+SASB ë¶„ì„ ì‹œì‘...")
            
            # Use company-specific Redis keys
            index_redis_key = f"company_sasb_keyword_index:{company}"
            result_redis_key = f"latest_company_sasb_analysis:{company}"
            status_redis_key = f"status:company_sasb_analysis:{company}"

            run_dual_search_analysis(
                redis_client=redis_client,
                analysis_service=analysis_service,
                keyword_list=RENEWABLE_KEYWORDS,
                index_redis_key=index_redis_key,
                result_redis_key=result_redis_key,
                status_redis_key=status_redis_key,
                companies=[company],  # ë‹¨ì¼ íšŒì‚¬
                search_type="company_sasb"  # íšŒì‚¬+SASB ì¡°í•©ë§Œ
            )
            logging.info(f"'{company}'ì— ëŒ€í•œ íšŒì‚¬+SASB ë¶„ì„ ì™„ë£Œ.")
        
        except Exception as e:
            logging.error(f"'{company}'ì— ëŒ€í•œ run_company_sasb_only_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

@celery_app.task
def run_combined_keywords_analysis():
    """
    ğŸ¯ ìƒˆë¡œìš´ ì¡°í•© ê²€ìƒ‰ Celery ì‘ì—…
    (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰
    ê´€ë ¨ì„± ë†’ì€ ì‹ ì¬ìƒì—ë„ˆì§€ ì‚°ì—… ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
    """
    logging.info("ğŸ¯ ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_combined_keywords_analysis (ì¡°í•© ê²€ìƒ‰)")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()
    
    try:
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        status_key = "status:combined_keywords_analysis"
        redis_client.set(status_key, "IN_PROGRESS")
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± (Celery ì›Œì»¤ì—ì„œ ì•ˆì „)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            # ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰ (íšŒì‚¬ëª… ì—†ì´)
            logging.info("ğŸ¯ SASB ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            analyzed_articles = loop.run_until_complete(
                async_analyze_with_combined_keywords(
                    analysis_service=analysis_service,
                    domain_keywords=RENEWABLE_DOMAIN_KEYWORDS,
                    issue_keywords=SASB_ISSUE_KEYWORDS,
                    company_name=None  # íšŒì‚¬ëª… ì—†ìŒ
                )
            )
            
            # ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° ì¶”ê°€)
            articles_with_metadata = []
            for article in analyzed_articles:
                article_dict = article.dict()
                article_dict['search_type'] = 'combined_keywords'
                article_dict['search_method'] = 'domain_and_issue_keywords'
                articles_with_metadata.append(article_dict)
            
        finally:
            loop.close()
        
        # Redisì— ê²°ê³¼ ì €ì¥
        result_key = "latest_combined_keywords_analysis"
        if articles_with_metadata:
            # ìµœëŒ€ 100ê°œë¡œ ì œí•œ
            limited_articles = articles_with_metadata[:MAX_ARTICLES_IN_CACHE]
            redis_client.set(
                result_key, 
                json.dumps(limited_articles, ensure_ascii=False), 
                ex=3600  # 1ì‹œê°„ ìºì‹œ
            )
            logging.info(f"ğŸ¯ ì¡°í•© ê²€ìƒ‰ ì™„ë£Œ: {len(limited_articles)}ê°œ ê¸°ì‚¬ ìºì‹œë¨")
        else:
            logging.warning("ğŸ¯ ì¡°í•© ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            redis_client.set(result_key, json.dumps([], ensure_ascii=False), ex=1800)
        
        # ìƒíƒœ ì™„ë£Œ ì—…ë°ì´íŠ¸
        redis_client.set(status_key, "COMPLETED")
        
    except Exception as e:
        logging.error(f"ğŸ¯ run_combined_keywords_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        redis_client.set("status:combined_keywords_analysis", "ERROR")

@celery_app.task 
def run_company_combined_keywords_analysis():
    """
    ğŸ¯ íšŒì‚¬ë³„ ì¡°í•© ê²€ìƒ‰ Celery ì‘ì—…
    (íšŒì‚¬ëª…) + (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰
    """
    logging.info("ğŸ¯ ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_company_combined_keywords_analysis (íšŒì‚¬ë³„ ì¡°í•© ê²€ìƒ‰)")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()
    
    for company in COMPANIES:
        try:
            logging.info(f"ğŸ¯ '{company}'ì— ëŒ€í•œ ì¡°í•© ê²€ìƒ‰ ì‹œì‘...")
            
            # íšŒì‚¬ë³„ ìƒíƒœ í‚¤
            status_key = f"status:company_combined_analysis:{company}"
            result_key = f"latest_company_combined_analysis:{company}"
            redis_client.set(status_key, "IN_PROGRESS")
            
            # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # íšŒì‚¬ë³„ ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰
                analyzed_articles = loop.run_until_complete(
                    async_analyze_with_combined_keywords(
                        analysis_service=analysis_service,
                        domain_keywords=RENEWABLE_DOMAIN_KEYWORDS,
                        issue_keywords=SASB_ISSUE_KEYWORDS,
                        company_name=company
                    )
                )
                
                # ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° ì¶”ê°€)
                articles_with_metadata = []
                for article in analyzed_articles:
                    article_dict = article.dict()
                    article_dict['search_type'] = 'company_combined_keywords'
                    article_dict['search_method'] = 'company_domain_and_issue_keywords'
                    article_dict['company'] = company
                    articles_with_metadata.append(article_dict)
                
            finally:
                loop.close()
            
            # Redisì— ê²°ê³¼ ì €ì¥
            if articles_with_metadata:
                limited_articles = articles_with_metadata[:MAX_ARTICLES_IN_CACHE]
                redis_client.set(
                    result_key, 
                    json.dumps(limited_articles, ensure_ascii=False), 
                    ex=3600
                )
                logging.info(f"ğŸ¯ '{company}' ì¡°í•© ê²€ìƒ‰ ì™„ë£Œ: {len(limited_articles)}ê°œ ê¸°ì‚¬ ìºì‹œë¨")
            else:
                logging.warning(f"ğŸ¯ '{company}' ì¡°í•© ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                redis_client.set(result_key, json.dumps([], ensure_ascii=False), ex=1800)
            
            # ìƒíƒœ ì™„ë£Œ
            redis_client.set(status_key, "COMPLETED")
            
        except Exception as e:
            logging.error(f"ğŸ¯ '{company}' ì¡°í•© ê²€ìƒ‰ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            redis_client.set(f"status:company_combined_analysis:{company}", "ERROR") 