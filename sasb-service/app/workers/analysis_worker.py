import asyncio
import json
import redis
from urllib.parse import urlparse
from typing import Optional, List, Dict, Any
from .celery_app import celery_app
from ..domain.service.analysis_service import AnalysisService
from ..domain.model.sasb_dto import NewsAnalysisRequest
from ..config.settings import settings
import logging

# =============================================================================
# ğŸ¯ ê°œì„ ëœ ì´ì¤‘ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
# ë¬¸ì œ: ê¸°ì¡´ ë‹¨ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì¸í•œ ë¹„ê´€ë ¨ ì‚°ì—… ë‰´ìŠ¤ ê³¼ë‹¤ ìˆ˜ì§‘ 
# í•´ê²°: (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•© ê²€ìƒ‰
# =============================================================================

# ê·¸ë£¹ 1: ì‹ ì¬ìƒì—ë„ˆì§€ ì‚°ì—…/ë¶„ì•¼ í‚¤ì›Œë“œ (Domain Keywords)
# ì´ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ì•¼ë§Œ ì‹ ì¬ìƒì—ë„ˆì§€ ì‚°ì—… ë‰´ìŠ¤ë¡œ ê°„ì£¼
RENEWABLE_DOMAIN_KEYWORDS = [
    # í•µì‹¬ ì—ë„ˆì§€ ë¶„ì•¼
    "ì‹ ì¬ìƒì—ë„ˆì§€", "ì¬ìƒì—ë„ˆì§€", "ì‹ ì—ë„ˆì§€", "ì²­ì •ì—ë„ˆì§€", "ì¹œí™˜ê²½ì—ë„ˆì§€",
    
    # ë°œì „ ê¸°ìˆ ë³„
    "íƒœì–‘ê´‘", "íƒœì–‘ì—´", "í’ë ¥", "ìˆ˜ë ¥", "ìˆ˜ë ¥ë°œì „", "ì¡°ë ¥", "ì§€ì—´", "ë°”ì´ì˜¤ì—ë„ˆì§€", 
    "ë°”ì´ì˜¤ë§¤ìŠ¤", "ë°”ì´ì˜¤ê°€ìŠ¤", "ì—°ë£Œì „ì§€",
    
    # ì—ë„ˆì§€ ì €ì¥ ë° ì¸í”„ë¼
    "ESS", "ì—ë„ˆì§€ì €ì¥ì¥ì¹˜", "ë°°í„°ë¦¬", "ìˆ˜ì†Œ", "ê·¸ë¦°ìˆ˜ì†Œ", "ë¸”ë£¨ìˆ˜ì†Œ", "ì•”ëª¨ë‹ˆì•„",
    
    # ì „ë ¥ ì‚°ì—…
    "ë°œì „ì†Œ", "ë°œì „ì‚¬", "ë°œì „ê³µê¸°ì—…", "ì „ë ¥", "ì „ë ¥ê³µì‚¬", "í•œì „", "ì „ë ¥ê±°ë˜ì†Œ", 
    "ì†¡ì „", "ë°°ì „", "ì „ë ¥ë§", "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ë§ˆì´í¬ë¡œê·¸ë¦¬ë“œ",
    
    # ì—ë„ˆì§€ ì „í™˜
    "ì—ë„ˆì§€ì „í™˜", "ì „ì›ë¯¹ìŠ¤", "ì „ì›êµ¬ì„±", "ì—ë„ˆì§€ë¯¹ìŠ¤", "RE100", "K-RE100",
    
    # ê´€ë ¨ ê¸°ì—…/ê¸°ê´€
    "ì—ë„ˆì§€ê³µì‚¬", "ë°œì „íšŒì‚¬", "ì „ë ¥íšŒì‚¬", "ì—ë„ˆì§€ê¸°ì—…", "ì „ë ¥ì‚°ì—…"
]

# ê·¸ë£¹ 2: SASB ì´ìŠˆ í‚¤ì›Œë“œ (Issue Keywords) - ê¸°ì¡´ 53ê°œ í‚¤ì›Œë“œ ìœ ì§€
SASB_ISSUE_KEYWORDS = [
    # 1. Greenhouse Gas Emissions & Energy Resource Planning
    "íƒ„ì†Œì¤‘ë¦½", "íƒ„ì†Œë°°ì¶œ", "ì˜¨ì‹¤ê°€ìŠ¤", "RE100", "CF100", "ì—ë„ˆì§€ë¯¹ìŠ¤", "ì „ì›êµ¬ì„±",
    "íƒ„ì†Œêµ­ê²½ì„¸", "ìŠ¤ì½”í”„", "ê°ì¶•ëª©í‘œ", "NDC", "ìë°œì  íƒ„ì†Œì‹œì¥",
    
    # 2. Air Quality  
    "ë¯¸ì„¸ë¨¼ì§€", "ëŒ€ê¸°ì˜¤ì—¼", "í™©ì‚°í™”ë¬¼", "ì§ˆì†Œì‚°í™”ë¬¼", "ë°”ì´ì˜¤ë§¤ìŠ¤", "ë¹„ì‚°ë¨¼ì§€",
    
    # 3. Water Management
    "ìˆ˜ì²˜ë¦¬", "íìˆ˜", "ìˆ˜ì§ˆì˜¤ì—¼", "ëƒ‰ê°ìˆ˜", "ìˆ˜ë ¥ë°œì „", "ê·¸ë¦°ìˆ˜ì†Œ", "ìˆ˜ì „í•´", "í•´ì–‘ìƒíƒœê³„",
    
    # 4. Waste & Byproduct Management
    "íë°°í„°ë¦¬", "ííŒ¨ë„", "íë¸”ë ˆì´ë“œ", "ìì›ìˆœí™˜", "ì¬í™œìš©", "ì¬ì‚¬ìš©",
    "í•µì‹¬ê´‘ë¬¼", "í¬í† ë¥˜", "ìˆœí™˜ê²½ì œ",
    
    # 5. Energy Affordability
    "ì „ê¸°ìš”ê¸ˆ", "ì—ë„ˆì§€ë³µì§€", "SMP", "REC", "PPA", "ê·¸ë¦¬ë“œíŒ¨ë¦¬í‹°", "ì—ë„ˆì§€ë¹ˆê³¤ì¸µ",
    
    # 6. Workforce Health & Safety
    "ì¤‘ëŒ€ì¬í•´", "ì‚°ì—…ì¬í•´", "ê°ì „ì‚¬ê³ ", "ì¶”ë½ì‚¬ê³ ", "ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•", "ì•ˆì „ë³´ê±´",
    
    # 7. End-Use Efficiency & Demand
    "ì—ë„ˆì§€íš¨ìœ¨", "ìˆ˜ìš”ê´€ë¦¬", "DR", "ê°€ìƒë°œì „ì†Œ", "VPP", "ë¶„ì‚°ì—ë„ˆì§€", "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ",
    
    # 8. Critical Incident Management
    "ESSí™”ì¬", "í­ë°œ", "ëŒ€ê·œëª¨ì •ì „", "ë¸”ë™ì•„ì›ƒ", "ìì—°ì¬í•´", "ëŒë¶•ê´´", "ì•ˆì „ì§„ë‹¨",
    
    # 9. Grid Resiliency
    "ì „ë ¥ë§", "ê³„í†µì•ˆì •", "ì¶œë ¥ì œì–´", "ì¶œë ¥ì œí•œ", "ê°„í—ì„±", "ì£¼íŒŒìˆ˜", "ì†¡ë°°ì „ë§",
    
    # 10. Ecological Impacts & Community Relations
    "ì…ì§€ê°ˆë“±", "ì£¼ë¯¼ìˆ˜ìš©ì„±", "í™˜ê²½ì˜í–¥í‰ê°€", "ì‚°ë¦¼í›¼ì†", "ì´ê²©ê±°ë¦¬", "ì†ŒìŒ", "ë¹›ë°˜ì‚¬",
    "ì¡°ë¥˜ì¶©ëŒ", "í•´ì–‘ìƒíƒœê³„", "ê³µì²­íšŒ", "ì´ìµê³µìœ ì œ"
]

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í‚¤ì›Œë“œ (deprecated, ìƒˆë¡œìš´ ë°©ì‹ ì‚¬ìš© ê¶Œì¥)
RENEWABLE_KEYWORDS = SASB_ISSUE_KEYWORDS

COMPANIES = ["ë‘ì‚°í“¨ì–¼ì…€", "LS ELECTRIC"]
MAX_ARTICLES_IN_CACHE = 100

# âœ… Python Path ì„¤ì • (shared ëª¨ë“ˆ ì ‘ê·¼ìš©)
import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))))

# âœ… ê³µí†µ Redis íŒ©í† ë¦¬ ì‚¬ìš©
from shared.core.redis_factory import RedisClientFactory

# --- Helper Functions ---
def get_redis_client():
    """Redis í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ê³µí†µ íŒ©í† ë¦¬ ì‚¬ìš©)"""
    return RedisClientFactory.create_from_url(settings.CELERY_BROKER_URL)

async def async_analyze_and_cache_news(analysis_service: AnalysisService, keywords: List[str], company_name: Optional[str] = None):
    """ë¹„ë™ê¸° ë‰´ìŠ¤ ë¶„ì„ ë˜í¼ í•¨ìˆ˜ (ê¸°ì¡´ ë°©ì‹)"""
    return await analysis_service.analyze_and_cache_news(keywords=keywords, company_name=company_name)

async def async_analyze_with_combined_keywords(
    analysis_service: AnalysisService, 
    domain_keywords: List[str],
    issue_keywords: List[str],
    company_name: Optional[str] = None
):
    """ğŸ¯ ì¡°í•© ê²€ìƒ‰ì„ ìœ„í•œ ë¹„ë™ê¸° ë˜í¼ í•¨ìˆ˜ (ê°œì„ ëœ ë°©ì‹)"""
    return await analysis_service.analyze_with_combined_keywords(
        domain_keywords=domain_keywords,
        issue_keywords=issue_keywords, 
        company_name=company_name,
        max_combinations=5  # API í˜¸ì¶œ ì œí•œ
    )

def run_dual_search_analysis(
    redis_client: redis.Redis,
    analysis_service: AnalysisService,
    keyword_list: List[str],
    index_redis_key: str,
    result_redis_key: str,
    status_redis_key: str,
    companies: Optional[List[str]] = None,
    search_type: str = "dual"  # "company_sasb", "sasb_only", "dual"
):
    """
    ìƒˆë¡œìš´ ì´ì¤‘ ê²€ìƒ‰ ë¡œì§ (ë¦¬íŒ©í† ë§ë¨)
    ê³µí†µ í—¬í¼ í´ë˜ìŠ¤ ì‚¬ìš©ìœ¼ë¡œ 112ì¤„ â†’ 30ì¤„ë¡œ ë‹¨ì¶•
    """
    from shared.services.worker_helper import DualSearchHelper, CacheManager, AsyncWorkflowManager
    
    # 1. ìƒíƒœ ì´ˆê¸°í™” ë° í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì¡°íšŒ
    CacheManager.update_status(redis_client, status_redis_key, "IN_PROGRESS")
    current_index, keyword_to_search = DualSearchHelper.get_current_keyword_index(
        redis_client, index_redis_key, keyword_list
    )
    
    try:
        # 2. ë¹„ë™ê¸° ì´ì¤‘ ê²€ìƒ‰ ì‹¤í–‰
        all_new_articles = _execute_dual_search_with_event_loop(
            analysis_service, keyword_to_search, companies, search_type
        )
        
        # 3. ìºì‹œ ê´€ë¦¬: ê¸°ì¡´ ê¸°ì‚¬ì™€ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        existing_articles = CacheManager.get_existing_articles(redis_client, result_redis_key)
        unique_articles = DualSearchHelper.merge_and_deduplicate_articles(
            existing_articles, all_new_articles
        )
        
        # 4. Redisì— ì €ì¥ ë° ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        total_articles = CacheManager.save_articles_to_cache(
            redis_client, result_redis_key, unique_articles, MAX_ARTICLES_IN_CACHE
        )
        DualSearchHelper.update_keyword_index(redis_client, index_redis_key, current_index, keyword_list)
        
        # 5. ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        CacheManager.update_status(redis_client, status_redis_key, "COMPLETED")
        logging.info(f"ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì™„ë£Œ. ì´ {total_articles}ê°œ ê¸°ì‚¬ ìºì‹œë¨.")
        return total_articles
        
    except Exception as e:
        logging.error(f"ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        CacheManager.update_status(redis_client, status_redis_key, f"FAILED: {str(e)}")
        return 0


def _execute_dual_search_with_event_loop(analysis_service, keyword: str, companies: Optional[List[str]], search_type: str):
    """ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ë©´ì„œ ì´ì¤‘ ê²€ìƒ‰ ì‹¤í–‰"""
    from shared.services.worker_helper import AsyncWorkflowManager
    
    loop = AsyncWorkflowManager.create_safe_event_loop()
    try:
        return loop.run_until_complete(
            AsyncWorkflowManager.run_dual_search_workflow(
                analysis_service, keyword, companies, search_type
            )
        )
    finally:
        AsyncWorkflowManager.close_event_loop(loop)


# --- Celery Tasks ---
@celery_app.task
def run_sasb_only_analysis():
    """Celery task for SASB-only renewable energy analysis (í‚¤ì›Œë“œë§Œ ì‚¬ìš©)."""
    logging.info("ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_sasb_only_analysis")
    try:
        redis_client = get_redis_client()
        analysis_service = AnalysisService()
        run_dual_search_analysis(
            redis_client=redis_client,
            analysis_service=analysis_service,
            keyword_list=RENEWABLE_KEYWORDS,
            index_redis_key="sasb_only_keyword_index",
            result_redis_key="latest_sasb_renewable_analysis",
            status_redis_key="status:sasb_renewable_analysis",
            companies=None,  # íšŒì‚¬ëª… ì—†ìŒ
            search_type="sasb_only"  # SASB í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        )
    except Exception as e:
        logging.error(f"run_sasb_only_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

@celery_app.task
def run_companies_dual_analysis():
    """
    Celery task that runs dual search analysis for each company:
    1. Company + SASB keyword combination
    2. SASB keyword only
    """
    logging.info("ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_companies_dual_analysis (ì´ì¤‘ ê²€ìƒ‰)")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()

    for company in COMPANIES:
        try:
            logging.info(f"'{company}'ì— ëŒ€í•œ ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì‹œì‘...")
            
            # Use company-specific Redis keys
            index_redis_key = f"company_dual_keyword_index:{company}"
            result_redis_key = f"latest_companies_renewable_analysis:{company}"
            status_redis_key = f"status:companies_renewable_analysis:{company}"

            run_dual_search_analysis(
                redis_client=redis_client,
                analysis_service=analysis_service,
                keyword_list=RENEWABLE_KEYWORDS,
                index_redis_key=index_redis_key,
                result_redis_key=result_redis_key,
                status_redis_key=status_redis_key,
                companies=[company],  # ë‹¨ì¼ íšŒì‚¬
                search_type="dual"  # ì´ì¤‘ ê²€ìƒ‰: íšŒì‚¬+SASB + SASBë§Œ
            )
            logging.info(f"'{company}'ì— ëŒ€í•œ ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ ì™„ë£Œ.")
        
        except Exception as e:
            logging.error(f"'{company}'ì— ëŒ€í•œ run_companies_dual_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

@celery_app.task
def run_company_sasb_only_analysis():
    """
    Celery task for company + SASB keyword combination analysis only.
    """
    logging.info("ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_company_sasb_only_analysis")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()

    for company in COMPANIES:
        try:
            logging.info(f"'{company}'ì— ëŒ€í•œ íšŒì‚¬+SASB ë¶„ì„ ì‹œì‘...")
            
            # Use company-specific Redis keys
            index_redis_key = f"company_sasb_keyword_index:{company}"
            result_redis_key = f"latest_company_sasb_analysis:{company}"
            status_redis_key = f"status:company_sasb_analysis:{company}"

            run_dual_search_analysis(
                redis_client=redis_client,
                analysis_service=analysis_service,
                keyword_list=RENEWABLE_KEYWORDS,
                index_redis_key=index_redis_key,
                result_redis_key=result_redis_key,
                status_redis_key=status_redis_key,
                companies=[company],  # ë‹¨ì¼ íšŒì‚¬
                search_type="company_sasb"  # íšŒì‚¬+SASB ì¡°í•©ë§Œ
            )
            logging.info(f"'{company}'ì— ëŒ€í•œ íšŒì‚¬+SASB ë¶„ì„ ì™„ë£Œ.")
        
        except Exception as e:
            logging.error(f"'{company}'ì— ëŒ€í•œ run_company_sasb_only_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

@celery_app.task
def run_combined_keywords_analysis():
    """
    ğŸ¯ ìƒˆë¡œìš´ ì¡°í•© ê²€ìƒ‰ Celery ì‘ì—…
    (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰
    ê´€ë ¨ì„± ë†’ì€ ì‹ ì¬ìƒì—ë„ˆì§€ ì‚°ì—… ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
    """
    logging.info("ğŸ¯ ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_combined_keywords_analysis (ì¡°í•© ê²€ìƒ‰)")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()
    
    try:
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        status_key = "status:combined_keywords_analysis"
        redis_client.set(status_key, "IN_PROGRESS")
        
        # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± (Celery ì›Œì»¤ì—ì„œ ì•ˆì „)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            # ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰ (íšŒì‚¬ëª… ì—†ì´)
            logging.info("ğŸ¯ SASB ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            analyzed_articles = loop.run_until_complete(
                async_analyze_with_combined_keywords(
                    analysis_service=analysis_service,
                    domain_keywords=RENEWABLE_DOMAIN_KEYWORDS,
                    issue_keywords=SASB_ISSUE_KEYWORDS,
                    company_name=None  # íšŒì‚¬ëª… ì—†ìŒ
                )
            )
            
            # ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° ì¶”ê°€)
            articles_with_metadata = []
            for article in analyzed_articles:
                article_dict = article.dict()
                article_dict['search_type'] = 'combined_keywords'
                article_dict['search_method'] = 'domain_and_issue_keywords'
                articles_with_metadata.append(article_dict)
            
        finally:
            loop.close()
        
        # Redisì— ê²°ê³¼ ì €ì¥
        result_key = "latest_combined_keywords_analysis"
        if articles_with_metadata:
            # ìµœëŒ€ 100ê°œë¡œ ì œí•œ
            limited_articles = articles_with_metadata[:MAX_ARTICLES_IN_CACHE]
            redis_client.set(
                result_key, 
                json.dumps(limited_articles, ensure_ascii=False), 
                ex=3600  # 1ì‹œê°„ ìºì‹œ
            )
            logging.info(f"ğŸ¯ ì¡°í•© ê²€ìƒ‰ ì™„ë£Œ: {len(limited_articles)}ê°œ ê¸°ì‚¬ ìºì‹œë¨")
        else:
            logging.warning("ğŸ¯ ì¡°í•© ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            redis_client.set(result_key, json.dumps([], ensure_ascii=False), ex=1800)
        
        # ìƒíƒœ ì™„ë£Œ ì—…ë°ì´íŠ¸
        redis_client.set(status_key, "COMPLETED")
        
    except Exception as e:
        logging.error(f"ğŸ¯ run_combined_keywords_analysisì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        redis_client.set("status:combined_keywords_analysis", "ERROR")

@celery_app.task 
def run_company_combined_keywords_analysis():
    """
    ğŸ¯ íšŒì‚¬ë³„ ì¡°í•© ê²€ìƒ‰ Celery ì‘ì—…
    (íšŒì‚¬ëª…) + (ì‚°ì—… í‚¤ì›Œë“œ) AND (SASB ì´ìŠˆ í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰
    """
    logging.info("ğŸ¯ ì‹¤í–‰ ì˜ˆì•½ëœ ì‘ì—…: run_company_combined_keywords_analysis (íšŒì‚¬ë³„ ì¡°í•© ê²€ìƒ‰)")
    redis_client = get_redis_client()
    analysis_service = AnalysisService()
    
    for company in COMPANIES:
        try:
            logging.info(f"ğŸ¯ '{company}'ì— ëŒ€í•œ ì¡°í•© ê²€ìƒ‰ ì‹œì‘...")
            
            # íšŒì‚¬ë³„ ìƒíƒœ í‚¤
            status_key = f"status:company_combined_analysis:{company}"
            result_key = f"latest_company_combined_analysis:{company}"
            redis_client.set(status_key, "IN_PROGRESS")
            
            # ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # íšŒì‚¬ë³„ ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰
                analyzed_articles = loop.run_until_complete(
                    async_analyze_with_combined_keywords(
                        analysis_service=analysis_service,
                        domain_keywords=RENEWABLE_DOMAIN_KEYWORDS,
                        issue_keywords=SASB_ISSUE_KEYWORDS,
                        company_name=company
                    )
                )
                
                # ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° ì¶”ê°€)
                articles_with_metadata = []
                for article in analyzed_articles:
                    article_dict = article.dict()
                    article_dict['search_type'] = 'company_combined_keywords'
                    article_dict['search_method'] = 'company_domain_and_issue_keywords'
                    article_dict['company'] = company
                    articles_with_metadata.append(article_dict)
                
            finally:
                loop.close()
            
            # Redisì— ê²°ê³¼ ì €ì¥
            if articles_with_metadata:
                limited_articles = articles_with_metadata[:MAX_ARTICLES_IN_CACHE]
                redis_client.set(
                    result_key, 
                    json.dumps(limited_articles, ensure_ascii=False), 
                    ex=3600
                )
                logging.info(f"ğŸ¯ '{company}' ì¡°í•© ê²€ìƒ‰ ì™„ë£Œ: {len(limited_articles)}ê°œ ê¸°ì‚¬ ìºì‹œë¨")
            else:
                logging.warning(f"ğŸ¯ '{company}' ì¡°í•© ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                redis_client.set(result_key, json.dumps([], ensure_ascii=False), ex=1800)
            
            # ìƒíƒœ ì™„ë£Œ
            redis_client.set(status_key, "COMPLETED")
            
        except Exception as e:
            logging.error(f"ğŸ¯ '{company}' ì¡°í•© ê²€ìƒ‰ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            redis_client.set(f"status:company_combined_analysis:{company}", "ERROR") 