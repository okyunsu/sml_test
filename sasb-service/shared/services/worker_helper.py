"""
ì›Œì»¤ í—¬í¼ ëª¨ë“ˆ
SASB Workerì—ì„œ ì‚¬ìš©í•˜ëŠ” ë³µì¡í•œ ë¶„ì„ ë¡œì§ì„ ì§€ì›í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ë“¤
"""
import asyncio
import json
import logging
from typing import List, Optional, Dict, Any, Set

logger = logging.getLogger(__name__)

class DualSearchHelper:
    """ì´ì¤‘ ê²€ìƒ‰ ë¶„ì„ í—¬í¼ í´ë˜ìŠ¤"""
    
    @staticmethod
    def get_current_keyword_index(redis_client, index_redis_key: str, keyword_list: List[str]) -> tuple:
        """í˜„ì¬ í‚¤ì›Œë“œ ì¸ë±ìŠ¤ì™€ í‚¤ì›Œë“œ ë°˜í™˜"""
        current_index_str = redis_client.get(index_redis_key)
        current_index = int(current_index_str) if current_index_str else 0
        keyword_to_search = keyword_list[current_index]
        
        logger.info(f"í˜„ì¬ í‚¤ì›Œë“œ ì¸ë±ìŠ¤: {current_index}, í‚¤ì›Œë“œ: '{keyword_to_search}'")
        return current_index, keyword_to_search
    
    @staticmethod
    async def execute_company_sasb_search(
        analysis_service, 
        keyword: str, 
        companies: List[str]
    ) -> List[Dict[str, Any]]:
        """íšŒì‚¬ + SASB í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰ ì‹¤í–‰"""
        all_articles = []
        
        for company in companies:
            logger.info(f"íšŒì‚¬+SASB ê²€ìƒ‰: {company} + {keyword}")
            
            # async_analyze_and_cache_news í•¨ìˆ˜ í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ì€ ì„œë¹„ìŠ¤ì— ë”°ë¼ ë‹¤ë¦„)
            company_sasb_articles = await DualSearchHelper._analyze_and_cache_news(
                analysis_service, keywords=[keyword], company_name=company
            )
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for article in company_sasb_articles:
                article_dict = article.dict() if hasattr(article, 'dict') else dict(article)
                article_dict['search_type'] = 'company_sasb'
                article_dict['company'] = company
                all_articles.append(article_dict)
        
        logger.info(f"íšŒì‚¬+SASB ê²€ìƒ‰ ì™„ë£Œ: {len(all_articles)}ê°œ ê¸°ì‚¬")
        return all_articles
    
    @staticmethod
    async def execute_sasb_only_search(
        analysis_service, 
        keyword: str
    ) -> List[Dict[str, Any]]:
        """SASB í‚¤ì›Œë“œ ì „ìš© ê²€ìƒ‰ ì‹¤í–‰"""
        logger.info(f"SASB ì „ìš© ê²€ìƒ‰: {keyword}")
        
        sasb_only_articles = await DualSearchHelper._analyze_and_cache_news(
            analysis_service, keywords=[keyword], company_name=None
        )
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        all_articles = []
        for article in sasb_only_articles:
            article_dict = article.dict() if hasattr(article, 'dict') else dict(article)
            article_dict['search_type'] = 'sasb_only'
            all_articles.append(article_dict)
        
        logger.info(f"SASB ì „ìš© ê²€ìƒ‰ ì™„ë£Œ: {len(all_articles)}ê°œ ê¸°ì‚¬")
        return all_articles
    
    @staticmethod
    async def _analyze_and_cache_news(analysis_service, keywords: List[str], company_name: Optional[str]):
        """ë‰´ìŠ¤ ë¶„ì„ ë° ìºì‹œ (ì‹¤ì œ êµ¬í˜„ì€ ì„œë¹„ìŠ¤ë³„ë¡œ ë‹¤ë¦„)"""
        # ì´ í•¨ìˆ˜ëŠ” ì›ë³¸ async_analyze_and_cache_news í•¨ìˆ˜ë¥¼ ì°¸ì¡°í•´ì•¼ í•¨
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” importë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©
        # ì„ì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ ì‹œ êµì²´ í•„ìš”)
        return []
    
    @staticmethod
    def merge_and_deduplicate_articles(
        existing_articles: List[Dict[str, Any]], 
        new_articles: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ğŸ¯ ìœ ì‚¬ë„ ê¸°ë°˜ ê¸°ì‚¬ ë³‘í•© ë° ì¤‘ë³µ ì œê±°"""
        from shared.services.news_search_helper import NewsSearchHelper
        
        combined_articles = existing_articles + new_articles
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (60% ì„ê³„ê°’)
        unique_articles = NewsSearchHelper.deduplicate_news_by_similarity(
            combined_articles, 
            similarity_threshold=0.6
        )
        
        logger.info(f"ğŸ¯ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(combined_articles)}ê°œ â†’ {len(unique_articles)}ê°œ")
        return unique_articles
    
    @staticmethod
    def update_keyword_index(redis_client, index_redis_key: str, current_index: int, keyword_list: List[str]):
        """ë‹¤ìŒ í‚¤ì›Œë“œ ì¸ë±ìŠ¤ë¡œ ì—…ë°ì´íŠ¸"""
        next_index = (current_index + 1) % len(keyword_list)
        redis_client.set(index_redis_key, str(next_index))
        logger.info(f"í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸: {current_index} â†’ {next_index}")

class CacheManager:
    """Redis ìºì‹œ ê´€ë¦¬ í—¬í¼ í´ë˜ìŠ¤"""
    
    @staticmethod
    def get_existing_articles(redis_client, result_redis_key: str) -> List[Dict[str, Any]]:
        """ê¸°ì¡´ ìºì‹œëœ ê¸°ì‚¬ë“¤ ì¡°íšŒ"""
        try:
            existing_data_str = redis_client.get(result_redis_key)
            existing_articles = json.loads(existing_data_str) if existing_data_str else []
            logger.info(f"ê¸°ì¡´ ìºì‹œëœ ê¸°ì‚¬: {len(existing_articles)}ê°œ")
            return existing_articles
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    @staticmethod
    def save_articles_to_cache(
        redis_client, 
        result_redis_key: str, 
        articles: List[Dict[str, Any]], 
        max_articles: int = 100,
        expire_seconds: int = 3600
    ) -> int:
        """ê¸°ì‚¬ë“¤ì„ Redisì— ìºì‹œ"""
        try:
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            if len(articles) > max_articles:
                articles = articles[:max_articles]
            
            # Redisì— ì €ì¥
            redis_client.set(
                result_redis_key, 
                json.dumps(articles, ensure_ascii=False), 
                ex=expire_seconds
            )
            
            logger.info(f"Redisì— {len(articles)}ê°œ ê¸°ì‚¬ ìºì‹œ ì™„ë£Œ")
            return len(articles)
            
        except Exception as e:
            logger.error(f"Redis ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0
    
    @staticmethod
    def update_status(redis_client, status_redis_key: str, status: str):
        """ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            redis_client.set(status_redis_key, status)
            logger.info(f"ìƒíƒœ ì—…ë°ì´íŠ¸: {status}")
        except Exception as e:
            logger.error(f"ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

class AsyncWorkflowManager:
    """ë¹„ë™ê¸° ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def create_safe_event_loop():
        """Celery ì›Œì»¤ì—ì„œ ì•ˆì „í•œ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±"""
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            logger.info("ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ì™„ë£Œ")
            return loop
        except Exception as e:
            logger.error(f"ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    @staticmethod
    def close_event_loop(loop):
        """ì´ë²¤íŠ¸ ë£¨í”„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ"""
        try:
            if loop and not loop.is_closed():
                loop.close()
                logger.info("ì´ë²¤íŠ¸ ë£¨í”„ ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì´ë²¤íŠ¸ ë£¨í”„ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
    
    @staticmethod
    async def run_dual_search_workflow(
        analysis_service,
        keyword: str,
        companies: Optional[List[str]],
        search_type: str
    ) -> List[Dict[str, Any]]:
        """ì´ì¤‘ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        all_new_articles = []
        
        try:
            # íšŒì‚¬ + SASB ê²€ìƒ‰
            if search_type in ["company_sasb", "dual"] and companies:
                company_articles = await DualSearchHelper.execute_company_sasb_search(
                    analysis_service, keyword, companies
                )
                all_new_articles.extend(company_articles)
            
            # SASB ì „ìš© ê²€ìƒ‰  
            if search_type in ["sasb_only", "dual"]:
                sasb_articles = await DualSearchHelper.execute_sasb_only_search(
                    analysis_service, keyword
                )
                all_new_articles.extend(sasb_articles)
            
            logger.info(f"ì´ì¤‘ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: ì´ {len(all_new_articles)}ê°œ ê¸°ì‚¬")
            return all_new_articles
            
        except Exception as e:
            logger.error(f"ì´ì¤‘ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}")
            raise 