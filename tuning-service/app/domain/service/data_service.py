import os
import re
import warnings
from typing import List, Dict, Any, Optional
import logging
import pandas as pd

from pdfminer.high_level import extract_text
from konlpy.tag import Okt
from sklearn.model_selection import train_test_split

from app.domain.model.tuning_request import TaskType
from app.domain.service.gri_data_service import GRIDataService

logger = logging.getLogger(__name__)

# PDF ì²˜ë¦¬ ê´€ë ¨ ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore", message=".*Cannot set gray.*color.*")
warnings.filterwarnings("ignore", message=".*invalid float value.*")

# pdfminer ë¡œê±° ë ˆë²¨ ì¡°ì •
logging.getLogger('pdfminer').setLevel(logging.ERROR)

class DataService:
    def __init__(self):
        self.okt = Okt()
        # GRI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.gri_service = GRIDataService()
        
        # ê¸°ë³¸ ESG í‚¤ì›Œë“œ (GRIë¡œ í™•ì¥ ì˜ˆì •)
        self.esg_keywords = {
            "í™˜ê²½": ["í™˜ê²½", "ê¸°í›„", "íƒ„ì†Œ", "ì˜¨ì‹¤ê°€ìŠ¤", "ì¬ìƒì—ë„ˆì§€", "ì¹œí™˜ê²½", "ì§€ì†ê°€ëŠ¥", "ë…¹ìƒ‰"],
            "ì‚¬íšŒ": ["ì‚¬íšŒ", "ì„ì§ì›", "ì¸ê¶Œ", "ë‹¤ì–‘ì„±", "ì•ˆì „", "ë³´ê±´", "ì§€ì—­ì‚¬íšŒ", "ì‚¬íšŒê³µí—Œ"],
            "ì§€ë°°êµ¬ì¡°": ["ì§€ë°°êµ¬ì¡°", "ì´ì‚¬íšŒ", "íˆ¬ëª…ì„±", "ìœ¤ë¦¬", "ì»´í”Œë¼ì´ì–¸ìŠ¤", "ë¦¬ìŠ¤í¬", "ê°ì‚¬"]
        }
        
        # GRI ê¸°ë°˜ í™•ì¥ í‚¤ì›Œë“œ (ëŸ°íƒ€ì„ì— ë¡œë“œ)
        self.enhanced_keywords = None
        self.gri_mapping = None
    
    async def initialize_gri_keywords(self):
        """GRI ê¸°ë°˜ í‚¤ì›Œë“œ ì´ˆê¸°í™”"""
        try:
            if self.gri_service.gri_standards:
                enhanced_mapping = await self.gri_service.create_enhanced_keyword_mapping()
                self.enhanced_keywords = enhanced_mapping["enhanced_keywords"]
                self.gri_mapping = enhanced_mapping["keyword_to_gri_mapping"]
                logger.info("âœ… GRI ê¸°ë°˜ í‚¤ì›Œë“œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.info("â„¹ï¸ GRI ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©")
        except Exception as e:
            logger.warning(f"âš ï¸ GRI í‚¤ì›Œë“œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©: {str(e)}")
    
    def get_active_keywords(self) -> Dict[str, List[str]]:
        """í™œì„± í‚¤ì›Œë“œ ë°˜í™˜ (GRI í™•ì¥ ë˜ëŠ” ê¸°ë³¸)"""
        return self.enhanced_keywords if self.enhanced_keywords else self.esg_keywords
    
    def get_gri_standard_for_keyword(self, keyword: str) -> Optional[str]:
        """í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” GRI í‘œì¤€ ë°˜í™˜"""
        if self.gri_mapping:
            return self.gri_mapping.get(keyword)
        return None
    
    async def extract_data_from_reports(
        self, 
        report_files: List[str], 
        task_type: TaskType
    ) -> pd.DataFrame:
        """ESG ë³´ê³ ì„œ ë˜ëŠ” GRI í•™ìŠµ ë°ì´í„°ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ê¸°ì¡´ PDF ì²˜ë¦¬ ë¡œì§
            all_data = []
            total_files = len(report_files)
            
            logger.info(f"ğŸ“Š ì´ {total_files}ê°œì˜ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
            
            for idx, file_path in enumerate(report_files, 1):
                file_name = os.path.basename(file_path)
                logger.info(f"ğŸ“„ [{idx}/{total_files}] ì²˜ë¦¬ ì¤‘: {file_name}")
                
                # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = await self._extract_text_from_pdf(file_path)
                
                if not text or len(text.strip()) < 100:
                    logger.warning(f"âš ï¸  [{idx}/{total_files}] {file_name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±")
                    continue
                
                logger.info(f"âœ… [{idx}/{total_files}] {file_name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(text):,} ë¬¸ì)")
                
                # ì‘ì—… íƒ€ì…ì— ë”°ë¥¸ ë°ì´í„° ìƒì„±
                if task_type == TaskType.CLASSIFICATION:
                    data = await self._create_classification_data(text)
                elif task_type == TaskType.QUESTION_ANSWERING:
                    data = await self._create_qa_data(text)
                elif task_type == TaskType.SUMMARIZATION:
                    data = await self._create_summarization_data(text)
                else:
                    data = await self._create_generation_data(text)
                
                logger.info(f"ğŸ”„ [{idx}/{total_files}] {file_name}: {len(data)}ê°œ í›ˆë ¨ ìƒ˜í”Œ ìƒì„±")
                all_data.extend(data)
                
                # ì§„í–‰ë¥  ê³„ì‚°
                progress = (idx / total_files) * 100
                logger.info(f"ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {progress:.1f}% ({idx}/{total_files} ì™„ë£Œ)")
            
            df = pd.DataFrame(all_data)
            logger.info(f"ğŸ‰ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ! ì´ {len(df):,}ê°œ í›ˆë ¨ ìƒ˜í”Œ ìƒì„±")
            return df
            
        except Exception as e:
            logger.error(f"âŒ ë³´ê³ ì„œ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise ValueError(f"ë³´ê³ ì„œ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def _load_gri_learning_dataset(self, dataset_path: str, task_type: TaskType) -> pd.DataFrame:
        """GRI í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ ë° Hugging Face í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            logger.info(f"ğŸ“– GRI í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {dataset_path}")
            
            # CSV íŒŒì¼ ë¡œë“œ
            df = pd.read_csv(dataset_path, encoding='utf-8')
            logger.info(f"ğŸ“Š ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(df)} ìƒ˜í”Œ")
            
            # Hugging Face í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if task_type == TaskType.TEXT_GENERATION:
                # í…ìŠ¤íŠ¸ ìƒì„±ìš© í˜•ì‹: input_text -> target_text
                hf_data = []
                for _, row in df.iterrows():
                    hf_data.append({
                        "text": f"ì§ˆë¬¸: {row['input_text']}\në‹µë³€: {row['target_text']}"
                    })
                
                hf_df = pd.DataFrame(hf_data)
                logger.info(f"âœ… GRI í…ìŠ¤íŠ¸ ìƒì„± ë°ì´í„°ì…‹ ë³€í™˜ ì™„ë£Œ: {len(hf_df)} ìƒ˜í”Œ")
                return hf_df
                
            elif task_type == TaskType.QUESTION_ANSWERING:
                # ì§ˆì˜ì‘ë‹µìš© í˜•ì‹
                hf_data = []
                for _, row in df.iterrows():
                    hf_data.append({
                        "question": row['input_text'],
                        "context": row['target_text'],
                        "answers": {
                            "text": [row['target_text'][:200]],  # ë‹µë³€ ì¼ë¶€
                            "answer_start": [0]
                        }
                    })
                
                hf_df = pd.DataFrame(hf_data)
                logger.info(f"âœ… GRI ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹ ë³€í™˜ ì™„ë£Œ: {len(hf_df)} ìƒ˜í”Œ")
                return hf_df
                
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±ìœ¼ë¡œ ì²˜ë¦¬
                return await self._load_gri_learning_dataset(dataset_path, TaskType.TEXT_GENERATION)
                
        except Exception as e:
            logger.error(f"âŒ GRI í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"GRI í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    async def _extract_text_from_pdf(self, file_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # í‘œì¤€ ì¶œë ¥/ì—ëŸ¬ë¥¼ ì„ì‹œë¡œ ì–µì œí•˜ì—¬ ìƒ‰ìƒ ê´€ë ¨ ê²½ê³  ìˆ¨ê¸°ê¸°
            import sys
            from io import StringIO
            
            old_stderr = sys.stderr
            sys.stderr = StringIO()
            
            try:
                text = extract_text(file_path)
            finally:
                # í‘œì¤€ ì—ëŸ¬ ë³µì›
                sys.stderr = old_stderr
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            text = re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
            text = re.sub(r'\n+', '\n', text)  # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ
            return text.strip()
            
        except Exception as e:
            logger.error(f"Failed to extract text from {file_path}: {str(e)}")
            # ë¹ˆ í…ìŠ¤íŠ¸ ëŒ€ì‹  ê¸°ë³¸ ESG í…ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ íŒŒì¸íŠœë‹ì´ ê³„ì† ì§„í–‰ë˜ë„ë¡ í•¨
            return "ESG ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½, ì‚¬íšŒ, ì§€ë°°êµ¬ì¡° ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤."
    
    async def _create_classification_data(self, text: str) -> List[Dict[str, Any]]:
        """ë¶„ë¥˜ ì‘ì—…ìš© ë°ì´í„° ìƒì„±"""
        data = []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self._split_into_sentences(text)
        
        for sentence in sentences:
            if len(sentence.strip()) < 10:  # ìµœì†Œ ê¸¸ì´ ì¤„ì„
                continue
            
            # GRI PDFëŠ” ëª¨ë“  ë‚´ìš©ì´ ESG ê´€ë ¨ì´ë¯€ë¡œ ëª¨ë‘ 1ë¡œ ì„¤ì •
            data.append({
                "text": sentence.strip(),
                "labels": 1  # ëª¨ë“  GRI ë‚´ìš©ì€ ESG ê´€ë ¨
            })
        
        return data[:100]  # ìµœëŒ€ 100ê°œ ìƒ˜í”Œë¡œ ì œí•œ
    
    async def _create_qa_data(self, text: str) -> List[Dict[str, Any]]:
        """ì§ˆì˜ì‘ë‹µ ì‘ì—…ìš© ë°ì´í„° ìƒì„±"""
        data = []
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
        paragraphs = self._split_into_paragraphs(text)
        
        for paragraph in paragraphs:
            if len(paragraph.strip()) < 50:  # ìµœì†Œ ê¸¸ì´ ì¤„ì„
                continue
            
            # GRI PDFëŠ” ëª¨ë“  ë‚´ìš©ì´ ESG ê´€ë ¨ì´ë¯€ë¡œ í•„í„° ì œê±°
            # ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±
            qa_pairs = self._generate_qa_pairs(paragraph)
            
            for question, answer, start_pos in qa_pairs:
                data.append({
                    "question": question,
                    "context": paragraph,
                    "answers": {
                        "text": [answer],
                        "answer_start": [start_pos]
                    }
                })
        
        return data[:50]  # ìµœëŒ€ 50ê°œ ìƒ˜í”Œë¡œ ì œí•œ
    
    async def _create_summarization_data(self, text: str) -> List[Dict[str, Any]]:
        """ìš”ì•½ ì‘ì—…ìš© ë°ì´í„° ìƒì„±"""
        data = []
        
        # ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• 
        sections = self._split_into_sections(text)
        
        for section in sections:
            if len(section.strip()) < 100:  # ìµœì†Œ ê¸¸ì´ ì¤„ì„
                continue
            
            # GRI PDFëŠ” ëª¨ë“  ë‚´ìš©ì´ ESG ê´€ë ¨ì´ë¯€ë¡œ í•„í„° ì œê±°
            # ìš”ì•½ ìƒì„± (ì²« ë²ˆì§¸ ë¬¸ì¥ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©)
            sentences = self._split_into_sentences(section)
            if len(sentences) > 1:
                summary = sentences[0]
                content = " ".join(sentences[1:])
                
                data.append({
                    "text": content,
                    "summary": summary
                })
        
        return data[:30]  # ìµœëŒ€ 30ê°œ ìƒ˜í”Œë¡œ ì œí•œ
    
    async def _create_generation_data(self, text: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ìš© ë°ì´í„° ìƒì„±"""
        data = []
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
        paragraphs = self._split_into_paragraphs(text)
        
        for paragraph in paragraphs:
            if len(paragraph.strip()) < 50:  # ìµœì†Œ ê¸¸ì´ ì¤„ì„
                continue
            
            # GRI PDFëŠ” ëª¨ë“  ë‚´ìš©ì´ ESG ê´€ë ¨ì´ë¯€ë¡œ í•„í„° ì œê±°
            # í”„ë¡¬í”„íŠ¸-ì‘ë‹µ ìŒ ìƒì„±
            prompts = self._generate_prompts(paragraph)
            
            for prompt in prompts:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
                combined_text = f"{prompt} {paragraph}"
                if len(combined_text) > 1000:
                    combined_text = combined_text[:1000] + "..."
                
                data.append({
                    "text": combined_text
                })
        
        return data
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        # í•œêµ­ì–´ ë¬¸ì¥ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _split_into_paragraphs(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• """
        paragraphs = text.split('\n\n')
        return [p.strip() for p in paragraphs if p.strip()]
    
    def _split_into_sections(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• """
        # ì œëª© íŒ¨í„´ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
        sections = re.split(r'\n(?=[0-9]+\.|[ê°€-í£]+\s*[0-9]*\.)', text)
        return [s.strip() for s in sections if s.strip()]
    
    def _is_esg_related(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ê°€ ESG ê´€ë ¨ì¸ì§€ íŒë‹¨ (GRI ê¸°ë°˜ í–¥ìƒ)"""
        text_lower = text.lower()
        
        # í™œì„± í‚¤ì›Œë“œ ì‚¬ìš© (GRI í™•ì¥ ë˜ëŠ” ê¸°ë³¸)
        active_keywords = self.get_active_keywords()
        
        # ESG í‚¤ì›Œë“œ ê²€ì‚¬
        for category, keywords in active_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    # GRI í‘œì¤€ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                    gri_standard = self.get_gri_standard_for_keyword(keyword)
                    if gri_standard:
                        logger.debug(f"í‚¤ì›Œë“œ '{keyword}' -> GRI: {gri_standard}")
                    return True
        
        return False
    
    def _generate_qa_pairs(self, paragraph: str) -> List[tuple]:
        """ë¬¸ë‹¨ì—ì„œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±"""
        qa_pairs = []
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ QA ìƒì„±
        sentences = self._split_into_sentences(paragraph)
        
        for i, sentence in enumerate(sentences):
            # ìˆ˜ì¹˜ê°€ í¬í•¨ëœ ë¬¸ì¥ì—ì„œ ì§ˆë¬¸ ìƒì„±
            numbers = re.findall(r'\d+(?:,\d+)*(?:\.\d+)?', sentence)
            if numbers:
                for num in numbers:
                    question = f"í•´ë‹¹ ìˆ˜ì¹˜ëŠ” ì–¼ë§ˆì…ë‹ˆê¹Œ?"
                    answer = num
                    start_pos = sentence.find(num)
                    if start_pos != -1:
                        qa_pairs.append((question, answer, start_pos))
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
            for category, keywords in self.esg_keywords.items():
                for keyword in keywords:
                    if keyword in sentence:
                        question = f"{category}ì— ëŒ€í•œ ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?"
                        answer = sentence
                        qa_pairs.append((question, answer, 0))
                        break
        
        return qa_pairs[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€
    
    def _generate_prompts(self, paragraph: str) -> List[str]:
        """ë¬¸ë‹¨ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompts = [
            "ë‹¤ìŒ ESG ë‚´ìš©ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”:",
            "ì´ ESG ë³´ê³ ì„œ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´:",
            "ESG ê´€ì ì—ì„œ ë‹¤ìŒ ë‚´ìš©ì˜ ì˜ë¯¸ëŠ”:",
            "ì§€ì†ê°€ëŠ¥ê²½ì˜ ì¸¡ë©´ì—ì„œ ë‹¤ìŒ ë‚´ìš©ì€:"
        ]
        
        return prompts[:2]  # ìµœëŒ€ 2ê°œê¹Œì§€
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def create_custom_dataset(
        self, 
        texts: List[str], 
        labels: Optional[List[int]] = None,
        task_type: TaskType = TaskType.CLASSIFICATION
    ) -> pd.DataFrame:
        """ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ìƒì„±"""
        if task_type == TaskType.CLASSIFICATION and labels is None:
            raise ValueError("ë¶„ë¥˜ ì‘ì—…ì—ëŠ” ë ˆì´ë¸”ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        data = []
        for i, text in enumerate(texts):
            processed_text = self.preprocess_text(text)
            
            if task_type == TaskType.CLASSIFICATION:
                data.append({
                    "text": processed_text,
                    "labels": labels[i]  # labels ì»¬ëŸ¼ë§Œ ì‚¬ìš©
                })
            else:
                data.append({
                    "text": processed_text
                })
        
        return pd.DataFrame(data) 