import os
import re
import json
import logging
import glob
from typing import Dict, List, Tuple, Optional, Any
from pdfminer.high_level import extract_text
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd
import PyPDF2
from pathlib import Path

logger = logging.getLogger(__name__)

class GRIDataService:
    """GRI Standard PDFë¥¼ í™œìš©í•œ í¬ê´„ì  ESG í‚¤ì›Œë“œ ë§¤í•‘ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')
        self.gri_standards = {}
        self.gri_keywords = {}
        self.gri_embeddings = {}
        self.gri_folder = "data/gri"
        self.training_folder = "data/training"
        
        # í›ˆë ¨ í´ë” ìƒì„±
        os.makedirs(self.training_folder, exist_ok=True)
        
    async def create_gri_standards_learning_dataset(self) -> Dict[str, Any]:
        """GRI ê¸°ì¤€ PDF íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ì‹¤ì œ GRI ê¸°ì¤€ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ ìƒì„±"""
        try:
            logger.info("ğŸ” GRI ê¸°ì¤€ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
            
            # GRI PDF íŒŒì¼ë“¤ ì°¾ê¸°
            gri_files = self._find_gri_files()
            if not gri_files:
                raise ValueError("GRI PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"ğŸ“ ë°œê²¬ëœ GRI íŒŒì¼ ìˆ˜: {len(gri_files)}")
            
            # ê° PDF íŒŒì¼ì—ì„œ ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ
            training_data = []
            processed_count = 0
            
            for file_path in gri_files[:5]:  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 5ê°œë§Œ
                try:
                    logger.info(f"ğŸ“– ì²˜ë¦¬ ì¤‘: {os.path.basename(file_path)}")
                    
                    # PDF ë‚´ìš© ì¶”ì¶œ
                    pdf_content = self._extract_pdf_content(file_path)
                    if not pdf_content or len(pdf_content.strip()) < 100:
                        logger.warning(f"âš ï¸ PDF ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤: {file_path}")
                        continue
                    
                    # GRI ì •ë³´ íŒŒì‹±
                    gri_info = self._parse_gri_info_from_filename(file_path)
                    
                    # ì‹¤ì œ GRI ê¸°ì¤€ í•™ìŠµì„ ìœ„í•œ ë‹¤ì–‘í•œ í›ˆë ¨ ë°ì´í„° ìƒì„±
                    file_training_data = self._create_gri_learning_samples(
                        pdf_content, gri_info, file_path
                    )
                    
                    training_data.extend(file_training_data)
                    processed_count += 1
                    
                    logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(file_path)} ({len(file_training_data)}ê°œ ìƒ˜í”Œ ìƒì„±)")
                    
                except Exception as e:
                    logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {str(e)}")
                    continue
            
            if not training_data:
                raise ValueError("ìœ íš¨í•œ GRI í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
            df = pd.DataFrame(training_data)
            output_path = os.path.join(self.training_folder, "gri_standards_learning_dataset.csv")
            df.to_csv(output_path, index=False, encoding='utf-8')
            
            result = {
                "success": True,
                "dataset_path": output_path,
                "total_samples": len(training_data),
                "processed_files": processed_count,
                "total_files": len(gri_files),
                "sample_data": training_data[:3] if training_data else []
            }
            
            logger.info(f"ğŸ‰ GRI ê¸°ì¤€ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ì´ {len(training_data)}ê°œ í•™ìŠµ ìƒ˜í”Œ ìƒì„±")
            logger.info(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {output_path}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ GRI ê¸°ì¤€ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "dataset_path": None,
                "total_samples": 0
            }
    
    def _extract_pdf_content(self, file_path: str) -> str:
        """PDF íŒŒì¼ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            content = ""
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # ì²˜ìŒ 10í˜ì´ì§€ë§Œ ì¶”ì¶œ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
                max_pages = min(10, len(pdf_reader.pages))
                
                for page_num in range(max_pages):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    if page_text:
                        content += page_text + "\n"
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = self._clean_pdf_text(content)
                
            return content
            
        except Exception as e:
            logger.error(f"PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {str(e)}")
            return ""
    
    def _clean_pdf_text(self, text: str) -> str:
        """PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ê¸°ë³¸ ì •ë¦¬
        text = text.replace('\n\n', '\n')
        text = text.replace('\t', ' ')
        text = ' '.join(text.split())  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        
        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë¥´ê¸° (í† í° ì œí•œ ê³ ë ¤)
        if len(text) > 3000:
            text = text[:3000] + "..."
        
        return text.strip()
    
    def _create_gri_learning_samples(self, pdf_content: str, gri_info: Dict, file_path: str) -> List[Dict]:
        """ì‹¤ì œ GRI ê¸°ì¤€ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ í›ˆë ¨ ìƒ˜í”Œ ìƒì„±"""
        samples = []
        
        gri_number = gri_info.get('number', 'Unknown')
        gri_title = gri_info.get('title', 'Unknown')
        category = gri_info.get('category', 'Unknown')
        
        # 1. GRI ê¸°ì¤€ ì„¤ëª… í•™ìŠµ
        samples.append({
            "input_text": f"GRI {gri_number} {gri_title}ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "target_text": f"GRI {gri_number} {gri_title}ëŠ” {category} ë¶„ì•¼ì˜ ì§€ì†ê°€ëŠ¥ì„± ë³´ê³  í‘œì¤€ì…ë‹ˆë‹¤.\n\n{pdf_content[:1000]}",
            "task_type": "gri_explanation",
            "gri_standard": gri_number,
            "gri_title": gri_title,
            "category": category,
            "source_file": os.path.basename(file_path)
        })
        
        # 2. GRI ê¸°ì¤€ ìš”êµ¬ì‚¬í•­ í•™ìŠµ
        samples.append({
            "input_text": f"GRI {gri_number}ì˜ ì£¼ìš” ìš”êµ¬ì‚¬í•­ê³¼ ê³µì‹œ í•­ëª©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "target_text": f"GRI {gri_number} {gri_title}ì˜ ì£¼ìš” ìš”êµ¬ì‚¬í•­:\n\n{pdf_content[500:1500]}",
            "task_type": "gri_requirements",
            "gri_standard": gri_number,
            "gri_title": gri_title,
            "category": category,
            "source_file": os.path.basename(file_path)
        })
        
        # 3. GRI ê¸°ì¤€ ì ìš© ê°€ì´ë“œ í•™ìŠµ
        samples.append({
            "input_text": f"ê¸°ì—…ì´ GRI {gri_number}ë¥¼ ì ìš©í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "target_text": f"GRI {gri_number} {gri_title} ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­:\n\n{pdf_content[1000:2000]}",
            "task_type": "gri_guidance",
            "gri_standard": gri_number,
            "gri_title": gri_title,
            "category": category,
            "source_file": os.path.basename(file_path)
        })
        
        # 4. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•™ìŠµ
        samples.append({
            "input_text": f"GRI {gri_number} {gri_title}ëŠ” ESGì˜ ì–´ë–¤ ì˜ì—­ì— í•´ë‹¹í•˜ë‚˜ìš”?",
            "target_text": f"GRI {gri_number} {gri_title}ëŠ” {category} ì˜ì—­ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì´ëŠ” ì§€ì†ê°€ëŠ¥ì„± ë³´ê³ ì—ì„œ {category.lower()} ì„±ê³¼ë¥¼ ì¸¡ì •í•˜ê³  ë³´ê³ í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.",
            "task_type": "category_classification",
            "gri_standard": gri_number,
            "gri_title": gri_title,
            "category": category,
            "source_file": os.path.basename(file_path)
        })
        
        # 5. ì‹¤ì œ ë‚´ìš© ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ í•™ìŠµ
        if len(pdf_content) > 500:
            content_snippet = pdf_content[200:800]
            samples.append({
                "input_text": f"ë‹¤ìŒ GRI {gri_number} ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”:\n\n{content_snippet[:300]}",
                "target_text": f"í•´ë‹¹ ë‚´ìš©ì˜ í•µì‹¬ í¬ì¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{content_snippet[300:]}",
                "task_type": "content_analysis",
                "gri_standard": gri_number,
                "gri_title": gri_title,
                "category": category,
                "source_file": os.path.basename(file_path)
            })
        
        return samples
    
    def _extract_gri_info_from_filename(self, filename: str) -> Dict[str, str]:
        """íŒŒì¼ëª…ì—ì„œ GRI ì •ë³´ ì¶”ì¶œ"""
        gri_info = {
            "number": "Unknown",
            "title": "Unknown", 
            "year": "Unknown",
            "category": "Unknown"
        }
        
        try:
            if "GRI " in filename:
                parts = filename.split("GRI ")[1]
                
                if "_" in parts:
                    number_part = parts.split("_")[0].strip()
                    title_part = parts.split("_")[1].split(".pdf")[0].strip()
                    
                    # ì—°ë„ ì¶”ì¶œ
                    year_match = re.search(r'\d{4}', title_part)
                    if year_match:
                        gri_info["year"] = year_match.group()
                        title_part = re.sub(r'\d{4}', '', title_part).strip()
                    
                    gri_info["number"] = number_part
                    gri_info["title"] = title_part
                    gri_info["category"] = self._categorize_gri_standard_by_number(number_part)
                    
        except Exception as e:
            logger.warning(f"íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨: {filename} - {str(e)}")
        
        return gri_info
    
    def _categorize_gri_standard_by_number(self, gri_number: str) -> str:
        """GRI í‘œì¤€ ë²ˆí˜¸ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        try:
            num_match = re.search(r'\d+', gri_number)
            if num_match:
                num = int(num_match.group())
                
                if num <= 100:
                    return "Foundation & General"
                elif num <= 200:
                    return "Economic"
                elif num <= 300:
                    return "Environmental"
                else:
                    return "Social"
        except:
            pass
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        gri_lower = gri_number.lower()
        if any(word in gri_lower for word in ['foundation', 'general', 'material']):
            return "Foundation & General"
        elif any(word in gri_lower for word in ['economic', 'performance', 'market', 'procurement']):
            return "Economic"
        elif any(word in gri_lower for word in ['environmental', 'emission', 'energy', 'water', 'waste', 'biodiversity']):
            return "Environmental"
        elif any(word in gri_lower for word in ['social', 'employment', 'training', 'diversity', 'health', 'safety']):
            return "Social"
        
        return "Unknown"
    
    def _split_gri_content_into_sections(self, content) -> Dict[str, str]:
        """GRI ê¸°ì¤€ì„œ ë‚´ìš©ì„ ì˜ë¯¸ìˆëŠ” ì„¹ì…˜ìœ¼ë¡œ ë¶„í• """
        if isinstance(content, dict):
            return content
        
        text = str(content)
        sections = {}
        
        # GRI ë¬¸ì„œì˜ ì¼ë°˜ì ì¸ ì„¹ì…˜ êµ¬ì¡°
        section_patterns = [
            r'(?i)(disclosure|requirement|guidance|management approach|reporting guidance)',
            r'(?i)(background|rationale|compilation requirements)',
            r'(?i)(definitions|glossary|references)'
        ]
        
        # íŒ¨í„´ ê¸°ë°˜ ì„¹ì…˜ ë¶„í• 
        current_section = "Main Content"
        current_content = []
        
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ì„¹ì…˜ í—¤ë” ê°ì§€
            section_found = False
            for pattern in section_patterns:
                if re.search(pattern, line):
                    # ì´ì „ ì„¹ì…˜ ì €ì¥
                    if current_content:
                        sections[current_section] = '\n'.join(current_content)
                    
                    # ìƒˆ ì„¹ì…˜ ì‹œì‘
                    current_section = line[:50] + "..." if len(line) > 50 else line
                    current_content = []
                    section_found = True
                    break
            
            if not section_found:
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_content:
            sections[current_section] = '\n'.join(current_content)
        
        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ
        if not sections:
            sections["Main Content"] = text
        
        return sections
    
    def _generate_training_data_from_sections(self, sections: Dict[str, str], gri_number: str, 
                                            gri_title: str, category: str, file_name: str) -> List[Dict]:
        """ì„¹ì…˜ë³„ë¡œ ë‹¤ì–‘í•œ í˜•íƒœì˜ í•™ìŠµ ë°ì´í„° ìƒì„±"""
        training_data = []
        
        for section_title, section_content in sections.items():
            if len(section_content) < 100:  # ë„ˆë¬´ ì§§ì€ ì„¹ì…˜ ì œì™¸
                continue
            
            # 1. GRI í‘œì¤€ ì„¤ëª… í•™ìŠµ
            training_data.append({
                "input_text": f"GRI {gri_number}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "target_text": f"GRI {gri_number} ({gri_title})ëŠ” {category} ì¹´í…Œê³ ë¦¬ì˜ í‘œì¤€ì…ë‹ˆë‹¤. {section_content[:300]}...",
                "gri_standard": gri_number,
                "gri_title": gri_title,
                "category": category,
                "section": section_title,
                "task_type": "standard_explanation",
                "source_file": file_name
            })
            
            # 2. GRI ìš”êµ¬ì‚¬í•­ í•™ìŠµ
            if "requirement" in section_title.lower() or "disclosure" in section_title.lower():
                training_data.append({
                    "input_text": f"GRI {gri_number}ì˜ ê³µì‹œ ìš”êµ¬ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "target_text": f"GRI {gri_number}ì˜ ê³µì‹œ ìš”êµ¬ì‚¬í•­: {section_content[:400]}...",
                    "gri_standard": gri_number,
                    "gri_title": gri_title,
                    "category": category,
                    "section": section_title,
                    "task_type": "requirement_explanation",
                    "source_file": file_name
                })
            
            # 3. GRI ê°€ì´ë˜ìŠ¤ í•™ìŠµ
            if "guidance" in section_title.lower() or "recommendation" in section_title.lower():
                training_data.append({
                    "input_text": f"GRI {gri_number} ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "target_text": f"GRI {gri_number} ì ìš© ê°€ì´ë˜ìŠ¤: {section_content[:400]}...",
                    "gri_standard": gri_number,
                    "gri_title": gri_title,
                    "category": category,
                    "section": section_title,
                    "task_type": "guidance",
                    "source_file": file_name
                })
            
            # 4. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ í•™ìŠµ
            training_data.append({
                "input_text": f"ë‹¤ìŒ ë‚´ìš©ì´ ì–´ë–¤ GRI ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ ë¶„ë¥˜í•´ì£¼ì„¸ìš”: {section_content[:200]}...",
                "target_text": f"ì´ ë‚´ìš©ì€ {category} ì¹´í…Œê³ ë¦¬ì˜ GRI {gri_number} ({gri_title}) í‘œì¤€ì— ì†í•©ë‹ˆë‹¤.",
                "gri_standard": gri_number,
                "gri_title": gri_title,
                "category": category,
                "section": section_title,
                "task_type": "category_classification",
                "source_file": file_name
            })
            
            # 5. í‚¤ì›Œë“œ ê¸°ë°˜ GRI ë§¤ì¹­ í•™ìŠµ
            keywords = self._extract_key_terms_from_content(section_content)
            if keywords:
                keyword_text = ", ".join(keywords[:5])
                training_data.append({
                    "input_text": f"'{keyword_text}' í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ GRI í‘œì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "target_text": f"'{keyword_text}' í‚¤ì›Œë“œëŠ” GRI {gri_number} ({gri_title}) í‘œì¤€ê³¼ ê´€ë ¨ë©ë‹ˆë‹¤. {section_content[:200]}...",
                    "gri_standard": gri_number,
                    "gri_title": gri_title,
                    "category": category,
                    "section": section_title,
                    "task_type": "keyword_matching",
                    "keywords": keyword_text,
                    "source_file": file_name
                })
        
        return training_data
    
    def _extract_key_terms_from_content(self, content: str) -> List[str]:
        """ë‚´ìš©ì—ì„œ í•µì‹¬ ìš©ì–´ ì¶”ì¶œ"""
        gri_terms = [
            # Environmental
            'emission', 'emissions', 'greenhouse gas', 'carbon', 'energy', 'renewable',
            'water', 'waste', 'biodiversity', 'materials', 'pollution', 'climate',
            
            # Social  
            'employment', 'training', 'education', 'diversity', 'health', 'safety',
            'community', 'human rights', 'labor', 'discrimination', 'child labor',
            
            # Economic
            'economic performance', 'market presence', 'procurement', 'anti-corruption',
            'tax', 'indirect economic', 'supply chain',
            
            # Governance
            'governance', 'ethics', 'compliance', 'risk management', 'transparency',
            'stakeholder', 'materiality', 'disclosure'
        ]
        
        content_lower = content.lower()
        found_terms = []
        
        for term in gri_terms:
            if term in content_lower:
                count = content_lower.count(term)
                if count >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥
                    found_terms.append((term, count))
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
        found_terms.sort(key=lambda x: x[1], reverse=True)
        
        return [term[0] for term in found_terms[:10]]
    
    def _generate_dataset_statistics(self, df: pd.DataFrame, processed_count: int, total_files: int) -> Dict:
        """ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ìƒì„±"""
        category_stats = df['category'].value_counts().to_dict()
        gri_stats = df['gri_standard'].value_counts().to_dict()
        task_stats = df['task_type'].value_counts().to_dict()
        
        return {
            "unique_gri_standards": df['gri_standard'].nunique(),
            "processed_files": processed_count,
            "available_gri_pdfs": total_files,
            "category_distribution": category_stats,
            "gri_distribution": dict(list(gri_stats.items())[:10]),  # ìƒìœ„ 10ê°œë§Œ
            "task_type_distribution": task_stats
        }
    
    async def list_gri_models(self, models_dir: str = "models") -> Dict:
        """GRI í‘œì¤€ í•™ìŠµ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            if not os.path.exists(models_dir):
                return {
                    "success": True,
                    "message": "ì•„ì§ í›ˆë ¨ëœ GRI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤",
                    "models": []
                }
            
            # GRI ê´€ë ¨ ëª¨ë¸ ì°¾ê¸°
            model_folders = [d for d in os.listdir(models_dir) 
                            if os.path.isdir(os.path.join(models_dir, d)) and 'gri' in d.lower()]
            
            models_info = []
            for model_folder in model_folders:
                model_path = os.path.join(models_dir, model_folder)
                
                model_info = {
                    "model_name": model_folder,
                    "model_path": model_path,
                    "created_date": "Unknown",
                    "model_size": "Unknown",
                    "tuning_type": "GRI Standards Learning"
                }
                
                # ëª¨ë¸ í¬ê¸° ê³„ì‚°
                try:
                    total_size = 0
                    for root, dirs, files in os.walk(model_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            total_size += os.path.getsize(file_path)
                    model_info["model_size"] = f"{total_size / (1024*1024):.1f} MB"
                except:
                    pass
                
                models_info.append(model_info)
            
            return {
                "success": True,
                "message": f"ì´ {len(models_info)}ê°œì˜ GRI í•™ìŠµ ëª¨ë¸ ë°œê²¬",
                "models": models_info,
                "usage_note": "ì´ ëª¨ë¸ë“¤ì€ GRI í‘œì¤€ì„ ì´í•´í•˜ê³  ê¸°ì—… ë³´ê³ ì„œë¥¼ GRI ê¸°ì¤€ì— ë”°ë¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            }
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            raise

    async def process_gri_pdf(self, gri_pdf_path: str) -> Dict:
        """GRI Standard PDFë¥¼ íŒŒì‹±í•˜ì—¬ í‘œì¤€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            logger.info(f"ğŸ“„ GRI Standard PDF ì²˜ë¦¬ ì‹œì‘: {gri_pdf_path}")
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = extract_text(gri_pdf_path)
            
            # GRI í‘œì¤€ë³„ ì„¹ì…˜ ë¶„í• 
            gri_sections = self._extract_gri_sections(text)
            
            # ê° GRI í‘œì¤€ë³„ í‚¤ì›Œë“œ ë° ì„¤ëª… ì¶”ì¶œ
            for gri_code, content in gri_sections.items():
                keywords = self._extract_keywords_from_gri_section(content)
                description = self._extract_description_from_gri_section(content)
                
                self.gri_standards[gri_code] = {
                    "keywords": keywords,
                    "description": description,
                    "content": content[:1000]  # ì²˜ìŒ 1000ìë§Œ ì €ì¥
                }
                
                # ì„ë² ë”© ìƒì„±
                self.gri_embeddings[gri_code] = self.sbert_model.encode(
                    f"{description} {' '.join(keywords)}"
                )
            
            logger.info(f"âœ… GRI Standard ì²˜ë¦¬ ì™„ë£Œ: {len(self.gri_standards)}ê°œ í‘œì¤€ ì¶”ì¶œ")
            return self.gri_standards
            
        except Exception as e:
            logger.error(f"âŒ GRI PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _extract_gri_sections(self, text: str) -> Dict[str, str]:
        """GRI í‘œì¤€ë³„ ì„¹ì…˜ ì¶”ì¶œ"""
        sections = {}
        
        # GRI í‘œì¤€ íŒ¨í„´ (ì˜ˆ: GRI 302: Energy, GRI 305: Emissions ë“±)
        gri_pattern = r'GRI\s+(\d+):\s*([^.\n]+)'
        matches = re.finditer(gri_pattern, text, re.IGNORECASE)
        
        for match in matches:
            gri_number = match.group(1)
            gri_title = match.group(2).strip()
            gri_code = f"GRI {gri_number}: {gri_title}"
            
            # í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš© ì¶”ì¶œ (ë‹¤ìŒ GRIê¹Œì§€)
            start_pos = match.start()
            next_match = re.search(r'GRI\s+\d+:', text[start_pos + 100:], re.IGNORECASE)
            
            if next_match:
                end_pos = start_pos + 100 + next_match.start()
                section_content = text[start_pos:end_pos]
            else:
                section_content = text[start_pos:start_pos + 2000]  # ìµœëŒ€ 2000ì
            
            sections[gri_code] = section_content
        
        return sections
    
    def _extract_keywords_from_gri_section(self, content: str) -> List[str]:
        """GRI ì„¹ì…˜ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # TF-IDFë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        vectorizer = TfidfVectorizer(
            max_features=20,
            stop_words=None,
            ngram_range=(1, 2)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform([content])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
            keyword_scores = list(zip(feature_names, tfidf_scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            
            keywords = [kw for kw, score in keyword_scores[:10] if score > 0.1]
            return keywords
            
        except Exception as e:
            logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _extract_description_from_gri_section(self, content: str) -> str:
        """GRI ì„¹ì…˜ì—ì„œ ì„¤ëª… ì¶”ì¶œ"""
        # ì²« ë²ˆì§¸ ë¬¸ë‹¨ì„ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
        paragraphs = content.split('\n\n')
        for para in paragraphs:
            if len(para.strip()) > 50:
                return para.strip()[:500]  # ìµœëŒ€ 500ì
        
        return content[:500]
    
    async def create_enhanced_keyword_mapping(self) -> Dict:
        """í–¥ìƒëœ í‚¤ì›Œë“œ ë§¤í•‘ ìƒì„±"""
        enhanced_mapping = {}
        
        # ê¸°ì¡´ í•˜ë“œì½”ë”© ë§¤í•‘ í™•ì¥
        base_keywords = {
            "í™˜ê²½": ["í™˜ê²½", "ê¸°í›„", "íƒ„ì†Œ", "ì˜¨ì‹¤ê°€ìŠ¤", "ì¬ìƒì—ë„ˆì§€", "ì¹œí™˜ê²½", "ì§€ì†ê°€ëŠ¥", "ë…¹ìƒ‰"],
            "ì‚¬íšŒ": ["ì‚¬íšŒ", "ì„ì§ì›", "ì¸ê¶Œ", "ë‹¤ì–‘ì„±", "ì•ˆì „", "ë³´ê±´", "ì§€ì—­ì‚¬íšŒ", "ì‚¬íšŒê³µí—Œ"],
            "ì§€ë°°êµ¬ì¡°": ["ì§€ë°°êµ¬ì¡°", "ì´ì‚¬íšŒ", "íˆ¬ëª…ì„±", "ìœ¤ë¦¬", "ì»´í”Œë¼ì´ì–¸ìŠ¤", "ë¦¬ìŠ¤í¬", "ê°ì‚¬"]
        }
        
        # GRI í‘œì¤€ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
        for gri_code, gri_data in self.gri_standards.items():
            category = self._categorize_gri_standard(gri_code)
            
            if category in base_keywords:
                # ê¸°ì¡´ í‚¤ì›Œë“œì— GRI í‚¤ì›Œë“œ ì¶”ê°€
                base_keywords[category].extend(gri_data["keywords"])
                # ì¤‘ë³µ ì œê±°
                base_keywords[category] = list(set(base_keywords[category]))
        
        # í‚¤ì›Œë“œë³„ GRI ë§¤í•‘ ìƒì„±
        for category, keywords in base_keywords.items():
            for keyword in keywords:
                best_gri = self._find_best_gri_match(keyword)
                if best_gri:
                    enhanced_mapping[keyword] = best_gri
        
        return {
            "enhanced_keywords": base_keywords,
            "keyword_to_gri_mapping": enhanced_mapping,
            "gri_standards": self.gri_standards
        }
    
    def _categorize_gri_standard(self, gri_code: str) -> str:
        """GRI í‘œì¤€ì„ ESG ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        environmental_codes = ["302", "303", "304", "305", "306", "307", "308"]
        social_codes = ["401", "402", "403", "404", "405", "406", "407", "408", "409", "410", "411", "412", "413", "414", "415", "416", "417", "418", "419"]
        governance_codes = ["201", "202", "203", "204", "205", "206", "207"]
        
        gri_number = re.search(r'GRI\s+(\d+)', gri_code)
        if gri_number:
            number = gri_number.group(1)
            if number in environmental_codes:
                return "í™˜ê²½"
            elif number in social_codes:
                return "ì‚¬íšŒ"
            elif number in governance_codes:
                return "ì§€ë°°êµ¬ì¡°"
        
        return "ê¸°íƒ€"
    
    def _find_best_gri_match(self, keyword: str) -> Optional[str]:
        """í‚¤ì›Œë“œì— ê°€ì¥ ì í•©í•œ GRI í‘œì¤€ ì°¾ê¸°"""
        if not self.gri_embeddings:
            return None
        
        keyword_embedding = self.sbert_model.encode(keyword)
        best_match = None
        best_score = 0.0
        
        for gri_code, gri_embedding in self.gri_embeddings.items():
            similarity = cosine_similarity(
                np.array([keyword_embedding]), 
                np.array([gri_embedding])
            )[0][0]
            if similarity > best_score and similarity > 0.3:  # ì„ê³„ê°’ 0.3
                best_score = similarity
                best_match = gri_code
        
        return best_match
    
    async def generate_training_data_from_gri(self, output_path: str) -> str:
        """GRI Standard ê¸°ë°˜ í›ˆë ¨ ë°ì´í„° ìƒì„±"""
        training_data = []
        
        for gri_code, gri_data in self.gri_standards.items():
            # ë¶„ë¥˜ ë°ì´í„° ìƒì„±
            for keyword in gri_data["keywords"]:
                training_data.append({
                    "text": f"{keyword}ì— ëŒ€í•œ ESG ë³´ê³ ì„œ ë‚´ìš©ì…ë‹ˆë‹¤: {gri_data['description'][:200]}",
                    "labels": 1,  # ESG ê´€ë ¨
                    "gri_standard": gri_code,
                    "category": self._categorize_gri_standard(gri_code)
                })
            
            # QA ë°ì´í„° ìƒì„±
            training_data.append({
                "question": f"{gri_code}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "context": gri_data["description"],
                "answers": {
                    "text": [gri_data["description"][:100]],
                    "answer_start": [0]
                },
                "gri_standard": gri_code
            })
        
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ ë° ì €ì¥
        df = pd.DataFrame(training_data)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"âœ… GRI ê¸°ë°˜ í›ˆë ¨ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(training_data)}ê°œ ìƒ˜í”Œ")
        return output_path
    
    async def save_enhanced_mapping(self, output_path: str) -> str:
        """í–¥ìƒëœ í‚¤ì›Œë“œ ë§¤í•‘ ì €ì¥"""
        enhanced_mapping = await self.create_enhanced_keyword_mapping()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(enhanced_mapping, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… í–¥ìƒëœ í‚¤ì›Œë“œ ë§¤í•‘ ì €ì¥ ì™„ë£Œ: {output_path}")
        return output_path
    
    def _generate_simple_training_data(self, gri_number: str, gri_title: str, 
                                     category: str, file_name: str, text_sample: str) -> List[Dict]:
        """ê°„ë‹¨í•œ í•™ìŠµ ë°ì´í„° ìƒì„± (PDF íŒŒì‹± ë¬¸ì œ í•´ê²°ìš©)"""
        training_data = []
        
        # 1. GRI í‘œì¤€ ì„¤ëª… í•™ìŠµ
        training_data.append({
            "input_text": f"GRI {gri_number}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "target_text": f"GRI {gri_number} ({gri_title})ëŠ” {category} ì¹´í…Œê³ ë¦¬ì˜ í‘œì¤€ì…ë‹ˆë‹¤. ì´ í‘œì¤€ì€ ì§€ì†ê°€ëŠ¥ì„± ë³´ê³ ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.",
            "gri_standard": gri_number,
            "gri_title": gri_title,
            "category": category,
            "task_type": "standard_explanation",
            "source_file": file_name
        })
        
        # 2. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•™ìŠµ
        training_data.append({
            "input_text": f"GRI {gri_number} {gri_title}ëŠ” ì–´ë–¤ ESG ì¹´í…Œê³ ë¦¬ì— ì†í•˜ë‚˜ìš”?",
            "target_text": f"GRI {gri_number} {gri_title}ëŠ” {category} ì¹´í…Œê³ ë¦¬ì— ì†í•©ë‹ˆë‹¤.",
            "gri_standard": gri_number,
            "gri_title": gri_title,
            "category": category,
            "task_type": "category_classification",
            "source_file": file_name
        })
        
        # 3. í‚¤ì›Œë“œ ë§¤ì¹­ í•™ìŠµ
        keywords = self._get_category_keywords(category)
        if keywords:
            keyword_text = ", ".join(keywords[:3])
            training_data.append({
                "input_text": f"'{keyword_text}' í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ GRI í‘œì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "target_text": f"'{keyword_text}' í‚¤ì›Œë“œëŠ” GRI {gri_number} ({gri_title}) í‘œì¤€ê³¼ ê´€ë ¨ë©ë‹ˆë‹¤.",
                "gri_standard": gri_number,
                "gri_title": gri_title,
                "category": category,
                "task_type": "keyword_matching",
                "keywords": keyword_text,
                "source_file": file_name
            })
        
        return training_data
    
    def _get_category_keywords(self, category: str) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ë°˜í™˜"""
        category_keywords = {
            "Environmental": ["emission", "energy", "water", "waste", "biodiversity"],
            "Social": ["employment", "training", "diversity", "health", "safety"],
            "Economic": ["economic performance", "market presence", "procurement"],
            "Foundation & General": ["materiality", "stakeholder", "disclosure"]
        }
        return category_keywords.get(category, [])

    def _find_gri_files(self):
        """GRI PDF íŒŒì¼ë“¤ì„ ì°¾ëŠ” ë©”ì„œë“œ"""
        gri_files = []
        for root, _, files in os.walk(self.gri_folder):
            for file in files:
                if file.endswith(('.pdf', '.PDF')):
                    gri_files.append(os.path.join(root, file))
        return gri_files

    def _parse_gri_info_from_filename(self, file_path: str) -> Dict:
        """íŒŒì¼ëª…ì—ì„œ GRI ì •ë³´ ì¶”ì¶œ"""
        gri_info = {
            "number": "Unknown",
            "title": "Unknown", 
            "year": "Unknown",
            "category": "Unknown"
        }
        
        try:
            if "GRI " in file_path:
                parts = file_path.split("GRI ")[1]
                
                if "_" in parts:
                    number_part = parts.split("_")[0].strip()
                    title_part = parts.split("_")[1].split(".pdf")[0].strip()
                    
                    # ì—°ë„ ì¶”ì¶œ
                    year_match = re.search(r'\d{4}', title_part)
                    if year_match:
                        gri_info["year"] = year_match.group()
                        title_part = re.sub(r'\d{4}', '', title_part).strip()
                    
                    gri_info["number"] = number_part
                    gri_info["title"] = title_part
                    gri_info["category"] = self._categorize_gri_standard_by_number(number_part)
                    
        except Exception as e:
            logger.warning(f"íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨: {file_path} - {str(e)}")
        
        return gri_info 