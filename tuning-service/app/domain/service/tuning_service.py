import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import logging



import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    AutoModelForQuestionAnswering, AutoModelForCausalLM,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset, load_dataset
import pandas as pd

from app.domain.model.tuning_request import TuningRequest, TaskType as RequestTaskType
from app.domain.model.tuning_response import (
    TuningStatus, JobStatus, ModelListResponse, ModelInfo, DeleteModelResponse
)
from app.domain.service.data_service import DataService
# RTX 2080 ìµœì í™” ì„¤ì • import
from app.config.gpu_config import (
    setup_rtx2080_environment, 
    get_optimal_batch_size,
    monitor_gpu_memory,
    cleanup_gpu_memory,
    get_rtx2080_training_args,
    get_rtx2080_lora_config
)

logger = logging.getLogger(__name__)

class TuningService:
    def __init__(self):
        self.jobs: Dict[str, Dict[str, Any]] = {}
        # ë¡œì»¬ í™˜ê²½ ëª…í™•í•œ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
        self.models_dir = r"C:\Users\bitcamp\Documents\321\tuning-service\models"
        self.data_service = DataService()
        
        # RTX 2080 í™˜ê²½ ì„¤ì •
        logger.info("ğŸ® RTX 2080 ìµœì í™” í™˜ê²½ ì„¤ì • ì¤‘...")
        gpu_available = setup_rtx2080_environment()
        if gpu_available:
            memory_info = monitor_gpu_memory()
            logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ìƒíƒœ: {memory_info}")
        else:
            logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # í—ˆê¹…í˜ì´ìŠ¤ í† í° í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        self.hf_token = os.getenv("HUGGINGFACE_HUB_TOKEN")
        if self.hf_token and self.hf_token.strip() == "your_token_here":
            self.hf_token = None  # ê¸°ë³¸ê°’ì¸ ê²½ìš° Noneìœ¼ë¡œ ì„¤ì •
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.models_dir, exist_ok=True)
    
    async def run_fine_tuning(self, job_id: str, request: TuningRequest):
        """íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        try:
            # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
            self.jobs[job_id] = {
                "status": JobStatus.RUNNING,
                "progress": 0.0,
                "created_at": datetime.now(),
                "started_at": datetime.now(),
                "request": request
            }
            
            logger.info(f"ğŸš€ íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘: {job_id}")
            logger.info(f"ğŸ“‹ ëª¨ë¸: {request.model_name}, íƒ€ì…: {request.model_type}, ì‘ì—…: {request.task_type}")
            logger.info(f"ğŸ“ ë°ì´í„° í´ë”: {request.reports_folder}")
            
            # 1. ë°ì´í„° ì¤€ë¹„
            await self._update_job_progress(job_id, 5.0, "ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì‹œì‘...")
            logger.info(f"ğŸ” 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            dataset = await self._prepare_dataset(request)
            await self._update_job_progress(job_id, 25.0, "âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
            
            # 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            await self._update_job_progress(job_id, 30.0, "ğŸ¤– ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            logger.info(f"ğŸ” 2ë‹¨ê³„: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            model, tokenizer = await self._load_model_and_tokenizer(request)
            await self._update_job_progress(job_id, 40.0, "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
            # 3. LoRA ì„¤ì • (í•„ìš”ì‹œ)
            if request.use_lora:
                await self._update_job_progress(job_id, 45.0, "âš™ï¸ LoRA ì„¤ì • ì¤‘...")
                logger.info(f"ğŸ” 3ë‹¨ê³„: LoRA ì„¤ì • ì¤‘...")
                # ê³„ì† í•™ìŠµì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ìƒˆë¡œìš´ LoRA ì„¤ì •
                if not request.is_continual_learning:
                    model = self._setup_lora(model, request)
                    logger.info(f"âœ… LoRA ì„¤ì • ì™„ë£Œ (r={request.lora_r}, alpha={request.lora_alpha})")
                else:
                    logger.info("âœ… ê³„ì† í•™ìŠµ: ê¸°ì¡´ LoRA ì–´ëŒ‘í„° ì‚¬ìš©")
            
            # 4. ë°ì´í„° ì „ì²˜ë¦¬
            await self._update_job_progress(job_id, 50.0, "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
            logger.info(f"ğŸ” 4ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
            train_dataset, eval_dataset = await self._preprocess_data(
                dataset, tokenizer, request
            )
            logger.info(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (í›ˆë ¨: {len(train_dataset)}, ê²€ì¦: {len(eval_dataset)})")
            
            # 5. í›ˆë ¨ ì„¤ì •
            await self._update_job_progress(job_id, 60.0, "âš™ï¸ í›ˆë ¨ ì„¤ì • ì¤‘...")
            logger.info(f"ğŸ” 5ë‹¨ê³„: í›ˆë ¨ ì„¤ì • ì¤‘...")
            training_args = self._create_training_args(request, job_id)
            logger.info(f"âœ… í›ˆë ¨ ì„¤ì • ì™„ë£Œ (ì—í¬í¬: {request.num_epochs}, ë°°ì¹˜: {request.batch_size}, LR: {request.learning_rate})")
            
            # 6. íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
            await self._update_job_progress(job_id, 70.0, "ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            logger.info(f"ğŸ” 6ë‹¨ê³„: íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨ ì‹œì‘...")
            logger.info(f"â° ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 1-3ì‹œê°„ (ë°ì´í„° í¬ê¸°ì— ë”°ë¼)")
            trainer = self._create_trainer(
                model, tokenizer, train_dataset, eval_dataset, training_args
            )
            
            # í›ˆë ¨ ì‹¤í–‰
            logger.info(f"ğŸš€ í›ˆë ¨ ì‹œì‘! ì´ {request.num_epochs} ì—í¬í¬ ì§„í–‰...")
            trainer.train()
            logger.info(f"ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
            
            # 7. ëª¨ë¸ ì €ì¥
            await self._update_job_progress(job_id, 90.0, "ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
            logger.info(f"ğŸ” 7ë‹¨ê³„: ëª¨ë¸ ì €ì¥ ì¤‘...")
            model_id = f"esg-{request.model_type}-{job_id[:8]}"
            model_path = os.path.join(self.models_dir, model_id)
            
            trainer.save_model(model_path)
            tokenizer.save_pretrained(model_path)
            logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            await self._save_model_metadata(model_id, model_path, request)
            
            # ì‘ì—… ì™„ë£Œ
            self.jobs[job_id].update({
                "status": JobStatus.COMPLETED,
                "progress": 100.0,
                "completed_at": datetime.now(),
                "model_id": model_id,
                "model_path": model_path
            })
            
            total_time = datetime.now() - self.jobs[job_id]["started_at"]
            logger.info(f"ğŸŠ íŒŒì¸íŠœë‹ ì‘ì—… ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {total_time}")
            logger.info(f"ğŸ“¦ ìƒì„±ëœ ëª¨ë¸ ID: {model_id}")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¸íŠœë‹ ì‘ì—… ì‹¤íŒ¨ ({job_id}): {str(e)}")
            self.jobs[job_id].update({
                "status": JobStatus.FAILED,
                "error_message": str(e),
                "completed_at": datetime.now()
            })
    
    async def _prepare_dataset(self, request: TuningRequest) -> Dataset:
        """ë°ì´í„°ì…‹ ì¤€ë¹„ - í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬"""
        # í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬
        import glob
        pdf_files = glob.glob(os.path.join(request.reports_folder, "*.pdf"))
        logger.info(f"Found {len(pdf_files)} PDF files in folder: {request.reports_folder}")
        
        if not pdf_files:
            raise ValueError(f"í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {request.reports_folder}")
        
        df = await self.data_service.extract_data_from_reports(
            pdf_files, request.task_type
        )
        
        return Dataset.from_pandas(df)
    
    async def _load_model_and_tokenizer(self, request: TuningRequest):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        # í† í°ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None
        token_kwargs = {"token": self.hf_token} if self.hf_token else {}
        
        # ê³„ì† í•™ìŠµì¸ ê²½ìš° ê¸°ì¡´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ
        if request.is_continual_learning and request.base_model_path:
            logger.info(f"ğŸ” ê³„ì† í•™ìŠµ ëª¨ë“œ: ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„")
            logger.info(f"ğŸ“ models_dir: {self.models_dir}")
            logger.info(f"ğŸ¯ base_model_path: {request.base_model_path}")
            
            # ê¸°ì¡´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ê²½ë¡œ í™•ì¸
            if not os.path.exists(request.base_model_path):
                # ëª¨ë¸ IDë¡œ ê²½ë¡œ ì°¾ê¸°
                model_path = os.path.join(self.models_dir, request.base_model_path)
                logger.info(f"ğŸ” ì „ì²´ ëª¨ë¸ ê²½ë¡œ: {model_path}")
                logger.info(f"ğŸ“‚ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(model_path)}")
                
                if os.path.exists(model_path):
                    request.base_model_path = model_path
                    logger.info(f"âœ… ëª¨ë¸ ê²½ë¡œ ì—…ë°ì´íŠ¸: {model_path}")
                else:
                    # ë””ë²„ê¹…ì„ ìœ„í•´ models_dir ë‚´ìš© í™•ì¸
                    try:
                        models_list = os.listdir(self.models_dir) if os.path.exists(self.models_dir) else []
                        logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤: {models_list}")
                    except Exception as e:
                        logger.error(f"âŒ models_dir ì½ê¸° ì‹¤íŒ¨: {e}")
                    
                    raise ValueError(f"ê¸°ì¡´ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.base_model_path}")
            
            # ê¸°ì¡´ ëª¨ë¸ì˜ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = os.path.join(request.base_model_path, "metadata.json")
            if os.path.exists(metadata_path):
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                base_model_name = metadata["base_model"]
            else:
                base_model_name = request.model_name
            
            # ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(base_model_name, **token_kwargs)
            
            if request.task_type == RequestTaskType.CLASSIFICATION:
                model = AutoModelForSequenceClassification.from_pretrained(
                    base_model_name, num_labels=2, **token_kwargs
                )
            elif request.task_type == RequestTaskType.QUESTION_ANSWERING:
                model = AutoModelForQuestionAnswering.from_pretrained(base_model_name, **token_kwargs)
            else:
                model = AutoModelForCausalLM.from_pretrained(base_model_name, **token_kwargs)
            
            # ê¸°ì¡´ LoRA ì–´ëŒ‘í„° ë¡œë“œ
            from peft import PeftModel
            model = PeftModel.from_pretrained(model, request.base_model_path)
            
            # ê³„ì† í•™ìŠµì„ ìœ„í•´ ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  gradient í™œì„±í™”
            model.train()
            for param in model.parameters():
                param.requires_grad = True
            
            # LoRA íŒŒë¼ë¯¸í„°ë§Œ í›ˆë ¨ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
            model.print_trainable_parameters()
            
            logger.info("ê¸°ì¡´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - gradient í™œì„±í™”ë¨")
            
        else:
            # ìƒˆë¡œìš´ íŒŒì¸íŠœë‹ì¸ ê²½ìš° ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(request.model_name, **token_kwargs)
            
            if request.task_type == RequestTaskType.CLASSIFICATION:
                model = AutoModelForSequenceClassification.from_pretrained(
                    request.model_name, num_labels=2, **token_kwargs  # ESG/Non-ESG ë¶„ë¥˜
                )
            elif request.task_type == RequestTaskType.QUESTION_ANSWERING:
                model = AutoModelForQuestionAnswering.from_pretrained(request.model_name, **token_kwargs)
            else:
                model = AutoModelForCausalLM.from_pretrained(request.model_name, **token_kwargs)
        
        # íŒ¨ë”© í† í° ì„¤ì • (BERTì˜ ê²½ìš° [PAD] í† í° ì‚¬ìš©)
        if tokenizer.pad_token is None:
            if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:
                tokenizer.pad_token = tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id)
            else:
                tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else "[PAD]"
        
        return model, tokenizer
    
    def _setup_lora(self, model, request: TuningRequest):
        """RTX 2080 ìµœì í™” LoRA ì„¤ì •"""
        if request.task_type == RequestTaskType.CLASSIFICATION:
            task_type = TaskType.SEQ_CLS
        elif request.task_type == RequestTaskType.QUESTION_ANSWERING:
            task_type = TaskType.QUESTION_ANS
        else:
            task_type = TaskType.CAUSAL_LM
        
        # RTX 2080 ìµœì í™” LoRA ì„¤ì • ì‚¬ìš©
        rtx2080_lora_config = get_rtx2080_lora_config()
        
        lora_config = LoraConfig(
            task_type=task_type,
            r=request.lora_r if hasattr(request, 'lora_r') and request.lora_r else rtx2080_lora_config["r"],
            lora_alpha=request.lora_alpha if hasattr(request, 'lora_alpha') and request.lora_alpha else rtx2080_lora_config["lora_alpha"],
            lora_dropout=request.lora_dropout if hasattr(request, 'lora_dropout') and request.lora_dropout else rtx2080_lora_config["lora_dropout"],
            target_modules=rtx2080_lora_config["target_modules"]
        )
        
        logger.info(f"ğŸ¯ RTX 2080 ìµœì í™” LoRA ì„¤ì •: r={lora_config.r}, alpha={lora_config.lora_alpha}, dropout={lora_config.lora_dropout}")
        
        return get_peft_model(model, lora_config)
    
    async def _preprocess_data(self, dataset: Dataset, tokenizer, request: TuningRequest):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        def tokenize_function(examples):
            # í…ìŠ¤íŠ¸ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            texts = examples["text"]
            if isinstance(texts, str):
                texts = [texts]
            elif isinstance(texts, list) and len(texts) > 0 and isinstance(texts[0], list):
                # ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í‰íƒ„í™”
                texts = [str(item) for sublist in texts for item in (sublist if isinstance(sublist, list) else [sublist])]
            
            # í† í¬ë‚˜ì´ì§• (íŒ¨ë”©ì€ ë‚˜ì¤‘ì— data_collatorì—ì„œ ì²˜ë¦¬)
            tokenized = tokenizer(
                texts,
                truncation=True,
                padding=False,  # ë°°ì¹˜ë³„ë¡œ ë™ì  íŒ¨ë”© ì‚¬ìš©
                max_length=request.max_length,
                return_tensors=None  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            )
            
            # í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì˜ ê²½ìš° labelsëŠ” DataCollatorForLanguageModelingì—ì„œ ìë™ ì²˜ë¦¬
            # ë‹¤ë¥¸ íƒœìŠ¤í¬ì˜ ê²½ìš°ì—ë§Œ ë¼ë²¨ ì„¤ì •
            if request.task_type != RequestTaskType.TEXT_GENERATION:
                if "labels" in examples:
                    labels = examples["labels"]
                elif "label" in examples:
                    labels = examples["label"]
                else:
                    labels = [0] * len(texts)  # ê¸°ë³¸ê°’
                
                # ë¼ë²¨ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if not isinstance(labels, list):
                    labels = [labels]
                
                tokenized["labels"] = labels
            
            return tokenized
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ í† í¬ë‚˜ì´ì§•
        tokenized_dataset = dataset.map(
            tokenize_function, 
            batched=True,
            remove_columns=dataset.column_names  # ì›ë³¸ ì»¬ëŸ¼ ì œê±°
        )
        
        # í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        print(f"Dataset columns after tokenization: {tokenized_dataset.column_names}")
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
        return split_dataset["train"], split_dataset["test"]
    
    def _create_training_args(self, request: TuningRequest, job_id: str) -> TrainingArguments:
        """RTX 2080 ìµœì í™” í›ˆë ¨ ì¸ì ìƒì„±"""
        output_dir = request.output_dir or f"/app/models/temp_{job_id}"
        
        # RTX 2080 ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        rtx2080_args = get_rtx2080_training_args()
        
        # ë°°ì¹˜ í¬ê¸° ìµœì í™” (ëª¨ë¸ íƒ€ì…ì— ë”°ë¼)
        optimal_batch_size = get_optimal_batch_size(request.model_name, "train")
        train_batch_size = min(request.batch_size, optimal_batch_size) if hasattr(request, 'batch_size') and request.batch_size else optimal_batch_size
        
        eval_batch_size = get_optimal_batch_size(request.model_name, "eval")
        
        logger.info(f"ğŸ¯ RTX 2080 ìµœì í™” ë°°ì¹˜ í¬ê¸°: í›ˆë ¨={train_batch_size}, ê²€ì¦={eval_batch_size}")
        
        # GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        memory_info = monitor_gpu_memory()
        if memory_info.get("usage_percent", 0) > 70:
            logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_info.get('usage_percent')}%")
            cleanup_gpu_memory()
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            learning_rate=request.learning_rate,
            per_device_train_batch_size=train_batch_size,
            per_device_eval_batch_size=eval_batch_size,
            num_train_epochs=request.num_epochs,
            warmup_steps=rtx2080_args["warmup_steps"],
            logging_dir=f"/app/logs/{job_id}",
            logging_steps=rtx2080_args["logging_steps"],
            save_steps=rtx2080_args["save_steps"],
            eval_steps=rtx2080_args["eval_steps"],
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=rtx2080_args["load_best_model_at_end"],
            metric_for_best_model=rtx2080_args["metric_for_best_model"],
            greater_is_better=rtx2080_args["greater_is_better"],
            save_total_limit=rtx2080_args["save_total_limit"],
            gradient_accumulation_steps=rtx2080_args["gradient_accumulation_steps"],
            gradient_checkpointing=rtx2080_args["gradient_checkpointing"],
            fp16=rtx2080_args["fp16"],  # RTX 2080 FP16 ì§€ì›
            dataloader_num_workers=rtx2080_args["dataloader_num_workers"],
            remove_unused_columns=rtx2080_args["remove_unused_columns"],
            report_to=[] if not request.wandb_project else ["wandb"],
            run_name=f"esg-tuning-{job_id[:8]}" if request.wandb_project else None,
        )
        
        logger.info(f"âœ… RTX 2080 ìµœì í™” í›ˆë ¨ ì„¤ì • ì™„ë£Œ (FP16: {training_args.fp16}, Gradient Checkpointing: {training_args.gradient_checkpointing})")
        
        return training_args
    
    def _create_trainer(self, model, tokenizer, train_dataset, eval_dataset, training_args):
        """íŠ¸ë ˆì´ë„ˆ ìƒì„±"""
        # í…ìŠ¤íŠ¸ ìƒì„±ìš© data collator ì‚¬ìš©
        from transformers import DataCollatorForLanguageModeling
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,  # ìê¸°íšŒê·€ ì–¸ì–´ ëª¨ë¸ë§ (GPT ìŠ¤íƒ€ì¼)
            pad_to_multiple_of=None
        )
        
        return Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=tokenizer,  # tokenizer ëŒ€ì‹  processing_class ì‚¬ìš©
            data_collator=data_collator
        )
    
    async def _save_model_metadata(self, model_id: str, model_path: str, request: TuningRequest):
        """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            "model_id": model_id,
            "base_model": request.model_name,
            "model_type": request.model_type,
            "task_type": request.task_type,
            "created_at": datetime.now().isoformat(),
            "hyperparameters": {
                "learning_rate": request.learning_rate,
                "batch_size": request.batch_size,
                "num_epochs": request.num_epochs,
                "max_length": request.max_length,
                "use_lora": request.use_lora,
                "lora_r": request.lora_r if request.use_lora else None,
                "lora_alpha": request.lora_alpha if request.use_lora else None
            }
        }
        
        metadata_path = os.path.join(model_path, "metadata.json")
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    async def _update_job_progress(self, job_id: str, progress: float, message: str = ""):
        """ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        if job_id in self.jobs:
            self.jobs[job_id]["progress"] = progress
            if message:
                self.jobs[job_id]["current_message"] = message
    
    async def get_job_status(self, job_id: str) -> TuningStatus:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        if job_id not in self.jobs:
            raise ValueError(f"Job {job_id} not found")
        
        job = self.jobs[job_id]
        
        return TuningStatus(
            job_id=job_id,
            status=job["status"],
            progress=job.get("progress", 0.0),
            created_at=job["created_at"],
            started_at=job.get("started_at"),
            completed_at=job.get("completed_at"),
            error_message=job.get("error_message"),
            model_id=job.get("model_id"),
            model_path=job.get("model_path")
        )
    
    async def list_available_models(self) -> ModelListResponse:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        models = []
        
        if os.path.exists(self.models_dir):
            for model_dir in os.listdir(self.models_dir):
                model_path = os.path.join(self.models_dir, model_dir)
                metadata_path = os.path.join(model_path, "metadata.json")
                
                if os.path.isdir(model_path) and os.path.exists(metadata_path):
                    with open(metadata_path, "r", encoding="utf-8") as f:
                        metadata = json.load(f)
                    
                    # íŒŒì¼ í¬ê¸° ê³„ì‚°
                    total_size = sum(
                        os.path.getsize(os.path.join(model_path, f))
                        for f in os.listdir(model_path)
                        if os.path.isfile(os.path.join(model_path, f))
                    )
                    
                    models.append(ModelInfo(
                        model_id=metadata["model_id"],
                        model_name=metadata["model_id"],
                        base_model=metadata["base_model"],
                        task_type=metadata["task_type"],
                        created_at=datetime.fromisoformat(metadata["created_at"]),
                        file_size=total_size
                    ))
        
        return ModelListResponse(
            models=models,
            total_count=len(models)
        )
    
    async def delete_model(self, model_id: str) -> DeleteModelResponse:
        """ëª¨ë¸ ì‚­ì œ"""
        model_path = os.path.join(self.models_dir, model_id)
        
        if not os.path.exists(model_path):
            raise ValueError(f"Model {model_id} not found")
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì‚­ì œ
        import shutil
        shutil.rmtree(model_path)
        
        return DeleteModelResponse(
            model_id=model_id,
            message=f"ëª¨ë¸ {model_id}ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            deleted_at=datetime.now()
        ) 