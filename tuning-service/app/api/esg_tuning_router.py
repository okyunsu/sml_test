from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from typing import List, Optional
from app.domain.controller.tuning_controller import TuningController
from app.domain.model.tuning_request import TuningRequest, InferenceRequest
from app.domain.model.tuning_response import TuningResponse, InferenceResponse, TuningStatus
from app.config.gpu_config import monitor_gpu_memory, cleanup_gpu_memory
import logging
import os

router = APIRouter(prefix="/api/v1/tuning", tags=["ESG Fine-tuning"])

tuning_controller = TuningController()
logger = logging.getLogger(__name__)

def _set_default_reports_folder(request: TuningRequest) -> None:
    """ê³µí†µ í´ë” ê²½ë¡œ ì„¤ì • ë¡œì§"""
    if not request.reports_folder or request.reports_folder == "/app/data/uploads":
        # ë¡œì»¬ í™˜ê²½ê³¼ Docker í™˜ê²½ êµ¬ë¶„
        if os.path.exists("/app"):
            request.reports_folder = "/app/data/uploads"
        else:
            request.reports_folder = "data/uploads"

# ============================================================================
# ğŸ¯ í•µì‹¬ íŠœë‹ ê¸°ëŠ¥
# ============================================================================

@router.post("/start", response_model=TuningResponse)
async def start_fine_tuning(
    background_tasks: BackgroundTasks,
    request: TuningRequest
):
    """ESG ë³´ê³ ì„œ ê¸°ë°˜ íŒŒì¸íŠœë‹ ì‹œì‘"""
    try:
        _set_default_reports_folder(request)
        return await tuning_controller.start_fine_tuning(background_tasks, request)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/continue", response_model=TuningResponse)
async def continue_fine_tuning(
    background_tasks: BackgroundTasks,
    request: TuningRequest
):
    """ê¸°ì¡´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì†ê°€ëŠ¥ê²½ì˜ë³´ê³ ì„œë¡œ ê³„ì† í•™ìŠµ ì‹œì‘"""
    try:
        # ì§€ì†ê°€ëŠ¥ê²½ì˜ë³´ê³ ì„œ í´ë”ë¡œ ì„¤ì •
        request.reports_folder = "data/uploads"
        request.is_continual_learning = True
        
        logger.info(f"ğŸ”„ ê³„ì† í•™ìŠµ ì‹œì‘: {request.base_model_path} â†’ ì§€ì†ê°€ëŠ¥ê²½ì˜ë³´ê³ ì„œ í•™ìŠµ")
        return await tuning_controller.start_fine_tuning(background_tasks, request)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/status/{job_id}", response_model=TuningStatus)
async def get_tuning_status(job_id: str):
    """íŒŒì¸íŠœë‹ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    try:
        return await tuning_controller.get_tuning_status(job_id)
    except Exception as e:
        raise HTTPException(status_code=404, detail=str(e))

@router.post("/inference", response_model=InferenceResponse)
async def inference(request: InferenceRequest):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡  ìˆ˜í–‰"""
    try:
        return await tuning_controller.inference(request)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# ğŸ“ ëª¨ë¸ ê´€ë¦¬
# ============================================================================

@router.get("/models")
async def list_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        return await tuning_controller.list_models()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/models/{model_id}")
async def delete_model(model_id: str):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚­ì œ"""
    try:
        return await tuning_controller.delete_model(model_id)
    except Exception as e:
        raise HTTPException(status_code=404, detail=str(e))

# ============================================================================
# ğŸ® ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
# ============================================================================

@router.get("/health")
async def health_check():
    """ğŸ¥ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ (GPU ìƒíƒœ í¬í•¨)"""
    try:
        import torch
        from datetime import datetime
        
        # ê¸°ë³¸ ì„œë¹„ìŠ¤ ìƒíƒœ
        service_status = {
            "service": "ESG Fine-tuning Service",
            "status": "healthy",
            "version": "2.0.0-rtx2080",
            "timestamp": datetime.now().isoformat()
        }
        
        # GPU ìƒíƒœ í†µí•©
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            memory_info = monitor_gpu_memory()
            
            service_status["gpu"] = {
                "available": True,
                "name": gpu_name,
                "memory": memory_info,
                "rtx2080_optimized": "2080" in gpu_name,
                "cuda_version": getattr(torch.version, 'cuda', 'unknown')
            }
        else:
            service_status["gpu"] = {
                "available": False,
                "message": "CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘"
            }
        
        return service_status
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

@router.post("/gpu/cleanup")
async def cleanup_gpu():
    """ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        import torch
        
        if not torch.cuda.is_available():
            return {
                "success": False,
                "message": "CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # ì •ë¦¬ ì „í›„ ë©”ëª¨ë¦¬ ìƒíƒœ ë¹„êµ
        before_memory = monitor_gpu_memory()
        cleanup_gpu_memory()
        after_memory = monitor_gpu_memory()
        
        freed_memory = before_memory.get("allocated_gb", 0) - after_memory.get("allocated_gb", 0)
        
        return {
            "success": True,
            "message": "GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ",
            "freed_memory_gb": round(freed_memory, 2),
            "memory_status": {
                "before": before_memory,
                "after": after_memory
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}") 