"""
RTX 2080 ìµœì í™” GPU ì„¤ì •
- VRAM: 8GB
- CUDA Cores: 2,944
- CUDA Capability: 7.5
"""

import torch
import gc
import os
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class RTX2080Config:
    """RTX 2080 ìµœì í™” ì„¤ì •"""
    
    # RTX 2080 ìŠ¤í™
    GPU_NAME = "RTX 2080"
    VRAM_SIZE = 8  # GB
    CUDA_CAPABILITY = 7.5
    CUDA_CORES = 2944
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    MAX_MEMORY_FRACTION = 0.85  # VRAMì˜ 85%ë§Œ ì‚¬ìš©
    MEMORY_GROWTH = True
    GRADIENT_CHECKPOINTING = True
    
    # ë°°ì¹˜ í¬ê¸° ìµœì í™” (VRAM 8GB ê¸°ì¤€)
    BATCH_SIZES = {
        "bert-base": {
            "train": 8,
            "eval": 16,
            "inference": 32
        },
        "bert-large": {
            "train": 4,
            "eval": 8,
            "inference": 16
        },
        "gpt2": {
            "train": 6,
            "eval": 12,
            "inference": 24
        }
    }
    
    # LoRA ìµœì í™” ì„¤ì •
    LORA_CONFIG = {
        "r": 16,  # RTX 2080ì— ìµœì í™”ëœ rank
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "target_modules": ["query", "value", "key", "dense"]
    }
    
    # í•™ìŠµ ìµœì í™” ì„¤ì •
    TRAINING_CONFIG = {
        "fp16": True,  # RTX 2080ì€ FP16 ì§€ì›
        "dataloader_num_workers": 2,  # CPU ì½”ì–´ ìˆ˜ ê³ ë ¤
        "gradient_accumulation_steps": 4,
        "warmup_steps": 100,
        "save_steps": 500,
        "eval_steps": 250,
        "logging_steps": 50
    }

def setup_rtx2080_environment():
    """RTX 2080 í™˜ê²½ ì„¤ì •"""
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not torch.cuda.is_available():
        logger.error("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return False
    
    # GPU ì •ë³´ í™•ì¸
    gpu_count = torch.cuda.device_count()
    current_device = torch.cuda.current_device()
    gpu_name = torch.cuda.get_device_name(current_device)
    
    logger.info(f"ğŸ® GPU ì •ë³´:")
    logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpu_count}ê°œ")
    logger.info(f"   - í˜„ì¬ GPU: {gpu_name}")
    logger.info(f"   - ë””ë°”ì´ìŠ¤ ID: {current_device}")
    
    # VRAM ì •ë³´
    total_memory = torch.cuda.get_device_properties(current_device).total_memory
    total_memory_gb = total_memory / (1024**3)
    logger.info(f"   - ì´ VRAM: {total_memory_gb:.1f}GB")
    
    # RTX 2080 í™•ì¸
    if "2080" in gpu_name:
        logger.info("âœ… RTX 2080 ê°ì§€ë¨ - ìµœì í™” ì„¤ì • ì ìš©")
    else:
        logger.warning(f"âš ï¸ RTX 2080ì´ ì•„ë‹Œ GPU ê°ì§€: {gpu_name}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    torch.cuda.empty_cache()
    gc.collect()
    
    # CUDA ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì •
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    return True

def get_optimal_batch_size(model_type: str, task: str = "train") -> int:
    """ëª¨ë¸ íƒ€ì…ê³¼ ì‘ì—…ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
    
    config = RTX2080Config()
    
    # ëª¨ë¸ íƒ€ì… ì •ê·œí™”
    if "bert" in model_type.lower():
        if "large" in model_type.lower():
            model_key = "bert-large"
        else:
            model_key = "bert-base"
    elif "gpt" in model_type.lower():
        model_key = "gpt2"
    else:
        model_key = "bert-base"  # ê¸°ë³¸ê°’
    
    batch_size = config.BATCH_SIZES.get(model_key, {}).get(task, 8)
    logger.info(f"ğŸ¯ ìµœì  ë°°ì¹˜ í¬ê¸°: {batch_size} (ëª¨ë¸: {model_type}, ì‘ì—…: {task})")
    
    return batch_size

def monitor_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    if not torch.cuda.is_available():
        return {}
    
    device = torch.cuda.current_device()
    allocated = torch.cuda.memory_allocated(device)
    cached = torch.cuda.memory_reserved(device)
    total = torch.cuda.get_device_properties(device).total_memory
    
    allocated_gb = allocated / (1024**3)
    cached_gb = cached / (1024**3)
    total_gb = total / (1024**3)
    
    usage_percent = (allocated / total) * 100
    
    memory_info = {
        "allocated_gb": round(allocated_gb, 2),
        "cached_gb": round(cached_gb, 2),
        "total_gb": round(total_gb, 2),
        "usage_percent": round(usage_percent, 1),
        "free_gb": round(total_gb - allocated_gb, 2)
    }
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 80% ì´ìƒì´ë©´ ê²½ê³ 
    if usage_percent > 80:
        logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {usage_percent:.1f}%")
    
    return memory_info

def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        logger.info("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

def get_rtx2080_training_args() -> Dict[str, Any]:
    """RTX 2080 ìµœì í™” í•™ìŠµ ì¸ì ë°˜í™˜"""
    
    config = RTX2080Config()
    
    return {
        "per_device_train_batch_size": config.BATCH_SIZES["bert-base"]["train"],
        "per_device_eval_batch_size": config.BATCH_SIZES["bert-base"]["eval"],
        "gradient_accumulation_steps": config.TRAINING_CONFIG["gradient_accumulation_steps"],
        "warmup_steps": config.TRAINING_CONFIG["warmup_steps"],
        "save_steps": config.TRAINING_CONFIG["save_steps"],
        "eval_steps": config.TRAINING_CONFIG["eval_steps"],
        "logging_steps": config.TRAINING_CONFIG["logging_steps"],
        "fp16": config.TRAINING_CONFIG["fp16"],
        "dataloader_num_workers": config.TRAINING_CONFIG["dataloader_num_workers"],
        "gradient_checkpointing": config.GRADIENT_CHECKPOINTING,
        "remove_unused_columns": False,
        "load_best_model_at_end": True,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": False,
        "save_total_limit": 2,  # ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½
    }

def get_rtx2080_lora_config() -> Dict[str, Any]:
    """RTX 2080 ìµœì í™” LoRA ì„¤ì • ë°˜í™˜"""
    
    config = RTX2080Config()
    return config.LORA_CONFIG.copy()

# ì´ˆê¸°í™” ì‹œ í™˜ê²½ ì„¤ì •
if __name__ == "__main__":
    setup_rtx2080_environment()
    memory_info = monitor_gpu_memory()
    print(f"GPU ë©”ëª¨ë¦¬ ì •ë³´: {memory_info}") 