# ESG Fine-tuning Service (RTX 2080 ìµœì í™”)

ğŸš€ **RTX 2080 ìµœì í™”ëœ ESG ë³´ê³ ì„œ ê¸°ë°˜ í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒì¸íŠœë‹ ì„œë¹„ìŠ¤**

## ğŸ® **RTX 2080 ìµœì í™” íŠ¹ì§•**

### **í•˜ë“œì›¨ì–´ ìµœì í™”**
- **GPU**: NVIDIA RTX 2080 (8GB VRAM)
- **CUDA**: 11.8 ì§€ì›
- **FP16**: í˜¼í•© ì •ë°€ë„ í›ˆë ¨ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± 2ë°° í–¥ìƒ
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ë™ì  ë©”ëª¨ë¦¬ í• ë‹¹ ë° ìë™ ì •ë¦¬

### **ì„±ëŠ¥ ìµœì í™”**
- **ë°°ì¹˜ í¬ê¸°**: ëª¨ë¸ë³„ ìµœì í™” (BERT-base: 8, BERT-large: 4)
- **LoRA ì„¤ì •**: RTX 2080ì— ìµœì í™”ëœ rank=16, alpha=32
- **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½
- **XFormers**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ attention ë©”ì»¤ë‹ˆì¦˜

### **ìë™ ëª¨ë‹ˆí„°ë§**
- **ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**
- **ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬**
- **ì„±ëŠ¥ ì§€í‘œ ì¶”ì **

## ğŸš€ **ë¹ ë¥¸ ì‹œì‘**

### **1. í™˜ê²½ ì„¤ì •**

```bash
# NVIDIA Docker ëŸ°íƒ€ì„ ì„¤ì¹˜ (í•„ìˆ˜)
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# í”„ë¡œì íŠ¸ í´ë¡ 
git clone <repository>
cd tuning-service
```

### **2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

`.env` íŒŒì¼ ìƒì„±:
```bash
# Hugging Face Hub Token
HUGGINGFACE_HUB_TOKEN=your_token_here

# RTX 2080 ìµœì í™” ì„¤ì •
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
CUDA_LAUNCH_BLOCKING=1
TORCH_CUDA_ARCH_LIST=7.5

# ì„±ëŠ¥ ìµœì í™”
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4
TOKENIZERS_PARALLELISM=false

# ë©”ëª¨ë¦¬ ìµœì í™”
TRANSFORMERS_CACHE=/app/models
HF_HOME=/app/models

# ë¡œê¹… ì„¤ì •
PYTHONUNBUFFERED=1
LOG_LEVEL=INFO

# Weights & Biases (ì„ íƒì‚¬í•­)
WANDB_PROJECT=esg-tuning-rtx2080
WANDB_DISABLED=true
```

### **3. ì„œë¹„ìŠ¤ ì‹¤í–‰**

```bash
# RTX 2080 ìµœì í™” ë¹Œë“œ ë° ì‹¤í–‰
docker-compose up --build -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f esg-tuning-service

# GPU ìƒíƒœ í™•ì¸
curl http://localhost:9001/api/v1/tuning/gpu/status
```

## ğŸ“Š **RTX 2080 ìµœì í™” ì„±ëŠ¥**

### **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**
| ëª¨ë¸ | ê¸°ì¡´ | RTX 2080 ìµœì í™” | ì ˆì•½ë¥  |
|------|------|----------------|--------|
| BERT-base | ~6GB | ~3.5GB | 42% |
| BERT-large | OOM | ~7GB | ì‚¬ìš© ê°€ëŠ¥ |

### **í›ˆë ¨ ì†ë„**
| ì‘ì—… | ê¸°ì¡´ | RTX 2080 ìµœì í™” | í–¥ìƒë¥  |
|------|------|----------------|--------|
| íŒŒì¸íŠœë‹ | 2-3ì‹œê°„ | 1-1.5ì‹œê°„ | 50% |
| ì¶”ë¡  | 100ms | 50ms | 100% |

### **ë°°ì¹˜ í¬ê¸° ìµœì í™”**
```python
# ìë™ ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
BERT-base: train=8, eval=16, inference=32
BERT-large: train=4, eval=8, inference=16
GPT-2: train=6, eval=12, inference=24
```

## ğŸ”§ **RTX 2080 ì „ìš© API**

### **GPU ìƒíƒœ ëª¨ë‹ˆí„°ë§**
```bash
# GPU ë©”ëª¨ë¦¬ ë° ìƒíƒœ í™•ì¸
GET /api/v1/tuning/gpu/status

# ì‘ë‹µ ì˜ˆì‹œ
{
  "gpu_available": true,
  "gpu_name": "NVIDIA GeForce RTX 2080",
  "memory_info": {
    "allocated_gb": 2.1,
    "total_gb": 8.0,
    "usage_percent": 26.3,
    "free_gb": 5.9
  },
  "is_rtx2080": true,
  "optimization_status": "RTX 2080 ìµœì í™” ì ìš©ë¨"
}
```

### **GPU ë©”ëª¨ë¦¬ ì •ë¦¬**
```bash
# ë©”ëª¨ë¦¬ ì •ë¦¬
POST /api/v1/tuning/gpu/cleanup

# ì‘ë‹µ ì˜ˆì‹œ
{
  "success": true,
  "message": "GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ",
  "freed_memory_gb": 1.2
}
```

## ğŸ¯ **íŒŒì¸íŠœë‹ ì˜ˆì‹œ (RTX 2080 ìµœì í™”)**

```json
{
  "model_name": "klue/bert-base",
  "model_type": "bert",
  "task_type": "classification",
  "reports_folder": "/app/data/uploads",
  "learning_rate": 0.00002,
  "batch_size": 8,  // RTX 2080 ìµœì í™”
  "num_epochs": 3,
  "use_lora": true
}
```

**ìë™ ìµœì í™” ì ìš©:**
- âœ… ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì • (8 â†’ ìµœì ê°’)
- âœ… LoRA ì„¤ì • ìµœì í™” (r=16, alpha=32)
- âœ… FP16 í˜¼í•© ì •ë°€ë„ í™œì„±í™”
- âœ… Gradient Checkpointing ì ìš©
- âœ… ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìë™ ì •ë¦¬

## ğŸ¥ **ìƒíƒœ í™•ì¸**

```bash
# ì„œë¹„ìŠ¤ ìƒíƒœ (RTX 2080 ì •ë³´ í¬í•¨)
curl http://localhost:9001/api/v1/tuning/health

# ì‘ë‹µ ì˜ˆì‹œ
{
  "service": "ESG Fine-tuning Service",
  "status": "healthy",
  "version": "2.0.0-rtx2080",
  "gpu_available": true,
  "gpu_name": "NVIDIA GeForce RTX 2080",
  "rtx2080_optimized": true,
  "cuda_version": "11.8"
}
```

## ğŸ” **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…**

### **CUDA ì˜¤ë¥˜**
```bash
# CUDA ì„¤ì¹˜ í™•ì¸
nvidia-smi
docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu22.04 nvidia-smi
```

### **ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
curl -X POST http://localhost:9001/api/v1/tuning/gpu/cleanup

# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
"batch_size": 4  # ê¸°ë³¸ê°’ 8ì—ì„œ 4ë¡œ ê°ì†Œ
```

### **ì„±ëŠ¥ ìµœì í™”**
```bash
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
docker exec esg-tuning-service env | grep CUDA
docker exec esg-tuning-service env | grep TORCH
```

## ğŸ“ˆ **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**

RTX 2080 ì „ìš© ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥:
- ğŸ® ì‹¤ì‹œê°„ GPU ì‚¬ìš©ë¥ 
- ğŸ’¾ VRAM ì‚¬ìš©ëŸ‰ ì¶”ì 
- ğŸ”¥ ì˜¨ë„ ëª¨ë‹ˆí„°ë§
- âš¡ ì „ë ¥ ì†Œë¹„ëŸ‰
- ğŸ“Š í›ˆë ¨ ì„±ëŠ¥ ì§€í‘œ

## ğŸŠ **RTX 2080 ìµœì í™” ê²°ê³¼**

### **Before (ì¼ë°˜ ì„¤ì •)**
- âŒ BERT-large í›ˆë ¨ ë¶ˆê°€ (OOM)
- â° íŒŒì¸íŠœë‹ 2-3ì‹œê°„ ì†Œìš”
- ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„íš¨ìœ¨ì 
- ğŸŒ ì¶”ë¡  ì†ë„ ëŠë¦¼

### **After (RTX 2080 ìµœì í™”)**
- âœ… BERT-large í›ˆë ¨ ê°€ëŠ¥
- âš¡ íŒŒì¸íŠœë‹ 1-1.5ì‹œê°„ ì™„ë£Œ
- ğŸ¯ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½
- ğŸš€ ì¶”ë¡  ì†ë„ 2ë°° í–¥ìƒ

---

**ğŸ® RTX 2080ìœ¼ë¡œ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ESG AI ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ê²½í—˜í•˜ì„¸ìš”!**

ESG ë³´ê³ ì„œ ê¸°ë°˜ í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒì¸íŠœë‹ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

- **ESG ë³´ê³ ì„œ ì—…ë¡œë“œ**: PDF í˜•íƒœì˜ ESG ë³´ê³ ì„œ íŒŒì¸íŠœë‹ìš© ë°ì´í„° ìë™ ìƒì„±
- **ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›**: BERT, RoBERTa, ELECTRA, GPT, T5 ë“±
- **ë‹¤ì–‘í•œ ì‘ì—… íƒ€ì…**: ë¶„ë¥˜, ì§ˆì˜ì‘ë‹µ, í…ìŠ¤íŠ¸ ìƒì„±, ìš”ì•½
- **LoRA íŒŒì¸íŠœë‹**: íš¨ìœ¨ì ì¸ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: íŒŒì¸íŠœë‹ ì§„í–‰ë¥  ë° ìƒíƒœ ì‹¤ì‹œê°„ í™•ì¸
- **ëª¨ë¸ ì¶”ë¡ **: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¶”ë¡  ì„œë¹„ìŠ¤

## ì‹œì‘í•˜ê¸°

### 1. Docker Composeë¡œ ì‹¤í–‰

```bash
cd tuning-service
docker-compose up -d
```

### 2. ë¡œì»¬ ê°œë°œ í™˜ê²½

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì„œë¹„ìŠ¤ ì‹¤í–‰
uvicorn main:app --host 0.0.0.0 --port 9001 --reload
```

## API ë¬¸ì„œ

ì„œë¹„ìŠ¤ ì‹¤í–‰ í›„ ë‹¤ìŒ URLì—ì„œ API ë¬¸ì„œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- Swagger UI: http://localhost:9001/docs
- ReDoc: http://localhost:9001/redoc

## API ì—”ë“œí¬ì¸íŠ¸

### 1. ESG ë³´ê³ ì„œ ì—…ë¡œë“œ

```bash
curl -X POST "http://localhost:9001/api/v1/tuning/upload-reports" \
  -H "Content-Type: multipart/form-data" \
  -F "files=@esg_report.pdf"
```

### 2. íŒŒì¸íŠœë‹ ì‹œì‘

```bash
curl -X POST "http://localhost:9001/api/v1/tuning/start" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "klue/bert-base",
    "model_type": "bert",
    "task_type": "classification",
    "report_files": ["/app/data/uploads/esg_report.pdf"],
    "learning_rate": 2e-5,
    "batch_size": 16,
    "num_epochs": 3,
    "use_lora": true
  }'
```

### 3. íŒŒì¸íŠœë‹ ìƒíƒœ í™•ì¸

```bash
curl -X GET "http://localhost:9001/api/v1/tuning/status/{job_id}"
```

### 4. ëª¨ë¸ ì¶”ë¡ 

```bash
curl -X POST "http://localhost:9001/api/v1/tuning/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": "esg-bert-12345678",
    "input_text": "ì‚¼ì„±ì „ìì˜ ESG ê²½ì˜ ì „ëµì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "max_length": 512,
    "temperature": 0.7
  }'
```

### 5. ëª¨ë¸ ëª©ë¡ ì¡°íšŒ

```bash
curl -X GET "http://localhost:9001/api/v1/tuning/models"
```

## ì§€ì›í•˜ëŠ” ëª¨ë¸

### ë² ì´ìŠ¤ ëª¨ë¸
- **BERT**: `klue/bert-base`, `bert-base-multilingual-cased`
- **RoBERTa**: `klue/roberta-base`, `klue/roberta-large`
- **ELECTRA**: `monologg/koelectra-base-v3-discriminator`
- **GPT**: `skt/kogpt2-base-v2`
- **T5**: `KETI-AIR/ke-t5-base`

### ì‘ì—… íƒ€ì…
- **ë¶„ë¥˜ (Classification)**: ESG/Non-ESG ë¶„ë¥˜
- **ì§ˆì˜ì‘ë‹µ (Question Answering)**: ESG ê´€ë ¨ ì§ˆì˜ì‘ë‹µ
- **í…ìŠ¤íŠ¸ ìƒì„± (Text Generation)**: ESG ë‚´ìš© ìƒì„±
- **ìš”ì•½ (Summarization)**: ESG ë³´ê³ ì„œ ìš”ì•½

## íŒŒì¸íŠœë‹ ì„¤ì •

### í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `learning_rate`: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-5)
- `batch_size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 16)
- `num_epochs`: ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
- `max_length`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 512)
- `warmup_steps`: ì›Œë°ì—… ìŠ¤í… (ê¸°ë³¸ê°’: 500)

### LoRA ì„¤ì •
- `use_lora`: LoRA ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: true)
- `lora_r`: LoRA rank (ê¸°ë³¸ê°’: 16)
- `lora_alpha`: LoRA alpha (ê¸°ë³¸ê°’: 32)
- `lora_dropout`: LoRA dropout (ê¸°ë³¸ê°’: 0.1)

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
tuning-service/
â”œâ”€â”€ main.py                 # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ requirements.txt        # Python ì˜ì¡´ì„±
â”œâ”€â”€ Dockerfile             # Docker ì„¤ì •
â”œâ”€â”€ docker-compose.yml     # Docker Compose ì„¤ì •
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/               # API ë¼ìš°í„°
â”‚   â”‚   â””â”€â”€ esg_tuning_router.py
â”‚   â””â”€â”€ domain/            # ë„ë©”ì¸ ë¡œì§
â”‚       â”œâ”€â”€ controller/    # ì»¨íŠ¸ë¡¤ëŸ¬
â”‚       â”œâ”€â”€ service/       # ì„œë¹„ìŠ¤ ë¡œì§
â”‚       â”œâ”€â”€ model/         # ë°ì´í„° ëª¨ë¸
â”‚       â””â”€â”€ repository/    # ë°ì´í„° ì €ì¥ì†Œ
â”œâ”€â”€ models/                # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥
â”œâ”€â”€ data/                  # ë°ì´í„° íŒŒì¼
â”‚   â”œâ”€â”€ uploads/          # ì—…ë¡œë“œëœ íŒŒì¼
â”‚   â””â”€â”€ reports/          # ESG ë³´ê³ ì„œ
â””â”€â”€ logs/                  # ë¡œê·¸ íŒŒì¼
```

## í™˜ê²½ ë³€ìˆ˜

- `PYTHONPATH`: Python ê²½ë¡œ (ê¸°ë³¸ê°’: /app)
- `CUDA_VISIBLE_DEVICES`: GPU ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: 0)
- `TRANSFORMERS_CACHE`: íŠ¸ëœìŠ¤í¬ë¨¸ ìºì‹œ ê²½ë¡œ (ê¸°ë³¸ê°’: /app/models)

## GPU ì§€ì›

GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `docker-compose.yml`ì—ì„œ ë‹¤ìŒ ì„¤ì •ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

## ëª¨ë‹ˆí„°ë§

### Weights & Biases ì—°ë™
íŒŒì¸íŠœë‹ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ë ¤ë©´ `wandb_project` íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”:

```json
{
  "wandb_project": "esg-finetuning",
  ...
}
```

### TensorBoard
ë¡œê·¸ ë””ë ‰í† ë¦¬ì—ì„œ TensorBoardë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
tensorboard --logdir=/app/logs
```

## ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡±
- ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš” (`batch_size`: 8 ë˜ëŠ” 4)
- LoRAë¥¼ ì‚¬ìš©í•˜ì„¸ìš” (`use_lora`: true)
- ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¤„ì´ì„¸ìš” (`max_length`: 256)

### í•™ìŠµ ì†ë„ ê°œì„ 
- GPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
- ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ì„¸ìš”
- ë©€í‹° GPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ê¸°ì—¬í•˜ê¸°

1. ì´ ì €ì¥ì†Œë¥¼ í¬í¬í•˜ì„¸ìš”
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•˜ì„¸ìš” (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ì„ ì»¤ë°‹í•˜ì„¸ìš” (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œí•˜ì„¸ìš” (`git push origin feature/amazing-feature`)
5. Pull Requestë¥¼ ìƒì„±í•˜ì„¸ìš” 


ê¸°ë³¸ ESG ë¶„ë¥˜ ëª¨ë¸ (ê¶Œì¥)
{
  "model_name": "klue/bert-base",
  "model_type": "bert",
  "task_type": "classification",
  "reports_folder": "data/uploads",
  "base_model_path": null,
  "is_continual_learning": false,
  "learning_rate": 2e-5,
  "batch_size": 8,
  "num_epochs": 3,
  "max_length": 512,
  "warmup_steps": 100,
  "use_lora": true,
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "save_steps": 500,
  "eval_steps": 250,
  "output_dir": "models/esg-classification",
  "wandb_project": "esg-tuning"
}


í…ìŠ¤íŠ¸ ìƒì„± (GPT ìŠ¤íƒ€ì¼)
{
  "model_name": "skt/kogpt2-base-v2",
  "model_type": "gpt2",
  "task_type": "text_generation",
  "reports_folder": "data/uploads",
  "batch_size": 6,
  "num_epochs": 2,
  "max_length": 256,
  "learning_rate": 1e-5
}



ìš”ì•½ ì‘ì—…
{
  "model_name": "gogamza/kobart-base-v2",
  "model_type": "bart",
  "task_type": "summarization",
  "reports_folder": "data/uploads",
  "batch_size": 4,
  "num_epochs": 3,
  "max_length": 1024
}





{
  "model_name": "klue/bert-base",
  "model_type": "bert",
  "task_type": "text_generation",
  "reports_folder": "data/uploads",
  "base_model_path": "esg-bert-f406e7e8",
  "is_continual_learning": true,
  "learning_rate": 0.00001,
  "batch_size": 4,
  "num_epochs": 2,
  "max_length": 512,
  "warmup_steps": 200,
  "use_lora": true,
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "save_steps": 250,
  "eval_steps": 250,
  "output_dir": null,
  "wandb_project": null
}




{
  "model_name": "dnotitia/Llama-DNA-1.0-8B-Instruct",
  "model_type": "llama-dna",
  "task_type": "text_generation",
  "reports_folder": "data/reports",
  "base_model_path": "",
  "is_continual_learning": false,
  "learning_rate": 0.00002,
  "batch_size": 4,
  "num_epochs": 2,
  "max_length": 1024,
  "warmup_steps": 200,
  "use_lora": true,
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "save_steps": 100,
  "eval_steps": 100,
  "output_dir": "models/llama-dna-8b",
  "wandb_project": ""
}