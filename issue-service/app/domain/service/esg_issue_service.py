import uuid
import json
import csv
import os
import logging
from keybert import KeyBERT
from konlpy.tag import Okt
from pdfminer.high_level import extract_text
from sentence_transformers import SentenceTransformer, util
from sklearn.cluster import KMeans
from collections import defaultdict, Counter
from app.domain.model.esg_issue_dto import ESGIssue

# ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ì„¤ì •
LOCAL_MODEL_PATH = "/app/models"
ESG_CONFIDENCE_THRESHOLD = 0.7

logger = logging.getLogger(__name__)

OUTPUT_DIR = "/app/output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

GRI_KEYWORD_MAP = {
    "ê¸°í›„ë³€í™”": "GRI 302: Energy",
    "íƒ„ì†Œì¤‘ë¦½": "GRI 305: Emissions",
    "íƒ„ì†Œë°°ì¶œ": "GRI 305: Emissions",
    "ì˜¨ì‹¤ê°€ìŠ¤": "GRI 305: Emissions",
    "ìˆ˜ìì›": "GRI 303: Water",
    "ë‹¤ì–‘ì„±": "GRI 405: Diversity",
    "ì¸ê¶Œ": "GRI 412: Human Rights",
    "ìœ¤ë¦¬": "GRI 205: Anti-corruption",
    "ì •ë³´ë³´í˜¸": "GRI 418: Customer Privacy",
    "ë³´ì•ˆ": "GRI 418: Customer Privacy"
}

GRI_TOPICS = {
    "GRI 302: Energy": "ì—ë„ˆì§€ ì†Œë¹„, íš¨ìœ¨ ê°œì„ , ì¬ìƒ ì—ë„ˆì§€ ì‚¬ìš© ë“±",
    "GRI 305: Emissions": "ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œ, íƒ„ì†Œì¤‘ë¦½, íƒ„ì†Œë°°ì¶œê¶Œ ë“±",
    "GRI 303: Water": "ìˆ˜ìì› ì ˆì•½, ìš©ìˆ˜ ê´€ë¦¬, ì˜¤ì—¼ ë°©ì§€ ë“±",
    "GRI 306: Waste": "íê¸°ë¬¼ ê°ì¶•, ìì›ìˆœí™˜, ì¬í™œìš© ë“±",
    "GRI 412: Human Rights": "ë…¸ë™ ì¸ê¶Œ, ê°•ì œë…¸ë™, ì°¨ë³„ê¸ˆì§€, ê²°ì‚¬ì˜ ììœ ",
    "GRI 405: Diversity": "ë‹¤ì–‘ì„±ê³¼ í¬ìš©, ì„±ë³„ ë‹¤ì–‘ì„±, í‰ë“± ê¸°íšŒ",
    "GRI 205: Anti-corruption": "ìœ¤ë¦¬ê²½ì˜, ë¶€íŒ¨ ë°©ì§€, ë‚´ë¶€ ê³ ë°œ ì‹œìŠ¤í…œ",
    "GRI 418: Customer Privacy": "ê°œì¸ì •ë³´ë³´í˜¸, ë³´ì•ˆ, ì •ë³´ ìœ ì¶œ ë°©ì§€",
    "GRI 403: Occupational Health": "ì‚°ì—…ì•ˆì „, ê±´ê°• ë³´í˜¸, ì‚¬ê³  ì˜ˆë°©",
    "GRI 201: Economic Performance": "ê²½ì œ ì„±ê³¼, ë§¤ì¶œ, íˆ¬ìì ê´€ê³„"
}

sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')

def extract_keywords_from_text(text: str, model) -> list:
    okt = Okt()
    nouns = ' '.join(okt.nouns(text))
    return [kw for kw, _ in model.extract_keywords(nouns, top_n=3)]

def map_keywords_to_gri(keywords: list[str]) -> str | None:
    for kw in keywords:
        for pattern, gri_code in GRI_KEYWORD_MAP.items():
            if pattern in kw:
                return gri_code
    return None

def match_to_gri_by_similarity(text: str) -> str | None:
    target_embedding = sbert_model.encode(text.strip(), convert_to_tensor=True)
    max_sim = 0.0
    matched_topic = None
    for gri_code, gri_desc in GRI_TOPICS.items():
        gri_embedding = sbert_model.encode(gri_desc, convert_to_tensor=True)
        sim = float(util.pytorch_cos_sim(target_embedding, gri_embedding))
        if sim > max_sim:
            max_sim = sim
            matched_topic = gri_code
    return matched_topic if max_sim > 0.4 else None

def map_keywords_to_gri_or_semantic(keywords: list[str], full_text: str) -> str | None:
    mapped = map_keywords_to_gri(keywords)
    if mapped:
        return mapped
    return match_to_gri_by_similarity(full_text)

def compute_keyword_frequency_score(keywords: list[str], text: str) -> float:
    total = sum(text.count(kw) for kw in keywords)
    return min(total / 3, 1.0)

def simple_sentiment_score(text: str) -> float:
    positive_words = ["í˜ì‹ ", "ê¸°íšŒ", "ê°•í™”", "í™•ëŒ€", "í–¥ìƒ"]
    negative_words = ["ìœ„í—˜", "ë¬¸ì œ", "ë¦¬ìŠ¤í¬", "ê°ì†Œ", "ì§€ì—°"]
    pos = sum(text.count(w) for w in positive_words)
    neg = sum(text.count(w) for w in negative_words)
    base = 0.5 + 0.1 * (pos - neg)
    return max(min(base, 1.0), 0.0)

def contextual_importance_score(text: str) -> float:
    emphasis_words = ["ì¤‘ìš”", "í•µì‹¬", "ìš°ì„ ", "ì „ëµ", "ì£¼ìš”", "ì¤‘ëŒ€"]
    return 0.2 if any(word in text for word in emphasis_words) else 0.0

def compute_issue_score(keywords: list[str], text: str) -> float:
    freq_score = compute_keyword_frequency_score(keywords, text)
    sent_score = simple_sentiment_score(text)
    ctx_score = contextual_importance_score(text)
    final_score = 0.4 * freq_score + 0.4 * sent_score + 0.2 * ctx_score
    return round(final_score, 3)

def save_issues_to_json(issues: list[ESGIssue], filename: str) -> str:
    path = os.path.join(OUTPUT_DIR, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump([issue.dict() for issue in issues], f, ensure_ascii=False, indent=2)
    return path

def save_issues_to_csv(issues: list[ESGIssue], filename: str) -> str:
    path = os.path.join(OUTPUT_DIR, filename)
    with open(path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["id", "text", "keywords", "mapped_gri", "score", "source_file"])
        for issue in issues:
            writer.writerow([
                issue.id, issue.text, ', '.join(issue.keywords),
                issue.mapped_gri or "", issue.score, issue.source_file
            ])
    return path

def extract_esg_issues_from_pdf(file_path: str) -> list[ESGIssue]:
    text = extract_text(file_path)
    paragraphs = list(set(p.strip() for p in text.split('\n') if len(p.strip()) > 50))
    esg_keywords = ['ê¸°í›„', 'íƒ„ì†Œ', 'ì—ë„ˆì§€', 'ìˆ˜ìì›', 'ì¸ê¶Œ', 'ê³µê¸‰ë§', 'ì •ë³´ë³´í˜¸', 'ë³´ì•ˆ', 'ìœ¤ë¦¬', 'ì¬ìƒ', 'í™˜ê²½', 'ë‹¤ì–‘ì„±']
    filtered_paragraphs = [p for p in paragraphs if any(k in p for k in esg_keywords)]

    kw_model = KeyBERT(model='distiluse-base-multilingual-cased-v2')
    results = []

    for para in filtered_paragraphs:
        keywords = extract_keywords_from_text(para, kw_model)
        mapped = map_keywords_to_gri_or_semantic(keywords, para)
        score = compute_issue_score(keywords, para)
        issue = ESGIssue(
            id=str(uuid.uuid4()),
            text=para,
            keywords=keywords,
            mapped_gri=mapped,
            score=score,
            source_file=file_path.split("/")[-1]
        )
        results.append(issue)
    return results

def cluster_keywords(issues: list[ESGIssue], n_clusters: int = 5):
    all_keywords = [kw for issue in issues for kw in issue.keywords]
    unique_keywords = list(set(all_keywords))
    if len(unique_keywords) < n_clusters:
        return {}, {}

    embeddings = sbert_model.encode(unique_keywords)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)

    clustered = defaultdict(list)
    for kw, label in zip(unique_keywords, labels):
        clustered[int(label)].append(kw)  # labelì„ intë¡œ ëª…ì‹œì  ë³€í™˜

    cluster_representatives = {
        int(label): Counter(kw_list).most_common(1)[0][0]  # ì—¬ê¸°ë„ ëª…ì‹œì  ë³€í™˜
        for label, kw_list in clustered.items()
    }

    return clustered, cluster_representatives

def save_clusters_to_json(clustered: dict, representatives: dict, filename: str) -> str:
    path = os.path.join(OUTPUT_DIR, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump({
            "clustered_keywords": clustered,
            "cluster_representatives": representatives
        }, f, ensure_ascii=False, indent=2)
    return path

def load_local_ai_model():
    """ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ"""
    try:
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        from peft import PeftModel
        
        # ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        base_model_name = "klue/bert-base"
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        model = PeftModel.from_pretrained(model, LOCAL_MODEL_PATH)
        
        logger.info(f"âœ… ë¡œì»¬ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {LOCAL_MODEL_PATH}")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"âŒ ë¡œì»¬ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None, None

def classify_text_with_local_ai(text: str, model, tokenizer) -> dict:
    """ë¡œì»¬ AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    try:
        import torch
        
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            text,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1)
            prediction = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][prediction].item()
        
        return {
            "is_esg": prediction == 1,
            "confidence": confidence,
            "probabilities": probabilities[0].tolist()
        }
        
    except Exception as e:
        logger.error(f"AI ëª¨ë¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"is_esg": False, "confidence": 0.0, "probabilities": []}

def filter_paragraphs_with_local_ai(paragraphs: list[str], model, tokenizer) -> list[dict]:
    """ë¡œì»¬ AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ESG ê´€ë ¨ ë¬¸ë‹¨ë“¤ì„ í•„í„°ë§"""
    esg_paragraphs = []
    
    logger.info(f"ğŸ¤– ë¡œì»¬ AI ëª¨ë¸ë¡œ {len(paragraphs)}ê°œ ë¬¸ë‹¨ ë¶„ë¥˜ ì‹œì‘...")
    
    for i, paragraph in enumerate(paragraphs, 1):
        if len(paragraph.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ ì œì™¸
            continue
            
        # AI ëª¨ë¸ë¡œ ë¶„ë¥˜
        classification = classify_text_with_local_ai(paragraph, model, tokenizer)
        
        if classification["is_esg"] and classification["confidence"] >= ESG_CONFIDENCE_THRESHOLD:
            esg_paragraphs.append({
                "text": paragraph,
                "confidence": classification["confidence"],
                "probabilities": classification["probabilities"]
            })
            logger.debug(f"âœ… ESG ë¬¸ë‹¨ ë°œê²¬ [{i}]: ì‹ ë¢°ë„ {classification['confidence']:.3f}")
        else:
            logger.debug(f"âŒ ë¹„ESG ë¬¸ë‹¨ [{i}]: ì‹ ë¢°ë„ {classification['confidence']:.3f}")
    
    logger.info(f"ğŸ¯ AI í•„í„°ë§ ê²°ê³¼: {len(esg_paragraphs)}/{len(paragraphs)}ê°œ ESG ë¬¸ë‹¨ ì¶”ì¶œ")
    return esg_paragraphs

def extract_esg_issues_from_pdf_with_local_ai(file_path: str, use_ai_model: bool = True) -> list[ESGIssue]:
    """ë¡œì»¬ AI ëª¨ë¸ì„ ì‚¬ìš©í•œ í–¥ìƒëœ ESG ì´ìŠˆ ì¶”ì¶œ"""
    logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {file_path}")
    
    # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = extract_text(file_path)
    paragraphs = list(set(p.strip() for p in text.split('\n') if len(p.strip()) > 50))
    
    logger.info(f"ğŸ“Š ì´ {len(paragraphs)}ê°œ ë¬¸ë‹¨ ì¶”ì¶œ")
    
    # AI ëª¨ë¸ ë˜ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
    if use_ai_model:
        try:
            # ë¡œì»¬ AI ëª¨ë¸ ë¡œë“œ
            model, tokenizer = load_local_ai_model()
            
            if model is not None and tokenizer is not None:
                # AI ëª¨ë¸ë¡œ ESG ë¬¸ë‹¨ í•„í„°ë§
                esg_paragraphs_data = filter_paragraphs_with_local_ai(paragraphs, model, tokenizer)
                filtered_paragraphs = [item["text"] for item in esg_paragraphs_data]
                confidence_scores = {item["text"]: item["confidence"] for item in esg_paragraphs_data}
                
                logger.info(f"ğŸ¤– ë¡œì»¬ AI ëª¨ë¸ í•„í„°ë§ ì™„ë£Œ: {len(filtered_paragraphs)}ê°œ ESG ë¬¸ë‹¨")
            else:
                raise Exception("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                
        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ì‚¬ìš© ì‹¤íŒ¨, í‚¤ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´: {str(e)}")
            filtered_paragraphs = fallback_keyword_filter(paragraphs)
            confidence_scores = {}
            
    else:
        # ê¸°ì¡´ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
        filtered_paragraphs = fallback_keyword_filter(paragraphs)
        confidence_scores = {}
        logger.info(f"ğŸ”¤ í‚¤ì›Œë“œ í•„í„°ë§ ì™„ë£Œ: {len(filtered_paragraphs)}ê°œ ESG ë¬¸ë‹¨")

    # KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì´ìŠˆ ìƒì„±
    kw_model = KeyBERT(model='distiluse-base-multilingual-cased-v2')
    results = []

    logger.info(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì´ìŠˆ ìƒì„± ì‹œì‘...")
    
    for i, para in enumerate(filtered_paragraphs, 1):
        keywords = extract_keywords_from_text(para, kw_model)
        mapped = map_keywords_to_gri_or_semantic(keywords, para)
        
        # AI ì‹ ë¢°ë„ê°€ ìˆìœ¼ë©´ ì ìˆ˜ì— ë°˜ì˜
        base_score = compute_issue_score(keywords, para)
        ai_confidence = confidence_scores.get(para, 0.0)
        
        # AI ì‹ ë¢°ë„ë¥¼ ì ìˆ˜ì— ë°˜ì˜ (ê°€ì¤‘ì¹˜ 0.3)
        final_score = base_score * 0.7 + ai_confidence * 0.3 if ai_confidence > 0 else base_score
        
        issue = ESGIssue(
            id=str(uuid.uuid4()),
            text=para,
            keywords=keywords,
            mapped_gri=mapped,
            score=round(final_score, 3),
            source_file=file_path.split("/")[-1]
        )
        results.append(issue)
        
        if i % 10 == 0:
            logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {i}/{len(filtered_paragraphs)} ì™„ë£Œ")

    logger.info(f"âœ… ESG ì´ìŠˆ ì¶”ì¶œ ì™„ë£Œ: {len(results)}ê°œ ì´ìŠˆ ìƒì„±")
    return results

def fallback_keyword_filter(paragraphs: list[str]) -> list[str]:
    """AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ (ê¸°ì¡´ ë°©ì‹)"""
    esg_keywords = ['ê¸°í›„', 'íƒ„ì†Œ', 'ì—ë„ˆì§€', 'ìˆ˜ìì›', 'ì¸ê¶Œ', 'ê³µê¸‰ë§', 'ì •ë³´ë³´í˜¸', 'ë³´ì•ˆ', 'ìœ¤ë¦¬', 'ì¬ìƒ', 'í™˜ê²½', 'ë‹¤ì–‘ì„±']
    return [p for p in paragraphs if any(k in p for k in esg_keywords)]